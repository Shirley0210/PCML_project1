{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Don't use it! It deacrease the performance \n",
    "from helpers import *\n",
    "\n",
    "tX_norm, mean_, std_ = standardize(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_data(tx):\n",
    "    nbrRows = tx.shape[0]\n",
    "    nbrColunms = tx.shape[1]\n",
    "    tx_cleaned = np.zeros((nbrRows,nbrColunms))\n",
    "    \n",
    "    for columnID in range(nbrColunms):\n",
    "        currentColumn = tx[:,columnID]\n",
    "\n",
    "        # replace -999 values with mean of the rest\n",
    "        nanIndices = np.where(currentColumn == -999)\n",
    "        tempColumm = np.delete(currentColumn, nanIndices, axis=0)\n",
    "        mean = np.mean(tempColumm)\n",
    "        currentColumn[nanIndices] = mean\n",
    "\n",
    "        # normalization of the data set\n",
    "        #norm=np.linalg.norm(currentColumn)\n",
    "        #tx_cleaned[:,columnID] = currentColumn/norm\n",
    "        \n",
    "    return tx_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX_cleaned = clean_data(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of ML methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression - gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from costs import *\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    # error\n",
    "    e = y - tx.dot(w)\n",
    "    \n",
    "    # gradient\n",
    "    gradient = - np.mean(np.transpose(tx).dot(e))\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, gamma, max_iters):\n",
    "    # Define parameters to store w and loss\n",
    "    w_temp = np.zeros(tx.shape[1]) # initialization of the weight\n",
    "    ws = [w_temp]\n",
    "    losses = []\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        grad = compute_gradient(y, tx, w_temp)\n",
    "        loss = compute_loss(y, tx, w_temp)\n",
    "        \n",
    "        # update w by gradient\n",
    "        w_temp = w_temp - gamma*grad\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w_temp))\n",
    "        losses.append(loss)\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression - stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    B = 35 # size of the batch\n",
    "    sum = 0\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, B):\n",
    "        sum += compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "\n",
    "    return sum / B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, gamma, max_iters):    \n",
    "    # Define parameters to store w and loss\n",
    "    w_temp = np.zeros(tx.shape[1]) # initialization of the weight\n",
    "    ws = [w_temp]\n",
    "    losses = []\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        grad = compute_stoch_gradient(y, tx, w_temp)\n",
    "        loss = compute_loss(y, tx, w_temp)\n",
    "\n",
    "        # update w by gradient\n",
    "        w_temp = w_temp - gamma*grad\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w_temp))\n",
    "        losses.append(loss)\n",
    "        \n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    # Compute optimum weight\n",
    "    tx_transpose = np.transpose(tx)\n",
    "    A = tx_transpose.dot(tx)\n",
    "    b = tx_transpose.dot(y)\n",
    "    w_opt = np.linalg.solve(A,b)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = compute_loss(y, tx, w_opt)\n",
    "        \n",
    "    return loss, w_opt # returns loss, and optimal weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):    \n",
    "    # Initiation variables\n",
    "    lamb_ = 2*len(y)*lambda_ \n",
    "\n",
    "    # Compute optimum weight\n",
    "    tx_transpose = np.transpose(tx)\n",
    "    A = np.dot(tx_transpose,tx) + lamb_*np.eye(tx.shape[1])\n",
    "    b = tx_transpose.dot(y)\n",
    "    w_opt = np.linalg.solve(A,b)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = compute_loss(y, tx, w_opt)\n",
    "    \n",
    "    return loss, w_opt # returns mse, and optimal weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1/(1+np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    # compute the loss\n",
    "    N = tx.shape[0]\n",
    "    l1 = tx.dot(w) + np.log(np.ones((N))+np.exp(-tx.dot(w)))\n",
    "    l2 = y*(tx.dot(w))\n",
    "    loss = (np.ones((1,N)).dot(l1-l2))[0]\n",
    "    \n",
    "    # compute the gradient\n",
    "    grad = np.transpose(tx).dot(sigmoid(tx.dot(w))-y)\n",
    "    \n",
    "    # update w\n",
    "    w = w - gamma*grad\n",
    "\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, gamma, max_iters):\n",
    "    print(\"\\n\\n\\nlogistic_regression(y, tx, gamma, max_iters):\\n\")\n",
    "\n",
    "    # init parameters\n",
    "    threshold = 1e-8\n",
    "    w_temp = np.zeros(tx.shape[1]) # initialization of the weight\n",
    "    ws = [w_temp]\n",
    "    losses = [8000]\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iters):        \n",
    "        # get loss and update w.\n",
    "        loss, w_temp = learning_by_gradient_descent(y, tx, w_temp, gamma)\n",
    "        \n",
    "        # log info\n",
    "        if iter % 1000 == 0:\n",
    "            print(\"Current iteration={i}:\\nLoss={l}\\nw={w}\\n\".format(i=iter, l=loss, w=ws[-1]))\n",
    "            \n",
    "       \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w_temp))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # converge criteria\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    \n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = len(y)\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    \n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train:\n",
    "    x_test = x[k_indices[k]]\n",
    "    y_test = y[k_indices[k]]\n",
    "    \n",
    "    tr_indices = np.delete(k_indices, k, axis=0)\n",
    "    x_tr = np.delete(x, k, axis=0)\n",
    "    y_tr = np.delete(y, k, axis=0)\n",
    "    \n",
    "    # form train and test data with polynomial basis function\n",
    "    poly_x_tr = build_poly(x_tr, degree)\n",
    "    poly_x_test = build_poly(x_test, degree)\n",
    "    \n",
    "    # calcualte weight and loss through least square.\n",
    "    loss_tr, weight_tr = ridge_regression(y_tr, poly_x_tr, lambda_)\n",
    "    loss_test, weight_te = ridge_regression(y_test, poly_x_test, lambda_)\n",
    "    \n",
    "    return loss_tr, loss_te, weight_tr, weight_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_GD=[0.5, 0.49999999427601749, 0.49999998855203615, 0.49999998282805563, 0.49999997710407629, 0.49999997138009794, 0.4999999656561207, 0.49999995993214458, 0.49999995420816917, 0.49999994848419493, 0.49999994276022192, 0.49999993703624973, 0.49999993131227854, 0.49999992558830864, 0.49999991986433956, 0.49999991414037154, 0.49999990841640457, 0.49999990269243855, 0.49999989696847363, 0.49999989124450978, 0.49999988552054697, 0.499999879796585, 0.49999987407262447, 0.49999986834866461, 0.49999986262470586, 0.49999985690074816, 0.49999985117679147, 0.49999984545283588, 0.49999983972888118, 0.4999998340049277, 0.49999982828097517, 0.4999998225570238, 0.49999981683307332, 0.49999981110912384, 0.49999980538517524, 0.49999979966122798, 0.49999979393728167, 0.49999978821333624, 0.49999978248939198, 0.49999977676544882, 0.49999977104150656, 0.49999976531756535, 0.49999975959362525, 0.49999975386968604, 0.49999974814574799, 0.49999974242181089, 0.4999997366978749, 0.4999997309739399, 0.49999972525000602, 0.49999971952607308, 0.49999971380214114, 0.49999970807821037, 0.4999997023542806, 0.4999996966303516, 0.49999969090642377, 0.49999968518249727, 0.49999967945857138, 0.49999967373464665, 0.4999996680107231, 0.49999966228680043, 0.49999965656287881, 0.49999965083895831, 0.4999996451150388, 0.49999963939112041, 0.49999963366720296, 0.49999962794328651, 0.49999962221937111, 0.49999961649545666, 0.49999961077154337, 0.49999960504763102, 0.4999995993237199, 0.49999959359980944, 0.49999958787590026, 0.49999958215199225, 0.49999957642808496, 0.49999957070417883, 0.49999956498027376, 0.4999995592563698, 0.499999553532467, 0.49999954780856487, 0.49999954208466385, 0.49999953636076394, 0.49999953063686503, 0.49999952491296723, 0.49999951918907037, 0.49999951346517446, 0.49999950774127983, 0.49999950201738613, 0.49999949629349344, 0.4999994905696018, 0.49999948484571105, 0.49999947912182152, 0.49999947339793299, 0.49999946767404546, 0.49999946195015882, 0.49999945622627345, 0.49999945050238898, 0.49999944477850566, 0.4999994390546233, 0.49999943333074176]\n",
      "\n",
      "\n",
      "weights_GD=[ -6.90647742e-05  -6.90647742e-05  -6.90647742e-05  -6.90647742e-05\n",
      "  -6.90647742e-05  -6.90647742e-05  -6.90647742e-05  -6.90647742e-05\n",
      "  -6.90647742e-05  -6.90647742e-05  -6.90647742e-05  -6.90647742e-05\n",
      "  -6.90647742e-05  -6.90647742e-05  -6.90647742e-05  -6.90647742e-05\n",
      "  -6.90647742e-05  -6.90647742e-05  -6.90647742e-05  -6.90647742e-05\n",
      "  -6.90647742e-05  -6.90647742e-05  -6.90647742e-05  -6.90647742e-05\n",
      "  -6.90647742e-05  -6.90647742e-05  -6.90647742e-05  -6.90647742e-05\n",
      "  -6.90647742e-05  -6.90647742e-05]\n"
     ]
    }
   ],
   "source": [
    "# Linear regression - gradient descent\n",
    "max_iters_GD = 100\n",
    "gamma_GD = 1.0e-8\n",
    "loss_GD, weights_GD = least_squares_GD(y, tX_cleaned, gamma_GD, max_iters_GD)\n",
    "print(\"loss_GD={loss}\\n\\n\".format(loss=loss_GD))\n",
    "print(\"weights_GD={w}\".format(w=weights_GD[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_SGD=[0.5, 0.49999999983645765, 0.49999999967291525, 0.49999999950937302, 0.4999999993458305, 0.49999999918228827, 0.49999999901874581, 0.49999999885520358, 0.49999999869166112, 0.49999999852811883, 0.49999999836457648, 0.49999999820103391, 0.49999999803749173, 0.49999999787394933, 0.49999999771040704, 0.49999999754686464, 0.49999999738332229, 0.49999999721977983, 0.49999999705623766, 0.49999999689269531, 0.49999999672915285, 0.49999999656561062, 0.49999999640206821, 0.49999999623852587, 0.49999999607498347, 0.49999999591144123, 0.49999999574789877, 0.49999999558435654, 0.49999999542081414, 0.49999999525727173, 0.4999999950937295, 0.49999999493018704, 0.49999999476664492, 0.49999999460310246, 0.49999999443956006, 0.49999999427601782, 0.49999999411247537, 0.49999999394893313, 0.49999999378539067, 0.49999999362184855, 0.49999999345830609, 0.49999999329476391, 0.49999999313122145, 0.49999999296767911, 0.49999999280413682, 0.49999999264059453, 0.49999999247705212, 0.49999999231350989, 0.49999999214996743, 0.49999999198642514, 0.49999999182288279, 0.49999999165934061, 0.49999999149579821, 0.49999999133225587, 0.49999999116871352, 0.49999999100517134, 0.49999999084162894, 0.49999999067808654, 0.49999999051454436, 0.4999999903510019, 0.49999999018745972, 0.49999999002391754, 0.49999998986037503, 0.49999998969683279, 0.4999999895332905, 0.4999999893697481, 0.49999998920620586, 0.49999998904266335, 0.49999998887912123, 0.49999998871557888, 0.4999999885520367, 0.49999998838849435, 0.4999999882249519, 0.49999998806140972, 0.49999998789786737, 0.49999998773432514, 0.49999998757078279, 0.49999998740724044, 0.49999998724369821, 0.49999998708015586, 0.49999998691661357, 0.49999998675307139, 0.49999998658952904, 0.49999998642598664, 0.49999998626244452, 0.49999998609890212, 0.49999998593535983, 0.4999999857718177, 0.49999998560827519, 0.49999998544473312, 0.49999998528119066, 0.49999998511764848, 0.49999998495410619, 0.49999998479056373, 0.49999998462702172, 0.49999998446347926, 0.49999998429993703, 0.49999998413639474, 0.49999998397285245, 0.49999998380931021]\n",
      "\n",
      "weights_SGD=[ -1.97328781e-06  -1.97328781e-06  -1.97328781e-06  -1.97328781e-06\n",
      "  -1.97328781e-06  -1.97328781e-06  -1.97328781e-06  -1.97328781e-06\n",
      "  -1.97328781e-06  -1.97328781e-06  -1.97328781e-06  -1.97328781e-06\n",
      "  -1.97328781e-06  -1.97328781e-06  -1.97328781e-06  -1.97328781e-06\n",
      "  -1.97328781e-06  -1.97328781e-06  -1.97328781e-06  -1.97328781e-06\n",
      "  -1.97328781e-06  -1.97328781e-06  -1.97328781e-06  -1.97328781e-06\n",
      "  -1.97328781e-06  -1.97328781e-06  -1.97328781e-06  -1.97328781e-06\n",
      "  -1.97328781e-06  -1.97328781e-06]\n"
     ]
    }
   ],
   "source": [
    "# Linear regression - stochastic gradient descent\n",
    "max_iters_SGD = 100\n",
    "gamma_SGD = 1.0e-8\n",
    "loss_SGD, weights_SGD = least_squares_SGD(y, tX_cleaned, gamma_SGD, max_iters_SGD)\n",
    "print(\"loss_SGD={loss}\\n\".format(loss=loss_SGD))\n",
    "print(\"weights_SGD={w}\".format(w=weights_SGD[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_LeastS=0.3438151967517036\n",
      "\n",
      "weights_LeastS=[  2.73031424e+00  -2.29747827e+02  -2.54770832e+02  -2.24815706e+01\n",
      "  -5.77338234e+01   1.10828828e+02  -1.61103684e+01   3.71264149e+02\n",
      "  -2.03574598e+01  -2.73887296e+05  -2.07311312e+02   5.89734344e+01\n",
      "   8.75575679e+01   6.25977052e+04  -2.71874704e-01  -6.71795197e-01\n",
      "   7.23979718e+04  -6.94764531e-01   1.12218635e+00   8.28082429e+01\n",
      "   4.98401250e-01  -6.28121902e+01  -1.97014413e+02  -2.57065452e+02\n",
      "   3.23642173e-01   2.59443567e-01  -2.93845037e+02   8.23643682e-01\n",
      "  -1.05994110e+00   1.70976515e+05]\n"
     ]
    }
   ],
   "source": [
    "# Least squares\n",
    "loss_LeastS, weights_LeastS = least_squares(y, tX_cleaned)\n",
    "print(\"loss_LeastS={loss}\\n\".format(loss=loss_LeastS))\n",
    "print(\"weights_LeastS={w}\".format(w=weights_LeastS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_RR=0.49999873083700275\n",
      "\n",
      "weights_RR=[ -1.42310341e-04  -2.25070869e-04  -1.43552430e-04  -3.82624819e-05\n",
      "  -1.14573968e-04  -9.39628425e-05  -1.09352936e-05  -1.47588356e-04\n",
      "  -1.07377301e-04  -8.41617898e-05  -1.82627085e-04   1.45050432e-04\n",
      "  -1.07448053e-04  -8.02112838e-05   9.74267298e-07  -1.38182658e-06\n",
      "  -1.48710861e-04   3.14544453e-06  -1.81264161e-06  -1.16941295e-04\n",
      "   4.42624474e-06  -1.01521866e-04  -6.65704885e-05  -1.17474574e-04\n",
      "   4.89414315e-07   1.30381096e-06  -1.52481609e-04   2.02551398e-06\n",
      "  -1.43140608e-06  -4.29280668e-05]\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression\n",
    "lambda_RR = 2\n",
    "loss_RR, weights_RR = ridge_regression(y, tX_cleaned, lambda_RR)\n",
    "print(\"loss_RR={loss}\\n\".format(loss=loss_RR))\n",
    "print(\"weights_RR={w}\".format(w=weights_RR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "logistic_regression(y, tx, gamma, max_iters):\n",
      "\n",
      "Current iteration=0:\n",
      "Loss=173286.79513997794\n",
      "w=[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "loss_LogR=[8000, 173286.79513997794, 173286.77399203292, 173286.75284408103, 173286.73169613071, 173286.71054818176, 173286.68940023438, 173286.66825228845, 173286.64710434401, 173286.62595640111, 173286.60480845967, 173286.58366051968, 173286.56251258121, 173286.54136464425, 173286.52021670877, 173286.49906877487, 173286.47792084233, 173286.45677291133, 173286.43562498182, 173286.41447705386, 173286.39332912728, 173286.37218120223, 173286.35103327874, 173286.32988535665, 173286.30873743611, 173286.2875895171, 173286.26644159947, 173286.24529368334, 173286.22414576885, 173286.20299785567, 173286.18184994417, 173286.16070203402, 173286.13955412543, 173286.11840621824, 173286.09725831266, 173286.07611040852, 173286.05496250585, 173286.03381460466, 173286.01266670501, 173285.9915188069, 173285.9703709101, 173285.94922301499, 173285.92807512131, 173285.90692722908, 173285.88577933834, 173285.86463144911, 173285.84348356139, 173285.82233567516, 173285.80118779044, 173285.78003990714, 173285.75889202539, 173285.7377441451, 173285.71659626637, 173285.69544838907, 173285.67430051323, 173285.6531526389, 173285.63200476611, 173285.61085689481, 173285.58970902496, 173285.56856115669, 173285.54741328978, 173285.52626542444, 173285.50511756059, 173285.48396969819, 173285.46282183731, 173285.44167397794, 173285.42052612, 173285.39937826368, 173285.37823040877, 173285.35708255536, 173285.33593470338, 173285.31478685298, 173285.29363900403, 173285.27249115656, 173285.25134331052, 173285.23019546614, 173285.20904762321, 173285.18789978162, 173285.16675194167, 173285.14560410322, 173285.12445626617, 173285.1033084307, 173285.08216059662, 173285.06101276414, 173285.03986493306, 173285.01871710358, 173284.99756927547, 173284.97642144893, 173284.95527362384, 173284.93412580027, 173284.91297797812, 173284.89183015755, 173284.87068233849, 173284.84953452085, 173284.82838670473, 173284.80723889009, 173284.786091077, 173284.76494326533, 173284.74379545517, 173284.7226476465, 173284.70149983931, 173284.68035203361, 173284.65920422948, 173284.63805642677, 173284.61690862558, 173284.59576082591, 173284.57461302762, 173284.55346523091, 173284.53231743569, 173284.51116964192, 173284.49002184969, 173284.46887405892, 173284.44772626966, 173284.42657848194, 173284.40543069565, 173284.38428291085, 173284.36313512758, 173284.34198734572, 173284.32083956539, 173284.29969178658, 173284.27854400923, 173284.25739623344, 173284.23624845906, 173284.21510068621, 173284.19395291479, 173284.172805145, 173284.15165737661, 173284.13050960971, 173284.10936184431, 173284.08821408043, 173284.06706631801, 173284.04591855707, 173284.02477079764, 173284.00362303975, 173283.98247528318, 173283.96132752835, 173283.94017977483, 173283.91903202288, 173283.89788427236, 173283.87673652344, 173283.85558877588, 173283.83444102993, 173283.81329328546, 173283.79214554236, 173283.77099780086, 173283.74985006085, 173283.72870232229, 173283.70755458521, 173283.68640684971, 173283.66525911563, 173283.6441113831, 173283.62296365199, 173283.60181592236, 173283.58066819428, 173283.55952046771, 173283.53837274257, 173283.51722501894, 173283.49607729679, 173283.47492957616, 173283.45378185704, 173283.43263413932, 173283.41148642317, 173283.3903387085, 173283.36919099535, 173283.34804328365, 173283.32689557347, 173283.30574786474, 173283.28460015755, 173283.26345245185, 173283.24230474757, 173283.22115704481, 173283.20000934368, 173283.17886164389, 173283.15771394561, 173283.13656624881, 173283.11541855359, 173283.09427085979, 173283.07312316747, 173283.05197547667, 173283.03082778736, 173283.00968009955, 173282.9885324132, 173282.96738472837, 173282.94623704505, 173282.92508936318, 173282.90394168277, 173282.88279400393, 173282.86164632661, 173282.84049865074, 173282.81935097632, 173282.79820330348, 173282.77705563203, 173282.75590796213, 173282.73476029368, 173282.71361262677, 173282.69246496129, 173282.67131729738, 173282.65016963487, 173282.62902197393, 173282.6078743145, 173282.5867266565, 173282.56557900005, 173282.54443134496, 173282.52328369155, 173282.50213603943, 173282.480988389, 173282.45984073999, 173282.43869309244, 173282.41754544637, 173282.39639780181, 173282.37525015872, 173282.35410251719, 173282.33295487714, 173282.31180723856, 173282.29065960139, 173282.26951196583, 173282.24836433175, 173282.2272166991, 173282.20606906796, 173282.18492143831, 173282.1637738102, 173282.14262618354, 173282.12147855843, 173282.10033093474, 173282.07918331254, 173282.05803569188, 173282.0368880727, 173282.01574045498, 173281.99459283875, 173281.97344522411, 173281.95229761084, 173281.93114999915, 173281.91000238888, 173281.88885478012, 173281.86770717287, 173281.84655956712, 173281.82541196287, 173281.80426435999, 173281.78311675877, 173281.76196915895, 173281.74082156067, 173281.71967396385, 173281.69852636848, 173281.67737877471, 173281.65623118239, 173281.63508359151, 173281.61393600219, 173281.5927884143, 173281.57164082793, 173281.55049324306, 173281.52934565966, 173281.50819807779, 173281.48705049741, 173281.46590291848, 173281.44475534107, 173281.42360776517, 173281.4024601907, 173281.38131261777, 173281.36016504632, 173281.33901747636, 173281.31786990783, 173281.29672234092, 173281.27557477547, 173281.25442721142, 173281.23327964894, 173281.21213208791, 173281.1909845284, 173281.1698369704, 173281.14868941382, 173281.12754185882, 173281.10639430527, 173281.08524675324, 173281.06409920263, 173281.0429516536, 173281.02180410599, 173281.00065655995, 173280.97950901531, 173280.95836147227, 173280.93721393059, 173280.91606639049, 173280.89491885185, 173280.87377131471, 173280.85262377909, 173280.8314762449, 173280.81032871222, 173280.78918118111, 173280.76803365143, 173280.74688612326, 173280.72573859649, 173280.70459107135, 173280.68344354763, 173280.6622960254, 173280.64114850471, 173280.62000098545, 173280.5988534677, 173280.57770595147, 173280.55655843674, 173280.53541092348, 173280.51426341163, 173280.49311590136, 173280.47196839258, 173280.45082088531, 173280.42967337946, 173280.40852587516, 173280.38737837237, 173280.366230871, 173280.34508337112, 173280.32393587279, 173280.30278837594, 173280.2816408806, 173280.26049338665, 173280.23934589434, 173280.21819840345, 173280.19705091402, 173280.17590342613, 173280.15475593973, 173280.13360845478, 173280.11246097134, 173280.09131348942, 173280.07016600898, 173280.04901853006, 173280.02787105262, 173280.0067235766, 173279.98557610213, 173279.96442862917, 173279.94328115764, 173279.92213368768, 173279.90098621909, 173279.87983875215, 173279.85869128656, 173279.83754382256, 173279.81639635999, 173279.79524889897, 173279.77410143943, 173279.75295398134, 173279.73180652474, 173279.71065906968, 173279.68951161607, 173279.66836416398, 173279.64721671343, 173279.62606926428, 173279.60492181667, 173279.58377437052, 173279.56262692588, 173279.54147948272, 173279.52033204114, 173279.49918460089, 173279.47803716219, 173279.45688972503, 173279.43574228938, 173279.41459485522, 173279.39344742245, 173279.37229999126, 173279.35115256149, 173279.33000513332, 173279.30885770655, 173279.28771028135, 173279.26656285761, 173279.24541543532, 173279.22426801454, 173279.20312059525, 173279.18197317753, 173279.16082576115, 173279.13967834634, 173279.11853093305, 173279.09738352121, 173279.07623611088, 173279.05508870209, 173279.03394129471, 173279.01279388886, 173278.99164648453, 173278.97049908162, 173278.94935168029, 173278.92820428044, 173278.90705688202, 173278.88590948505, 173278.86476208965, 173278.84361469583, 173278.82246730337, 173278.80131991248, 173278.78017252294, 173278.75902513508, 173278.73787774862, 173278.71673036361, 173278.69558298017, 173278.67443559814, 173278.65328821773, 173278.63214083869, 173278.61099346116, 173278.58984608512, 173278.56869871065, 173278.54755133757, 173278.52640396607, 173278.50525659596, 173278.48410922743, 173278.46296186038, 173278.44181449481, 173278.4206671307, 173278.39951976814, 173278.37837240702, 173278.35722504745, 173278.33607768931, 173278.31493033274, 173278.29378297756, 173278.27263562396, 173278.25148827181, 173278.23034092115, 173278.20919357202, 173278.18804622433, 173278.16689887817, 173278.14575153348, 173278.12460419023, 173278.10345684859, 173278.08230950838, 173278.06116216962, 173278.04001483243, 173278.01886749672, 173277.99772016244, 173277.97657282971, 173277.9554254984, 173277.93427816872, 173277.91313084043, 173277.8919835136, 173277.87083618832, 173277.84968886449, 173277.8285415422, 173277.80739422143, 173277.78624690208, 173277.76509958421, 173277.74395226795, 173277.72280495305, 173277.70165763976, 173277.6805103278, 173277.65936301748, 173277.63821570863, 173277.61706840119, 173277.59592109529, 173277.5747737909, 173277.55362648799, 173277.53247918657, 173277.51133188664, 173277.49018458818, 173277.46903729122, 173277.44788999582, 173277.42674270185, 173277.40559540933, 173277.38444811842, 173277.36330082893, 173277.34215354093, 173277.32100625441, 173277.29985896937, 173277.27871168591, 173277.25756440387, 173277.23641712338, 173277.21526984428, 173277.19412256675, 173277.17297529068, 173277.15182801613, 173277.13068074299, 173277.10953347149, 173277.08838620139, 173277.06723893277, 173277.04609166563, 173277.02494440007, 173277.00379713596, 173276.9826498733, 173276.96150261216, 173276.94035535245, 173276.9192080943, 173276.89806083773, 173276.8769135825, 173276.85576632884, 173276.83461907663, 173276.81347182591, 173276.79232457673, 173276.77117732904, 173276.75003008283, 173276.7288828381, 173276.70773559483, 173276.68658835307, 173276.66544111288, 173276.64429387407, 173276.62314663688, 173276.601999401, 173276.58085216678, 173276.55970493393, 173276.53855770262, 173276.51741047285, 173276.49626324448, 173276.47511601768, 173276.45396879231, 173276.43282156851, 173276.41167434616, 173276.39052712527, 173276.36937990593, 173276.348232688, 173276.32708547165, 173276.30593825679, 173276.28479104338, 173276.26364383145, 173276.24249662101, 173276.22134941211, 173276.20020220472, 173276.17905499876, 173276.15790779429, 173276.13676059135, 173276.11561338988, 173276.09446618985, 173276.07331899134, 173276.05217179446, 173276.03102459889, 173276.00987740493, 173275.98873021244, 173275.96758302138, 173275.94643583187, 173275.9252886438, 173275.90414145726, 173275.88299427216, 173275.86184708861, 173275.84069990652, 173275.81955272594, 173275.79840554684, 173275.77725836929, 173275.75611119322, 173275.73496401857, 173275.7138168455, 173275.69266967382, 173275.67152250378, 173275.65037533501, 173275.62922816793, 173275.60808100222, 173275.58693383803, 173275.56578667538, 173275.54463951418, 173275.52349235455, 173275.50234519635, 173275.4811980396, 173275.46005088437, 173275.43890373071, 173275.41775657842, 173275.3966094277, 173275.37546227843, 173275.35431513068, 173275.33316798441, 173275.3120208396, 173275.29087369633, 173275.26972655457, 173275.24857941427, 173275.22743227539, 173275.20628513809, 173275.18513800233, 173275.16399086799, 173275.14284373517, 173275.1216966038, 173275.10054947392, 173275.07940234558, 173275.05825521873, 173275.03710809335, 173275.01596096944, 173274.99481384701, 173274.97366672615, 173274.95251960674, 173274.93137248879, 173274.91022537235, 173274.88907825737, 173274.86793114396, 173274.84678403201, 173274.82563692157, 173274.80448981261, 173274.78334270517, 173274.76219559915, 173274.74104849464, 173274.7199013916, 173274.69875429006, 173274.67760719004, 173274.65646009165, 173274.63531299453, 173274.61416589894, 173274.59301880494, 173274.57187171234, 173274.55072462131, 173274.52957753174, 173274.50843044365, 173274.48728335707, 173274.46613627192, 173274.44498918828, 173274.42384210616, 173274.40269502558, 173274.3815479464, 173274.36040086881, 173274.33925379266, 173274.31810671795, 173274.29695964479, 173274.27581257315, 173274.25466550296, 173274.23351843428, 173274.21237136703, 173274.19122430138, 173274.17007723716, 173274.14893017447, 173274.12778311319, 173274.10663605348, 173274.08548899522, 173274.06434193847, 173274.04319488321, 173274.02204782941, 173274.00090077712, 173273.97975372637, 173273.95860667704, 173273.93745962926, 173273.91631258299, 173273.89516553807, 173273.87401849474, 173273.85287145295, 173273.83172441271, 173273.81057737378, 173273.78943033644, 173273.76828330057, 173273.74713626615, 173273.72598923321, 173273.7048422019, 173273.68369517193, 173273.66254814359, 173273.64140111665, 173273.62025409116, 173273.59910706728, 173273.57796004481, 173273.55681302387, 173273.53566600441, 173273.51451898646, 173273.4933719699, 173273.47222495498, 173273.45107794148, 173273.42993092947, 173273.408783919, 173273.38763690996, 173273.3664899024, 173273.34534289644, 173273.32419589188, 173273.3030488888, 173273.2819018873, 173273.26075488719, 173273.23960788862, 173273.21846089151, 173273.19731389589, 173273.17616690186, 173273.1550199092, 173273.13387291814, 173273.11272592848, 173273.09157894034, 173273.07043195379, 173273.04928496864, 173273.02813798492, 173273.00699100283, 173272.98584402213, 173272.96469704289, 173272.94355006522, 173272.922403089, 173272.9012561143, 173272.88010914112, 173272.85896216938, 173272.83781519916, 173272.8166682304, 173272.79552126318, 173272.77437429738, 173272.75322733313, 173272.73208037039, 173272.71093340917, 173272.68978644937, 173272.66863949102, 173272.64749253422, 173272.6263455789, 173272.6051986251, 173272.58405167283, 173272.56290472197, 173272.54175777259, 173272.52061082472, 173272.4994638784, 173272.4783169335, 173272.45716999014, 173272.43602304824, 173272.41487610791, 173272.39372916895, 173272.37258223159, 173272.3514352956, 173272.33028836115, 173272.30914142824, 173272.28799449682, 173272.26684756685, 173272.24570063845, 173272.22455371148, 173272.20340678596, 173272.18225986196, 173272.16111293953, 173272.1399660185, 173272.11881909904, 173272.09767218103, 173272.07652526448, 173272.05537834944, 173272.03423143589, 173272.01308452385, 173271.99193761329, 173271.9707907043, 173271.94964379669, 173271.92849689058, 173271.907349986, 173271.88620308292, 173271.86505618133, 173271.84390928116, 173271.82276238254, 173271.80161548546, 173271.78046858983, 173271.75932169569, 173271.73817480297, 173271.71702791186, 173271.69588102214, 173271.67473413399, 173271.65358724736, 173271.63244036213, 173271.61129347846, 173271.59014659619, 173271.56899971541, 173271.54785283623, 173271.52670595856, 173271.50555908223, 173271.4844122075, 173271.46326533426, 173271.44211846252, 173271.42097159222, 173271.39982472343, 173271.37867785612, 173271.35753099032, 173271.33638412604, 173271.31523726322, 173271.29409040188, 173271.27294354205, 173271.2517966837, 173271.2306498269, 173271.20950297153, 173271.18835611758, 173271.16720926526, 173271.14606241434, 173271.12491556501, 173271.10376871709, 173271.08262187068, 173271.06147502575, 173271.04032818237, 173271.01918134044, 173270.99803449999, 173270.97688766103, 173270.95574082359, 173270.93459398759, 173270.91344715314, 173270.89230032015, 173270.87115348867, 173270.85000665867, 173270.8288598301, 173270.80771300313, 173270.78656617767, 173270.76541935359, 173270.74427253104, 173270.72312571004, 173270.70197889046, 173270.68083207242, 173270.65968525584, 173270.63853844075, 173270.61739162714, 173270.5962448151, 173270.57509800445, 173270.5539511953, 173270.53280438777, 173270.51165758161, 173270.49051077696, 173270.46936397385, 173270.44821717215, 173270.42707037201, 173270.40592357336, 173270.38477677619, 173270.36362998051, 173270.34248318625, 173270.32133639359, 173270.30018960242, 173270.27904281265, 173270.25789602444, 173270.23674923769, 173270.21560245249, 173270.19445566868, 173270.17330888644, 173270.15216210572, 173270.13101532642, 173270.10986854864, 173270.08872177236, 173270.06757499761, 173270.0464282243, 173270.02528145246, 173270.00413468207, 173269.98298791324, 173269.96184114597, 173269.94069438009, 173269.91954761569, 173269.89840085286, 173269.87725409149, 173269.85610733164, 173269.83496057324, 173269.81381381635, 173269.79266706092, 173269.771520307, 173269.75037355459, 173269.7292268037, 173269.70808005426, 173269.68693330622, 173269.66578655984, 173269.64463981491, 173269.62349307135, 173269.60234632937, 173269.58119958895, 173269.56005284988, 173269.53890611246, 173269.51775937638, 173269.49661264193, 173269.47546590891, 173269.45431917731, 173269.43317244732, 173269.41202571872, 173269.39087899169, 173269.36973226609, 173269.34858554206, 173269.32743881945, 173269.30629209831, 173269.28514537876, 173269.26399866067, 173269.24285194403, 173269.22170522888, 173269.20055851527, 173269.17941180308, 173269.15826509247, 173269.13711838334, 173269.11597167561, 173269.09482496945, 173269.0736782648, 173269.05253156164, 173269.0313848599, 173269.01023815974, 173268.989091461, 173268.96794476378, 173268.94679806801, 173268.92565137375, 173268.90450468104, 173268.88335798975, 173268.8622113, 173268.84106461171, 173268.81991792493, 173268.79877123964, 173268.77762455586, 173268.75647787357, 173268.73533119276, 173268.7141845134, 173268.69303783562, 173268.67189115926, 173268.65074448445, 173268.62959781106, 173268.60845113921, 173268.58730446879, 173268.56615779997, 173268.54501113255, 173268.52386446664, 173268.50271780224, 173268.48157113936, 173268.46042447787, 173268.43927781799, 173268.41813115953, 173268.39698450256, 173268.37583784718, 173268.35469119318, 173268.33354454071, 173268.31239788973, 173268.29125124027, 173268.27010459226, 173268.24895794576, 173268.22781130075, 173268.20666465725, 173268.18551801526, 173268.16437137465, 173268.14322473563, 173268.12207809809, 173268.10093146202, 173268.07978482748, 173268.05863819434, 173268.03749156278, 173268.01634493269, 173267.99519830415, 173267.97405167698, 173267.95290505135, 173267.93175842726, 173267.9106118046, 173267.88946518351, 173267.86831856385, 173267.84717194564, 173267.82602532895, 173267.80487871385, 173267.7837321001, 173267.76258548794, 173267.74143887724, 173267.72029226809, 173267.69914566033, 173267.67799905414, 173267.65685244935, 173267.63570584619, 173267.61455924436, 173267.59341264411, 173267.57226604538, 173267.55111944809, 173267.5299728523, 173267.5088262581, 173267.48767966515, 173267.46653307398, 173267.44538648415, 173267.42423989577, 173267.40309330897, 173267.38194672368, 173267.36080013978, 173267.3396535574, 173267.31850697656, 173267.29736039718, 173267.27621381931, 173267.25506724292, 173267.23392066802, 173267.2127740946, 173267.19162752273, 173267.17048095237, 173267.14933438343, 173267.12818781592, 173267.10704124998, 173267.0858946855, 173267.06474812259, 173267.0436015611, 173267.02245500113, 173267.00130844268, 173266.9801618857, 173266.95901533018, 173266.93786877621, 173266.91672222369, 173266.89557567253, 173266.87442912307, 173266.85328257506, 173266.83213602845, 173266.81098948338, 173266.78984293988, 173266.76869639783, 173266.74754985719, 173266.72640331811, 173266.70525678049, 173266.68411024439, 173266.66296370979, 173266.64181717666, 173266.62067064503, 173266.59952411486, 173266.57837758627, 173266.55723105907, 173266.53608453338, 173266.51493800926, 173266.49379148654, 173266.47264496543, 173266.45149844568, 173266.43035192744, 173266.40920541075, 173266.38805889548, 173266.36691238178, 173266.34576586957, 173266.32461935878, 173266.30347284948, 173266.28232634175, 173266.26117983548, 173266.24003333077, 173266.21888682747, 173266.19774032562, 173266.17659382534, 173266.15544732657, 173266.13430082923, 173266.11315433338, 173266.09200783906, 173266.07086134615, 173266.04971485486, 173266.02856836491, 173266.00742187659, 173265.98627538976, 173265.9651289043, 173265.94398242049, 173265.92283593805, 173265.90168945715, 173265.88054297771, 173265.85939649979, 173265.83825002334, 173265.81710354844, 173265.79595707497, 173265.774810603, 173265.7536641325, 173265.73251766351, 173265.71137119606, 173265.69022473012, 173265.66907826558]\n",
      "\n",
      "weights_LogR=[ -3.71732907e-03  -4.28161276e-03  -3.66892742e-03  -2.06470214e-03\n",
      "  -3.47449242e-03  -3.10570641e-03   8.69586459e-04  -3.84997499e-03\n",
      "  -2.69210940e-03  -2.86048526e-03  -3.98166104e-03   1.71770198e-03\n",
      "  -3.33820695e-03  -2.96557524e-03   3.23367404e-05  -2.57456011e-06\n",
      "  -3.74710643e-03   7.00020637e-05  -7.80315093e-05  -3.13249407e-03\n",
      "   5.82216477e-05  -3.15610491e-03  -2.43501538e-03  -3.36141819e-03\n",
      "   1.08146487e-05   3.50874527e-05  -3.92017928e-03   4.73107416e-05\n",
      "  -1.02725566e-05  -1.92338776e-03]\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "max_iters_LogR = 1000\n",
    "gamma_LogR = 1.0e-8\n",
    "loss_LogR, weights_LogR = logistic_regression(y, tX_cleaned, gamma_LogR, max_iters_LogR)\n",
    "print(\"loss_LogR={loss}\\n\".format(loss=loss_LogR))\n",
    "print(\"weights_LogR={w}\".format(w=weights_LogR[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/dataSubmission_RR.csv' \n",
    "y_pred = predict_labels(weights_RR, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/dataSubmission_LogR.csv' \n",
    "y_pred = predict_labels(weights_LogR[1000], tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
