{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT 1: Higgs Boson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the usual import needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Library only used for observing the data in a more visual way.\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the data in a pandas DataFrame to see more easily how they look like, but we are not going to use this specific library for other reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.910</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.240</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>89.744</td>\n",
       "      <td>13.550</td>\n",
       "      <td>59.149</td>\n",
       "      <td>116.344</td>\n",
       "      <td>2.636</td>\n",
       "      <td>284.584</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.362</td>\n",
       "      <td>61.619</td>\n",
       "      <td>...</td>\n",
       "      <td>2.237</td>\n",
       "      <td>282.849</td>\n",
       "      <td>3.0</td>\n",
       "      <td>90.547</td>\n",
       "      <td>-2.412</td>\n",
       "      <td>-0.653</td>\n",
       "      <td>56.165</td>\n",
       "      <td>0.224</td>\n",
       "      <td>3.106</td>\n",
       "      <td>193.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100006</th>\n",
       "      <td>1.0</td>\n",
       "      <td>148.754</td>\n",
       "      <td>28.862</td>\n",
       "      <td>107.782</td>\n",
       "      <td>106.130</td>\n",
       "      <td>0.733</td>\n",
       "      <td>158.359</td>\n",
       "      <td>0.113</td>\n",
       "      <td>2.941</td>\n",
       "      <td>2.545</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.443</td>\n",
       "      <td>294.074</td>\n",
       "      <td>2.0</td>\n",
       "      <td>123.010</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1.450</td>\n",
       "      <td>56.867</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>179.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100007</th>\n",
       "      <td>1.0</td>\n",
       "      <td>154.916</td>\n",
       "      <td>10.418</td>\n",
       "      <td>94.714</td>\n",
       "      <td>29.169</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.897</td>\n",
       "      <td>1.526</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.761</td>\n",
       "      <td>187.299</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.638</td>\n",
       "      <td>-0.715</td>\n",
       "      <td>-1.724</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>30.638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        labels        0        1        2        3        4        5        6  \\\n",
       "ids                                                                             \n",
       "100000     1.0  138.470   51.655   97.827   27.980    0.910  124.711    2.666   \n",
       "100001    -1.0  160.937   68.768  103.235   48.146 -999.000 -999.000 -999.000   \n",
       "100002    -1.0 -999.000  162.172  125.953   35.635 -999.000 -999.000 -999.000   \n",
       "100003    -1.0  143.905   81.417   80.943    0.414 -999.000 -999.000 -999.000   \n",
       "100004    -1.0  175.864   16.915  134.805   16.405 -999.000 -999.000 -999.000   \n",
       "100005    -1.0   89.744   13.550   59.149  116.344    2.636  284.584   -0.540   \n",
       "100006     1.0  148.754   28.862  107.782  106.130    0.733  158.359    0.113   \n",
       "100007     1.0  154.916   10.418   94.714   29.169 -999.000 -999.000 -999.000   \n",
       "\n",
       "            7       8   ...        20       21   22       23       24  \\\n",
       "ids                     ...                                             \n",
       "100000  3.064  41.928   ...    -0.277  258.733  2.0   67.435    2.150   \n",
       "100001  3.473   2.078   ...    -1.916  164.546  1.0   46.226    0.725   \n",
       "100002  3.148   9.336   ...    -2.186  260.414  1.0   44.251    2.053   \n",
       "100003  3.310   0.414   ...     0.060   86.062  0.0 -999.000 -999.000   \n",
       "100004  3.891  16.405   ...    -0.871   53.131  0.0 -999.000 -999.000   \n",
       "100005  1.362  61.619   ...     2.237  282.849  3.0   90.547   -2.412   \n",
       "100006  2.941   2.545   ...    -1.443  294.074  2.0  123.010    0.864   \n",
       "100007  2.897   1.526   ...    -1.761  187.299  1.0   30.638   -0.715   \n",
       "\n",
       "             25       26       27       28       29  \n",
       "ids                                                  \n",
       "100000    0.444   46.062    1.240   -2.475  113.497  \n",
       "100001    1.158 -999.000 -999.000 -999.000   46.226  \n",
       "100002   -2.028 -999.000 -999.000 -999.000   44.251  \n",
       "100003 -999.000 -999.000 -999.000 -999.000    0.000  \n",
       "100004 -999.000 -999.000 -999.000 -999.000    0.000  \n",
       "100005   -0.653   56.165    0.224    3.106  193.660  \n",
       "100006    1.450   56.867    0.131   -2.767  179.877  \n",
       "100007   -1.724 -999.000 -999.000 -999.000   30.638  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([pd.DataFrame({'labels': y, 'ids': ids}), pd.DataFrame(tX)], axis=1)\n",
    "df_indexed = df.set_index(['ids'])\n",
    "df_indexed.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed that many entries are filled by the value *-999*, which represent a lack of information. Theses values disturb our statistical calculations. Therefore it is very useful to clean the data before performing any further computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that take as input the matrix of the different features *tX* for all the samples and return the same matrix matrix after some cleaning have been performed.\n",
    "\n",
    "In order to *\"clean\"* our data, we are first going to replace the *-999* values by the mean of the features on all the sample for which we have an information. With this process, we preserve our statistical result of the influence of theses outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "\n",
    "def clean_data(tx):\n",
    "    nbrRows = tx.shape[0]\n",
    "    nbrColunms = tx.shape[1]\n",
    "    tx_temp = np.zeros((nbrRows,nbrColunms))\n",
    "    modified_columns = [False] * nbrColunms\n",
    "    \n",
    "    for columnID in range(nbrColunms):\n",
    "        currentColumn = tx[:,columnID].copy()\n",
    "        \n",
    "        # extract indices with -999 values\n",
    "        nanIndices = []\n",
    "\n",
    "        for rowID in range(nbrRows):\n",
    "            if currentColumn[rowID] == -999.000:\n",
    "                nanIndices.append(rowID)\n",
    "                modified_columns[columnID] = True\n",
    "        \n",
    "        tempColumm = np.delete(currentColumn, nanIndices, axis=0)\n",
    "\n",
    "        # replace -999 values with median\n",
    "        median = np.median(tempColumm)\n",
    "        currentColumn[nanIndices] = median\n",
    "         \n",
    "        tx_temp[:,columnID] = currentColumn\n",
    "        \n",
    "    return tx_temp, modified_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_cleaned, modified_columns = clean_data(tX)\n",
    "tX_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that we replaced correctly the values -999 with the median of each collumn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.910</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.240</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>2.107</td>\n",
       "      <td>225.885</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>47.902</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>112.406</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>2.107</td>\n",
       "      <td>225.885</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>47.902</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>2.107</td>\n",
       "      <td>225.885</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.561</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>47.902</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>2.107</td>\n",
       "      <td>225.885</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.561</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>47.902</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>89.744</td>\n",
       "      <td>13.550</td>\n",
       "      <td>59.149</td>\n",
       "      <td>116.344</td>\n",
       "      <td>2.636</td>\n",
       "      <td>284.584</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.362</td>\n",
       "      <td>61.619</td>\n",
       "      <td>...</td>\n",
       "      <td>2.237</td>\n",
       "      <td>282.849</td>\n",
       "      <td>3.0</td>\n",
       "      <td>90.547</td>\n",
       "      <td>-2.412</td>\n",
       "      <td>-0.653</td>\n",
       "      <td>56.165</td>\n",
       "      <td>0.224</td>\n",
       "      <td>3.106</td>\n",
       "      <td>193.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100006</th>\n",
       "      <td>1.0</td>\n",
       "      <td>148.754</td>\n",
       "      <td>28.862</td>\n",
       "      <td>107.782</td>\n",
       "      <td>106.130</td>\n",
       "      <td>0.733</td>\n",
       "      <td>158.359</td>\n",
       "      <td>0.113</td>\n",
       "      <td>2.941</td>\n",
       "      <td>2.545</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.443</td>\n",
       "      <td>294.074</td>\n",
       "      <td>2.0</td>\n",
       "      <td>123.010</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1.450</td>\n",
       "      <td>56.867</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>179.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100007</th>\n",
       "      <td>1.0</td>\n",
       "      <td>154.916</td>\n",
       "      <td>10.418</td>\n",
       "      <td>94.714</td>\n",
       "      <td>29.169</td>\n",
       "      <td>2.107</td>\n",
       "      <td>225.885</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>2.897</td>\n",
       "      <td>1.526</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.761</td>\n",
       "      <td>187.299</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.638</td>\n",
       "      <td>-0.715</td>\n",
       "      <td>-1.724</td>\n",
       "      <td>47.902</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>30.638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        labels        0        1        2        3      4        5      6  \\\n",
       "ids                                                                         \n",
       "100000     1.0  138.470   51.655   97.827   27.980  0.910  124.711  2.666   \n",
       "100001    -1.0  160.937   68.768  103.235   48.146  2.107  225.885 -0.244   \n",
       "100002    -1.0  112.406  162.172  125.953   35.635  2.107  225.885 -0.244   \n",
       "100003    -1.0  143.905   81.417   80.943    0.414  2.107  225.885 -0.244   \n",
       "100004    -1.0  175.864   16.915  134.805   16.405  2.107  225.885 -0.244   \n",
       "100005    -1.0   89.744   13.550   59.149  116.344  2.636  284.584 -0.540   \n",
       "100006     1.0  148.754   28.862  107.782  106.130  0.733  158.359  0.113   \n",
       "100007     1.0  154.916   10.418   94.714   29.169  2.107  225.885 -0.244   \n",
       "\n",
       "            7       8   ...        20       21   22       23     24     25  \\\n",
       "ids                     ...                                                  \n",
       "100000  3.064  41.928   ...    -0.277  258.733  2.0   67.435  2.150  0.444   \n",
       "100001  3.473   2.078   ...    -1.916  164.546  1.0   46.226  0.725  1.158   \n",
       "100002  3.148   9.336   ...    -2.186  260.414  1.0   44.251  2.053 -2.028   \n",
       "100003  3.310   0.414   ...     0.060   86.062  0.0   65.561  0.000 -0.033   \n",
       "100004  3.891  16.405   ...    -0.871   53.131  0.0   65.561  0.000 -0.033   \n",
       "100005  1.362  61.619   ...     2.237  282.849  3.0   90.547 -2.412 -0.653   \n",
       "100006  2.941   2.545   ...    -1.443  294.074  2.0  123.010  0.864  1.450   \n",
       "100007  2.897   1.526   ...    -1.761  187.299  1.0   30.638 -0.715 -1.724   \n",
       "\n",
       "            26     27     28       29  \n",
       "ids                                    \n",
       "100000  46.062  1.240 -2.475  113.497  \n",
       "100001  47.902 -0.010 -0.002   46.226  \n",
       "100002  47.902 -0.010 -0.002   44.251  \n",
       "100003  47.902 -0.010 -0.002    0.000  \n",
       "100004  47.902 -0.010 -0.002    0.000  \n",
       "100005  56.165  0.224  3.106  193.660  \n",
       "100006  56.867  0.131 -2.767  179.877  \n",
       "100007  47.902 -0.010 -0.002   30.638  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = pd.concat([pd.DataFrame({'labels': y, 'ids': ids}), pd.DataFrame(tX_cleaned)], axis=1)\n",
    "df_cleaned_indexed = df_cleaned.set_index(['ids'])\n",
    "df_cleaned_indexed.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can aslo check which columns have been filled and which ones have not been modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also standardize our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalized data set\n",
    "tX_stand, mean_training, std_training = standardize(tX_cleaned)\n",
    "tX_stand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.341522</td>\n",
       "      <td>0.068332</td>\n",
       "      <td>0.407680</td>\n",
       "      <td>-0.469966</td>\n",
       "      <td>-1.353339</td>\n",
       "      <td>-0.640013</td>\n",
       "      <td>1.579473</td>\n",
       "      <td>0.882478</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147267</td>\n",
       "      <td>0.386847</td>\n",
       "      <td>1.044402</td>\n",
       "      <td>-0.202115</td>\n",
       "      <td>1.556350</td>\n",
       "      <td>0.330677</td>\n",
       "      <td>-0.262878</td>\n",
       "      <td>1.142622</td>\n",
       "      <td>-2.526840</td>\n",
       "      <td>0.412510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.766558</td>\n",
       "      <td>0.552505</td>\n",
       "      <td>0.540136</td>\n",
       "      <td>-0.153167</td>\n",
       "      <td>-0.090817</td>\n",
       "      <td>-0.188805</td>\n",
       "      <td>0.086029</td>\n",
       "      <td>1.404888</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.051683</td>\n",
       "      <td>-0.357719</td>\n",
       "      <td>0.021305</td>\n",
       "      <td>-0.644524</td>\n",
       "      <td>0.525758</td>\n",
       "      <td>0.838833</td>\n",
       "      <td>-0.159461</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.273820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.151562</td>\n",
       "      <td>3.195156</td>\n",
       "      <td>1.096560</td>\n",
       "      <td>-0.349710</td>\n",
       "      <td>-0.090817</td>\n",
       "      <td>-0.188805</td>\n",
       "      <td>0.086029</td>\n",
       "      <td>0.989770</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.200672</td>\n",
       "      <td>0.400135</td>\n",
       "      <td>0.021305</td>\n",
       "      <td>-0.685722</td>\n",
       "      <td>1.486197</td>\n",
       "      <td>-1.428652</td>\n",
       "      <td>-0.159461</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.293970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444343</td>\n",
       "      <td>0.910379</td>\n",
       "      <td>-0.005853</td>\n",
       "      <td>-0.903016</td>\n",
       "      <td>-0.090817</td>\n",
       "      <td>-0.188805</td>\n",
       "      <td>0.086029</td>\n",
       "      <td>1.196690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038692</td>\n",
       "      <td>-0.978149</td>\n",
       "      <td>-1.001792</td>\n",
       "      <td>-0.241206</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>-0.008805</td>\n",
       "      <td>-0.159461</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.745439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.048950</td>\n",
       "      <td>-0.914556</td>\n",
       "      <td>1.313369</td>\n",
       "      <td>-0.651804</td>\n",
       "      <td>-0.090817</td>\n",
       "      <td>-0.188805</td>\n",
       "      <td>0.086029</td>\n",
       "      <td>1.938794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.475042</td>\n",
       "      <td>-1.238475</td>\n",
       "      <td>-1.001792</td>\n",
       "      <td>-0.241206</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>-0.008805</td>\n",
       "      <td>-0.159461</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.745439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.580287</td>\n",
       "      <td>-1.009761</td>\n",
       "      <td>-0.539646</td>\n",
       "      <td>0.918192</td>\n",
       "      <td>0.467139</td>\n",
       "      <td>0.072976</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-1.291464</td>\n",
       "      <td>...</td>\n",
       "      <td>1.239982</td>\n",
       "      <td>0.577488</td>\n",
       "      <td>2.067499</td>\n",
       "      <td>0.279989</td>\n",
       "      <td>-1.742991</td>\n",
       "      <td>-0.450060</td>\n",
       "      <td>0.304958</td>\n",
       "      <td>0.214296</td>\n",
       "      <td>3.175385</td>\n",
       "      <td>1.230371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100006</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.536077</td>\n",
       "      <td>-0.576543</td>\n",
       "      <td>0.651504</td>\n",
       "      <td>0.757735</td>\n",
       "      <td>-1.540028</td>\n",
       "      <td>-0.489952</td>\n",
       "      <td>0.269245</td>\n",
       "      <td>0.725371</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.790677</td>\n",
       "      <td>0.666224</td>\n",
       "      <td>1.044402</td>\n",
       "      <td>0.957151</td>\n",
       "      <td>0.626286</td>\n",
       "      <td>1.046650</td>\n",
       "      <td>0.344414</td>\n",
       "      <td>0.129322</td>\n",
       "      <td>-2.825182</td>\n",
       "      <td>1.089751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100007</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.652651</td>\n",
       "      <td>-1.098374</td>\n",
       "      <td>0.331435</td>\n",
       "      <td>-0.451288</td>\n",
       "      <td>-0.090817</td>\n",
       "      <td>-0.188805</td>\n",
       "      <td>0.086029</td>\n",
       "      <td>0.669171</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.966153</td>\n",
       "      <td>-0.177852</td>\n",
       "      <td>0.021305</td>\n",
       "      <td>-0.969682</td>\n",
       "      <td>-0.515682</td>\n",
       "      <td>-1.212294</td>\n",
       "      <td>-0.159461</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.432856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        labels    0         1         2         3         4         5  \\\n",
       "ids                                                                     \n",
       "100000     1.0  1.0  0.341522  0.068332  0.407680 -0.469966 -1.353339   \n",
       "100001    -1.0  1.0  0.766558  0.552505  0.540136 -0.153167 -0.090817   \n",
       "100002    -1.0  1.0 -0.151562  3.195156  1.096560 -0.349710 -0.090817   \n",
       "100003    -1.0  1.0  0.444343  0.910379 -0.005853 -0.903016 -0.090817   \n",
       "100004    -1.0  1.0  1.048950 -0.914556  1.313369 -0.651804 -0.090817   \n",
       "100005    -1.0  1.0 -0.580287 -1.009761 -0.539646  0.918192  0.467139   \n",
       "100006     1.0  1.0  0.536077 -0.576543  0.651504  0.757735 -1.540028   \n",
       "100007     1.0  1.0  0.652651 -1.098374  0.331435 -0.451288 -0.090817   \n",
       "\n",
       "               6         7         8    ...           21        22        23  \\\n",
       "ids                                     ...                                    \n",
       "100000 -0.640013  1.579473  0.882478    ...    -0.147267  0.386847  1.044402   \n",
       "100001 -0.188805  0.086029  1.404888    ...    -1.051683 -0.357719  0.021305   \n",
       "100002 -0.188805  0.086029  0.989770    ...    -1.200672  0.400135  0.021305   \n",
       "100003 -0.188805  0.086029  1.196690    ...     0.038692 -0.978149 -1.001792   \n",
       "100004 -0.188805  0.086029  1.938794    ...    -0.475042 -1.238475 -1.001792   \n",
       "100005  0.072976 -0.065881 -1.291464    ...     1.239982  0.577488  2.067499   \n",
       "100006 -0.489952  0.269245  0.725371    ...    -0.790677  0.666224  1.044402   \n",
       "100007 -0.188805  0.086029  0.669171    ...    -0.966153 -0.177852  0.021305   \n",
       "\n",
       "              24        25        26        27        28        29        30  \n",
       "ids                                                                           \n",
       "100000 -0.202115  1.556350  0.330677 -0.262878  1.142622 -2.526840  0.412510  \n",
       "100001 -0.644524  0.525758  0.838833 -0.159461  0.000489 -0.000124 -0.273820  \n",
       "100002 -0.685722  1.486197 -1.428652 -0.159461  0.000489 -0.000124 -0.293970  \n",
       "100003 -0.241206  0.001422 -0.008805 -0.159461  0.000489 -0.000124 -0.745439  \n",
       "100004 -0.241206  0.001422 -0.008805 -0.159461  0.000489 -0.000124 -0.745439  \n",
       "100005  0.279989 -1.742991 -0.450060  0.304958  0.214296  3.175385  1.230371  \n",
       "100006  0.957151  0.626286  1.046650  0.344414  0.129322 -2.825182  1.089751  \n",
       "100007 -0.969682 -0.515682 -1.212294 -0.159461  0.000489 -0.000124 -0.432856  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stand = pd.concat([pd.DataFrame({'labels': y, 'ids': ids}), pd.DataFrame(tX_stand)], axis=1)\n",
    "df_stand_indexed = df_stand.set_index(['ids'])\n",
    "df_stand_indexed.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(y, x, seed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # set mask\n",
    "    ratio = 0.7\n",
    "    msk = np.random.rand(len(y)) < ratio\n",
    "    \n",
    "    # training data set\n",
    "    x_tr = x[msk]\n",
    "    y_tr = y[msk]\n",
    "    \n",
    "    # test data set\n",
    "    x_test = x[~msk]\n",
    "    y_test = y[~msk]\n",
    "    \n",
    "    return x_tr, x_test, y_tr, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prediction(y, tX, gamma, lambda_, max_iters, method):\n",
    "    # split data\n",
    "    x_tr, x_test, y_tr, y_test = split_data(y, tX)\n",
    "    \n",
    "    # training\n",
    "    loss = 0\n",
    "    weights = []\n",
    "    if method == 1:\n",
    "        loss, weights = least_squares_GD(y_tr, x_tr, gamma, max_iters)\n",
    "    elif method == 2:\n",
    "        loss, weights = least_squares_SGD(y_tr, x_tr, gamma, max_iters)\n",
    "    elif method == 3:\n",
    "        loss, weights = least_squares(y_tr, x_tr)\n",
    "    elif method == 4:\n",
    "        loss, weights = ridge_regression(y_tr, x_tr, lambda_)\n",
    "    elif method == 5:\n",
    "        loss, weights = logistic_regression(y_tr, x_tr, gamma, max_iters)\n",
    "    else:\n",
    "        loss, weights = reg_logistic_regression(y_tr, x_tr, lambda_, gamma, max_iters)\n",
    "        \n",
    "    # compute prediction\n",
    "    y_pred = predict_labels(weights, x_test)    \n",
    "    \n",
    "    # accuracy of the prediction\n",
    "    N = y_test.shape[0]\n",
    "    pred = np.sum(y_pred == y_test)/N\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of ML methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression - gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from costs import *\n",
    "\n",
    "def compute_gradient(y, tX, w):\n",
    "    # error\n",
    "    e = y - tX.dot(w)\n",
    "    \n",
    "    # gradient \n",
    "    N=y.shape[0]\n",
    "    gradient = - np.transpose(tX).dot(e)/N\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tX, gamma, max_iters):\n",
    "    # init parameters\n",
    "    threshold = 1e-10\n",
    "    w_init = np.zeros(tX.shape[1])\n",
    "    ws = [w_init]\n",
    "    w_temp = w_init\n",
    "    losses = [8000]\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        grad = compute_gradient(y, tX, w_temp)\n",
    "        loss = compute_loss(y, tX, w_temp)\n",
    "        \n",
    "        # update w by gradient\n",
    "        w_temp -= gamma*grad\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w_temp))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # converge criteria\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "\n",
    "    return losses, ws[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression - stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tX, w):\n",
    "    B = 35 # size of the batch\n",
    "    sum = 0\n",
    "    for minibatch_y, minibatch_tX in batch_iter(y, tX, B):\n",
    "        sum += compute_gradient(minibatch_y, minibatch_tX, w)\n",
    "\n",
    "    return sum / B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tX, gamma, max_iters):    \n",
    "    # init parameters\n",
    "    threshold = 1e-8\n",
    "    w_init = np.zeros(tX.shape[1])\n",
    "    ws = [w_init]\n",
    "    w_temp = w_init\n",
    "    losses = [8000]\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        grad = compute_stoch_gradient(y, tX, w_temp)\n",
    "        loss = compute_loss(y, tX, w_temp)\n",
    "\n",
    "        # update w by gradient\n",
    "        w_temp -= gamma*grad\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w_temp))\n",
    "        losses.append(loss)\n",
    "        \n",
    "    # converge criteria\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "        \n",
    "    return losses, ws[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tX):\n",
    "    # Compute optimum weight\n",
    "    tX_transpose = np.transpose(tX)\n",
    "    A = tX_transpose.dot(tX)\n",
    "    b = tX_transpose.dot(y)\n",
    "    w_opt = np.linalg.solve(A,b)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = compute_loss(y, tX, w_opt)\n",
    "        \n",
    "    return loss, w_opt # returns loss, and optimal weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    poly = np.ones(x.shape)\n",
    "    \n",
    "    for m in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, m)]\n",
    "    \n",
    "    #poly = np.c_[poly, np.absolute(np.sqrt(x))]\n",
    "   \n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tX_poly, lambda_):    \n",
    "    # Initiation variables\n",
    "    lamb_ = 2*len(y)*lambda_\n",
    "    \n",
    "    # Compute optimum weight\n",
    "    A = np.dot(np.transpose(tX_poly), tX_poly) + lamb_*np.eye(tX_poly.shape[1])\n",
    "    b = np.transpose(tX_poly).dot(y)\n",
    "    w_opt = np.linalg.solve(A,b)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = compute_loss(y, tX_poly, w_opt)\n",
    "    \n",
    "    return loss, w_opt # returns mse, and optimal weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    temp = 1+np.exp(-t)\n",
    "    return 1/(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tX, w, gamma, lambda_):\n",
    "    # Initiation variables\n",
    "    lamb_ = 2*len(y)*lambda_\n",
    "    \n",
    "    # compute the loss\n",
    "    N = tX.shape[0]\n",
    "    l1 = tX.dot(w) + np.log(np.ones((N))+np.exp(-tX.dot(w)))\n",
    "    l2 = y*(tX.dot(w))\n",
    "    loss = (np.ones((1,N)).dot(l1-l2))[0]\n",
    "    \n",
    "    # compute the gradient\n",
    "    grad = np.transpose(tX).dot(sigmoid(tX.dot(w))-y) + lamb_*w.dot(w)\n",
    "    \n",
    "    # update w\n",
    "    w -= gamma*grad\n",
    "\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tX, gamma, max_iters):\n",
    "    # init parameters\n",
    "    threshold = 1e-8\n",
    "    w_temp = np.zeros(tX.shape[1]) # initialization of the weight\n",
    "    ws = [w_temp]\n",
    "    losses = [8000]\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iters):        \n",
    "        # get loss and update w.\n",
    "        loss, w_temp = learning_by_gradient_descent(y, tX, w_temp, gamma, 0)\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w_temp))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # converge criteria\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    \n",
    "    return losses, ws[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tX, gamma, lambda_, max_iters):\n",
    "    # init parameters\n",
    "    threshold = 1e-8\n",
    "    w_temp = np.zeros(tX.shape[1]) # initialization of the weight\n",
    "    ws = [w_temp]\n",
    "    losses = [8000]\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iters):        \n",
    "        # get loss and update w.\n",
    "        loss, w_temp = learning_by_gradient_descent(y, tX, w_temp, gamma, lambda_)\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w_temp))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # converge criteria\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    \n",
    "    return losses, ws[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = len(y)\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    \n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train:\n",
    "    x_test = x[k_indices[k]]\n",
    "    y_test = y[k_indices[k]]\n",
    "    \n",
    "    tr_indices = np.delete(k_indices, k, axis=0)\n",
    "    x_tr = np.delete(x, k, axis=0)\n",
    "    y_tr = np.delete(y, k, axis=0)\n",
    "    \n",
    "    # form train and test data with polynomial basis function\n",
    "    poly_x_tr = build_poly(x_tr, degree)\n",
    "    poly_x_test = build_poly(x_test, degree)\n",
    "    \n",
    "    # calcualte weight and loss through least square.\n",
    "    loss_tr, weight_tr = ridge_regression(y_tr, poly_x_tr, lambda_)\n",
    "    loss_test, weight_te = ridge_regression(y_test, poly_x_test, lambda_)\n",
    "    \n",
    "    return loss_tr, loss_te, weight_tr, weight_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on half of our training set and test the model on the other half:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's divide our data in two part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:3: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  app.launch_new_instance()\n",
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:4: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:6: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:7: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/numpy/core/numeric.py:190: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  a = empty(shape, dtype, order)\n",
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:13: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:14: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "N = y.shape[0]\n",
    "\n",
    "y1 = y[:N/2].copy()\n",
    "y2 = y[N/2:].copy()\n",
    "\n",
    "tX1 = tX_cleaned[:N/2, :].copy()\n",
    "tX2 = tX_cleaned[N/2:, :].copy()\n",
    "\n",
    "w0 = np.ones([1,N/2])\n",
    "tX1 = np.insert(tX1, 0, w0, axis=1)\n",
    "tX2 = np.insert(tX2, 0, w0, axis=1)\n",
    "\n",
    "ids1 = ids[:N/2].copy()\n",
    "ids2 = ids[N/2:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression - gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights_GD:\n",
      " [ -7.02181144e-06  -6.76675669e-04  -8.97777621e-04  -5.23192467e-04\n",
      "   2.57001876e-04  -7.23174466e-06   2.64585418e-04  -1.25179752e-05\n",
      "  -1.61558759e-05  -1.24443507e-04  -5.14468323e-05  -1.73979905e-05\n",
      "   1.66560497e-05  -1.55670651e-06   7.19400927e-06  -7.40805407e-08\n",
      "  -2.81217008e-07  -3.21900711e-04   3.46775238e-07  -1.22571171e-07\n",
      "  -2.17271338e-04   7.12554648e-07  -4.05208168e-04   3.29750958e-07\n",
      "  -2.11976201e-04  -1.71803251e-07   1.87877644e-07  -3.17364813e-04\n",
      "   1.33344399e-07  -2.20120610e-07   2.63259972e-04] \n",
      "\n",
      "pred_GD =  0.670099800399\n"
     ]
    }
   ],
   "source": [
    "max_iters_GD_test = 2000\n",
    "gamma_GD_test = 1.0e-8\n",
    "method = 1\n",
    "\n",
    "loss_GD_test, weights_GD_test = least_squares_GD(y1, tX1, gamma_GD_test, max_iters_GD_test)\n",
    "pred_GD_test = prediction(y1, tX1, gamma_GD_test, 0, max_iters_GD_test, method)\n",
    "\n",
    "print(\"\\nweights_GD:\\n\",weights_GD_test,\"\\n\")\n",
    "print(\"pred_GD = \", pred_GD_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.0248"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_GD_test = predict_labels(weights_GD_test, tX2)\n",
    "n_correct_GD = np.count_nonzero((y_pred_GD_test - y2) == 0)\n",
    "perc_correct_GD = n_correct_GD/(N/2) * 100\n",
    "perc_correct_GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression - stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights_SGD:\n",
      " [ -3.75046095e-05   1.40705033e-04  -6.44799833e-03  -4.50093212e-04\n",
      "   2.12750846e-03  -1.85823047e-05   6.30237753e-04  -9.42820546e-05\n",
      "  -2.04482715e-05  -8.98834444e-04   8.45356531e-04  -1.46466146e-04\n",
      "   1.71564204e-04   3.27632594e-06   2.17281745e-03  -3.22683260e-06\n",
      "  -8.82613457e-06  -1.20860000e-03   2.20461637e-06   2.63886386e-06\n",
      "  -1.05067113e-03   6.44749522e-06  -6.17323081e-04  -9.37945950e-06\n",
      "  -1.37941055e-03  -6.67551307e-07   3.06851103e-07  -2.13627821e-03\n",
      "   7.14567462e-07  -2.98916776e-06  -1.18859167e-04] \n",
      "\n",
      "pred_SGD =  0.69623419827\n"
     ]
    }
   ],
   "source": [
    "max_iters_SGD_test = 500\n",
    "gamma_SGD_test = 1.0e-8\n",
    "method = 2\n",
    "\n",
    "loss_SGD_test, weights_SGD_test = least_squares_SGD(y1, tX1, gamma_SGD_test, max_iters_SGD_test)\n",
    "print(\"\\nweights_SGD:\\n\",weights_SGD_test,\"\\n\")\n",
    "\n",
    "pred_SGD_test = prediction(y1, tX1, gamma_SGD_test, 0, max_iters_SGD_test, method)\n",
    "print(\"pred_SGD = \", pred_SGD_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.2272"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_SGD_test = predict_labels(weights_SGD_test, tX2)\n",
    "n_correct_SGD = np.count_nonzero((y_pred_SGD_test - y2) == 0)\n",
    "perc_correct_SGD = n_correct_SGD/(N/2) * 100\n",
    "perc_correct_SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights_LeastS:\n",
      " [ -1.16074033e+00   1.44296347e-04  -7.13890244e-03  -6.35632610e-03\n",
      "   1.18983305e-04   1.66537567e-02   4.56475343e-04   2.70567148e-03\n",
      "   3.54887909e-01  -1.19904231e-03  -4.22515836e+00  -2.21232683e-01\n",
      "   1.02368577e-01   3.52579962e-01   4.23358426e+00  -3.27685587e-03\n",
      "  -1.63291643e-03   4.23825620e+00   1.63545099e-03   2.70568868e-04\n",
      "   3.04571909e-03  -3.81839093e-04  -3.77901455e-04   4.71385100e-02\n",
      "  -8.23199214e-04   1.15058943e-03  -2.02148685e-03  -1.23809562e-03\n",
      "  -3.37838817e-04  -3.15646992e-03   4.22484938e+00] \n",
      "\n",
      "pred_LeastS =  0.747624750499\n"
     ]
    }
   ],
   "source": [
    "method = 3\n",
    "\n",
    "loss_LeastS_test, weights_LeastS_test = least_squares(y1, tX1)\n",
    "print(\"\\nweights_LeastS:\\n\",weights_LeastS_test,\"\\n\")\n",
    "\n",
    "pred_LeastS_test = prediction(y1, tX1, 0, 0, 0, method)\n",
    "print(\"pred_LeastS = \", pred_LeastS_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.4256"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_LS_test = predict_labels(weights_LeastS_test, tX2)\n",
    "n_correct_LS = np.count_nonzero((y_pred_LS_test - y2) == 0)\n",
    "perc_correct_LS = n_correct_LS/(N/2) * 100\n",
    "perc_correct_LS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights_RR:\n",
      " [ -4.85618666e+08   1.46604323e+09   2.45597904e+06  -8.30305735e+08\n",
      "  -6.18084754e+07  -9.67732014e+07   1.84997277e+07  -1.24928535e+07\n",
      "  -2.70981877e-04  -2.70981877e-04  -2.70981877e-04  -2.70981877e-04\n",
      "  -2.70981877e-04  -2.70981877e-04  -2.70981877e-04  -2.70981877e-04\n",
      "  -2.70981877e-04  -2.70981877e-04  -2.70981877e-04  -2.70981877e-04\n",
      "  -2.70981877e-04  -2.70981877e-04  -2.70981877e-04  -2.70981877e-04\n",
      "  -2.70982966e-04  -2.70982966e-04  -2.70982966e-04  -2.70982966e-04\n",
      "  -2.70982966e-04  -2.70982966e-04  -2.70982966e-04  -2.70982966e-04\n",
      "  -7.68945536e-03  -2.08207528e-03  -4.79139626e-03  -3.42676624e-03\n",
      "  -4.26766364e-04  -8.74550129e-03  -7.18849120e-04   1.26432434e-04\n",
      "  -2.33864898e-04  -7.20819274e-03  -4.46489216e-04   1.00147101e-03\n",
      "   7.11221470e-04  -2.30821056e-03  -1.38912141e-04  -9.67043325e-05\n",
      "  -2.65842173e-03   1.74589155e-05   6.76899702e-05  -4.99416690e-03\n",
      "   2.52089926e-05  -6.76571641e-03  -1.62681692e-04  -3.78544631e-03\n",
      "  -1.50592983e-04   4.99741696e-05  -3.33356446e-03   1.72821195e-05\n",
      "  -6.48371365e-05  -2.24628878e-03  -2.70982480e-04  -8.96089713e-04\n",
      "   2.15396596e-04   6.28824442e-04   7.42338694e-05  -5.81077756e-04\n",
      "   3.51774576e-05   5.44383444e-05   6.44469067e-04  -4.19804495e-05\n",
      "  -1.44264177e-04  -8.39163817e-04   3.97810226e-04   8.22236951e-04\n",
      "  -2.94587886e-04  -5.72466892e-04  -3.22657283e-04  -1.15015763e-03\n",
      "  -1.16894055e-03  -3.18029488e-04   8.05629010e-05  -3.37982165e-04\n",
      "   7.46966151e-05  -1.79143855e-04  -1.06593410e-06   1.21109563e-03\n",
      "  -1.80736581e-04  -4.40639461e-03   6.28708310e-04  -1.33054966e-04\n",
      "   3.80499421e-04  -2.70983351e-04   2.18967331e-05  -1.34804393e-05\n",
      "  -1.15203322e-05  -6.72818803e-07  -5.71477941e-04  -7.21826583e-08\n",
      "  -1.92112431e-03   1.30735536e-03   7.51445780e-07   1.54333027e-06\n",
      "  -1.23002675e-03   1.46100772e-03   8.27984981e-04   5.16089862e-05\n",
      "  -2.28716873e-04  -1.20903168e-04   5.83849940e-05  -1.99365975e-05\n",
      "   1.29979804e-04  -3.58869577e-07  -1.37045159e-05  -3.09372894e-07\n",
      "  -2.11086422e-04   4.14243966e-07  -2.53191084e-04   1.05801131e-04\n",
      "   1.51923092e-04   3.65041493e-06  -1.08633628e-04  -3.53456916e-06\n",
      "  -2.70983467e-04  -2.01659289e-07   2.23026684e-07   8.84381218e-08\n",
      "   3.36530807e-09  -1.09205411e-04   8.74396924e-11   4.70628831e-05\n",
      "   1.96345421e-03  -3.90381786e-09  -8.10092415e-09  -8.92513072e-04\n",
      "   5.67776417e-04   8.04651114e-04  -1.42051745e-06  -1.11244137e-03\n",
      "  -3.88834565e-04  -1.11025034e-06  -2.06473293e-03  -4.00767575e-04\n",
      "   3.11882565e-11  -4.60979179e-04   3.50631274e-10  -2.72014502e-04\n",
      "  -6.27029586e-09   2.94378672e-03  -3.08619062e-04  -2.25128818e-06\n",
      "   1.53916780e-03  -2.89342124e-04   1.78901244e-08  -2.70983466e-04\n",
      "   9.64128910e-10  -1.72259553e-09  -3.61591435e-10  -9.29732853e-12\n",
      "   9.25604196e-04  -6.66973675e-14   2.58957581e-05   2.08197437e-03\n",
      "   6.58708549e-12   2.38413408e-11   7.91632190e-04   1.88567345e-03\n",
      "   7.73912574e-04   1.83479590e-08  -2.04496857e-04  -2.11898735e-05\n",
      "   1.12820753e-08  -7.14088363e-05   1.96113402e-04   3.52145528e-12\n",
      "  -1.43389249e-04   1.06081245e-12  -3.84969751e-04   4.48626777e-11\n",
      "  -5.59516767e-05   1.92707106e-04   1.82048494e-08  -6.15277788e-05\n",
      "  -1.75237460e-04  -5.45355104e-11  -2.70983466e-04  -2.66723493e-12\n",
      "   7.35651661e-12   8.63817142e-13   1.32245314e-14   1.41925622e-03\n",
      "   3.28432784e-17  -1.59200298e-06   8.26508492e-04  -1.04021080e-15\n",
      "  -4.20101722e-14  -2.25789592e-04   6.71036527e-04   7.42754253e-04\n",
      "  -1.32833990e-10  -1.49165316e-03  -2.38988147e-04  -6.74204983e-11\n",
      "  -2.57652763e-03  -1.75381754e-04  -7.28451489e-15  -2.80725367e-04\n",
      "  -4.10059929e-15  -5.84195422e-04  -1.77696727e-13   8.60285583e-04\n",
      "  -3.64400729e-04  -8.72690085e-11   5.80358101e-04  -4.21731403e-04\n",
      "   1.03770751e-13  -2.70983466e-04   4.42631153e-15  -1.84044461e-14\n",
      "  -1.24302973e-15  -7.72973332e-18  -8.90546930e-04  -1.03924958e-20\n",
      "  -8.47977415e-08  -1.32053015e-03   1.94078225e-18   4.52340149e-17\n",
      "   3.19183849e-05   2.16862367e-03   7.13515703e-04   5.68681769e-13\n",
      "   1.80548030e-04   1.59012893e-05   2.43377418e-13  -3.77595379e-05\n",
      "  -6.15189487e-05   2.94841389e-18   4.56482800e-05   5.92771774e-18\n",
      "  -9.03136950e-04   4.08095501e-16   1.17473792e-05  -1.39070431e-04\n",
      "   2.54139359e-13   9.01652654e-06   2.66381091e-05  -1.23474439e-16\n",
      "  -2.70983466e-04  -4.34329342e-18   2.68442616e-17   1.05940986e-18\n",
      "   2.89472990e-23   1.70691298e-04   2.03351157e-24   9.17186286e-09\n",
      "   3.49633855e-04  -8.97325195e-21  -2.91086541e-20  -2.39688933e-06\n",
      "   6.96894830e-04   6.86808147e-04  -1.42805782e-15  -7.59560512e-05\n",
      "   4.87032774e-05  -5.20461234e-16  -5.85045551e-05   4.89768145e-05\n",
      "  -6.50665915e-22   7.59861969e-05  -4.42903776e-21  -1.30089222e-03\n",
      "  -5.38082767e-19  -1.18547222e-04   7.40939197e-05  -4.40564609e-16\n",
      "  -7.13097909e-05   6.94083674e-05   8.88787338e-20  -2.70983466e-04\n",
      "   2.32110750e-21  -2.11291284e-20  -4.91954734e-22   4.36921756e-25\n",
      "  -1.24447410e-05  -2.23081711e-28   1.64961698e-11  -2.59035472e-05\n",
      "  -2.84325314e-24   1.02633031e-23   9.10615859e-08   1.86213577e-03\n",
      "   6.62623168e-04   1.94303235e-18  -3.10760228e-05  -1.57962520e-06\n",
      "   6.05729786e-19   7.05334514e-06   3.79655803e-06   3.67540569e-24\n",
      "  -3.26760686e-06   1.69269822e-24  -1.37404361e-03   3.76128572e-22\n",
      "  -3.54313415e-07   1.29086708e-05   4.17438500e-19  -3.27258845e-07\n",
      "  -6.48670748e-07  -3.52507193e-23  -2.70983466e-04  -5.20373431e-25\n",
      "   6.93196208e-24   9.58094000e-26   4.51071977e-28   2.50573753e-07\n",
      "   1.04523048e-32  -1.27513017e-11  -2.91416084e-07  -1.47613663e-27\n",
      "  -1.52262648e-27  -1.37312580e-09   5.64142821e-04   6.40740620e-04\n",
      "  -1.10507644e-21   4.66072780e-05  -2.19658667e-06  -2.95158731e-22\n",
      "   7.22459324e-05  -3.09928667e-06   1.73865398e-27  -4.60177068e-06\n",
      "  -2.61363318e-28   6.31542568e-04  -1.07518402e-25   3.62801213e-06\n",
      "  -3.75017976e-06  -1.66221952e-22   2.01407833e-06  -2.43896495e-06\n",
      "   5.89203715e-27] \n",
      "\n",
      "pred_RR for degree  10  =  0.809394544245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80.4136"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree_RR_test = 10\n",
    "\n",
    "tX1_poly = build_poly(tX1, degree_RR_test)\n",
    "tX2_poly = build_poly(tX2, degree_RR_test)\n",
    "\n",
    "lambda_RR_test = 5\n",
    "method = 4\n",
    "\n",
    "loss_RR_test, weights_RR_test = ridge_regression(y1, tX1_poly, lambda_RR_test)\n",
    "print(\"\\nweights_RR:\\n\",weights_RR_test,\"\\n\")\n",
    "\n",
    "pred_RR_test = prediction(y1, tX1_poly, 0, lambda_RR_test, 0, method)\n",
    "print(\"pred_RR for degree \", str(degree_RR_test), \" = \", pred_RR_test)\n",
    "\n",
    "\n",
    "y_pred_RR_test = predict_labels(weights_RR_test, tX2_poly)\n",
    "n_correct_RR = np.count_nonzero((y_pred_RR_test - y2) == 0)\n",
    "perc_correct_RR = n_correct_RR/(N/2) * 100\n",
    "perc_correct_RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_RR for degree  0  =  0.657165668663\n",
      "pred_RR on testing set for degree  0  =  65.6112 %\n",
      "pred_RR for degree  1  =  0.733546240852\n",
      "pred_RR on testing set for degree  1  =  73.0224 %\n",
      "pred_RR for degree  2  =  0.74876912841\n",
      "pred_RR on testing set for degree  2  =  74.6928 %\n",
      "pred_RR for degree  3  =  0.759015302728\n",
      "pred_RR on testing set for degree  3  =  75.7112 %\n",
      "pred_RR for degree  4  =  0.768622754491\n",
      "pred_RR on testing set for degree  4  =  76.80799999999999 %\n",
      "pred_RR for degree  5  =  0.77870924817\n",
      "pred_RR on testing set for degree  5  =  77.9576 %\n",
      "pred_RR for degree  6  =  0.791137724551\n",
      "pred_RR on testing set for degree  6  =  79.1112 %\n",
      "pred_RR for degree  7  =  0.480958083832\n",
      "pred_RR on testing set for degree  7  =  71.1 %\n",
      "pred_RR for degree  8  =  0.801809713906\n",
      "pred_RR on testing set for degree  8  =  80.0224 %\n",
      "pred_RR for degree  9  =  0.671856287425\n",
      "pred_RR on testing set for degree  9  =  75.0232 %\n",
      "pred_RR for degree  10  =  0.809394544245\n",
      "pred_RR on testing set for degree  10  =  80.4136 %\n",
      "pred_RR for degree  11  =  0.643459747172\n",
      "pred_RR on testing set for degree  11  =  81.2832 %\n",
      "pred_RR for degree  12  =  0.815089820359\n",
      "pred_RR on testing set for degree  12  =  81.444 %\n",
      "pred_RR for degree  13  =  0.814770459082\n",
      "pred_RR on testing set for degree  13  =  81.2696 %\n",
      "pred_RR for degree  14  =  0.795821689953\n",
      "pred_RR on testing set for degree  14  =  57.2832 %\n",
      "pred_RR for degree  15  =  0.783206919494\n",
      "pred_RR on testing set for degree  15  =  76.16159999999999 %\n",
      "pred_RR for degree  16  =  0.804843646041\n",
      "pred_RR on testing set for degree  16  =  65.8256 %\n",
      "pred_RR for degree  17  =  0.794011976048\n",
      "pred_RR on testing set for degree  17  =  76.14800000000001 %\n",
      "pred_RR for degree  18  =  0.632787757818\n",
      "pred_RR on testing set for degree  18  =  56.90880000000001 %\n",
      "pred_RR for degree  19  =  0.614291417166\n",
      "pred_RR on testing set for degree  19  =  76.6048 %\n"
     ]
    }
   ],
   "source": [
    "pred_RR_test_VEC = np.zeros(20)\n",
    "perc_correct_RR_VEC = np.zeros(20)\n",
    "\n",
    "degrees = range(0,20)\n",
    "\n",
    "for degree_RR_test in degrees:\n",
    "\n",
    "    tX1_poly = build_poly(tX1, degree_RR_test)\n",
    "    tX2_poly = build_poly(tX2, degree_RR_test)\n",
    "\n",
    "    lambda_RR_test = 5\n",
    "    method = 4\n",
    "\n",
    "    loss_RR_test, weights_RR_test = ridge_regression(y1, tX1_poly, lambda_RR_test)\n",
    "\n",
    "    pred_RR_test = prediction(y1, tX1_poly, 0, lambda_RR_test, 0, method)\n",
    "    print(\"pred_RR for degree \", str(degree_RR_test), \" = \", pred_RR_test)\n",
    "\n",
    "    pred_RR_test_VEC[degree_RR_test] = pred_RR_test*100\n",
    "\n",
    "    y_pred_RR_test = predict_labels(weights_RR_test, tX2_poly)\n",
    "    n_correct_RR = np.count_nonzero((y_pred_RR_test - y2) == 0)\n",
    "    \n",
    "    perc_correct_RR = n_correct_RR/(N/2) * 100\n",
    "    \n",
    "    perc_correct_RR_VEC[degree_RR_test] = perc_correct_RR\n",
    "    \n",
    "    print(\"pred_RR on testing set for degree \", str(degree_RR_test), \" = \", perc_correct_RR, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAEZCAYAAACn/z6eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x192e3cf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = plt.plot(degrees, pred_RR_test_VEC, degrees, perc_correct_RR_VEC)\n",
    "plt.title(\"Percentage of correct prediction in function of polynomial degree\") \n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('Correct prediction (%)')\n",
    "plt.legend(p, [\"On training set\", \"On testing set\"], bbox_to_anchor=(1.5, 1))\n",
    "plt.ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that with a 7 or more degree polynomial, we are overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights_LogR:\n",
      " [ -7.93971123e-03  -9.25427112e-01  -6.87095681e-01  -6.61222434e-01\n",
      "  -1.67225941e-01  -1.29103044e-02  -1.00695125e+00  -5.13309675e-03\n",
      "  -1.86704410e-02  -1.57261061e-01  -8.31681534e-01  -1.53710195e-02\n",
      "   8.75789549e-03  -2.72906766e-03  -1.81113730e-01   7.07021183e-06\n",
      "  -9.30049190e-05  -3.87601218e-01   2.48637077e-04  -2.40084245e-04\n",
      "  -3.13451695e-01   4.10680696e-04  -1.25211521e+00  -4.67453228e-03\n",
      "  -4.78044443e-01  -7.96808619e-05   1.75176394e-04  -3.92081664e-01\n",
      "   1.14565931e-04  -9.08258862e-05  -2.62966530e-01] \n",
      "\n",
      "pred_LogR =  0.657165668663\n"
     ]
    }
   ],
   "source": [
    "max_iters_LogR_test = 2000\n",
    "gamma_LogR_test = 1.0e-10\n",
    "method = 5\n",
    "\n",
    "loss_LogR_test, weights_LogR_test = logistic_regression(y1, tX1, gamma_LogR_test, max_iters_LogR_test)\n",
    "print(\"\\nweights_LogR:\\n\",weights_LogR_test,\"\\n\")\n",
    "\n",
    "pred_LogR_test = prediction(y1, tX1, gamma_LogR_test, 0, max_iters_LogR_test, method)\n",
    "print(\"pred_LogR = \", pred_LogR_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.6112"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_LogR_test = predict_labels(weights_LogR_test, tX2)\n",
    "n_correct_LogR = np.count_nonzero((y_pred_LogR_test - y2) == 0)\n",
    "perc_correct_LogR = n_correct_LogR/(N/2) * 100\n",
    "perc_correct_LogR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights_RLogR:\n",
      " [ -3.97591135e-03  -4.63417801e-01  -3.43851231e-01  -3.31086769e-01\n",
      "  -8.38949357e-02  -6.46798583e-03  -5.04951374e-01  -2.56499077e-03\n",
      "  -9.34990276e-03  -7.87331013e-02  -4.16658730e-01  -7.69410972e-03\n",
      "   4.38058657e-03  -1.36723968e-03  -9.07765961e-02   3.62428974e-06\n",
      "  -4.64316411e-05  -1.94069099e-01   1.24478454e-04  -1.20333767e-04\n",
      "  -1.56955876e-01   2.05426874e-04  -6.27166821e-01  -2.34213240e-03\n",
      "  -2.39448596e-01  -3.98241840e-05   8.77391719e-05  -1.96338684e-01\n",
      "   5.73464088e-05  -4.53979083e-05  -1.31813007e-01] \n",
      "\n",
      "pred_RLogR =  0.657165668663\n"
     ]
    }
   ],
   "source": [
    "max_iters_RLogR_test = 1000\n",
    "lambda_RLogR_test = 1.0e-8\n",
    "gamma_RLogR_test = 1.0e-10\n",
    "method = 5\n",
    "\n",
    "loss_RLogR_test, weights_RLogR_test = reg_logistic_regression(y1, tX1, gamma_RLogR_test, lambda_RLogR_test, max_iters_RLogR_test)\n",
    "print(\"\\nweights_RLogR:\\n\", weights_RLogR_test,\"\\n\")\n",
    "\n",
    "pred_RLogR_test = prediction(y1, tX1, gamma_RLogR_test, lambda_RLogR_test, max_iters_RLogR_test, 6)\n",
    "print(\"pred_RLogR = \", pred_RLogR_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.6112"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_RLogR_test = predict_labels(weights_RLogR_test, tX2)\n",
    "n_correct_RLogR = np.count_nonzero((y_pred_RLogR_test - y2) == 0)\n",
    "perc_correct_RLogR = n_correct_RLogR/(N/2) * 100\n",
    "perc_correct_RLogR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the entire training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression - gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iters_GD = 5000\n",
    "gamma_GD = 1.0e-8\n",
    "method = 1\n",
    "\n",
    "loss_GD, weights_GD = least_squares_GD(y, tX_stand, gamma_GD, max_iters_GD)\n",
    "pred_GD = prediction(y, tX_stand, gamma_GD, 0, max_iters_GD, method)\n",
    "\n",
    "print(\"\\nweights_GD:\\n\",weights_GD,\"\\n\")\n",
    "print(\"pred_GD = \", pred_GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression - stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iters_SGD = 1000\n",
    "gamma_SGD = 1.0e-8\n",
    "method = 2\n",
    "\n",
    "loss_SGD, weights_SGD = least_squares_SGD(y, tX_stand, gamma_SGD, max_iters_SGD)\n",
    "print(\"\\nweights_SGD:\\n\",weights_SGD,\"\\n\")\n",
    "\n",
    "pred_SGD = prediction(y, tX_stand, gamma_SGD, 0, max_iters_SGD, method)\n",
    "print(\"pred_SGD = \", pred_SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method = 3\n",
    "\n",
    "tX_cleaned_TEST = tX_cleaned.copy()\n",
    "w0 = np.ones([1,tX_cleaned_TEST.shape[0]])\n",
    "tX_cleaned_TEST = np.insert(tX_cleaned_TEST, 0, w0, axis=1)\n",
    "\n",
    "loss_LeastS, weights_LeastS = least_squares(y, tX_cleaned_TEST)\n",
    "print(\"\\nweights_LeastS:\\n\",weights_LeastS,\"\\n\")\n",
    "\n",
    "pred_LeastS = prediction(y, tX_cleaned_TEST, 0, 0, 0, method)\n",
    "print(\"pred_LeastS = \", pred_LeastS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_RR =  0.619521355228\n"
     ]
    }
   ],
   "source": [
    "lambda_RR = 5\n",
    "method = 4\n",
    "\n",
    "degree_RR = 12\n",
    "\n",
    "tX_cleaned_TEST = tX_cleaned.copy()\n",
    "w0 = np.ones([1,tX_cleaned_TEST.shape[0]])\n",
    "tX_cleaned_TEST = np.insert(tX_cleaned_TEST, 0, w0, axis=1)\n",
    "\n",
    "tX_poly_TEST = build_poly(tX_cleaned_TEST, degree_RR)\n",
    "\n",
    "loss_RR, weights_RR = ridge_regression(y, tX_poly_TEST, lambda_RR)\n",
    "# print(\"\\nweights_RR:\\n\", weights_RR,\"\\n\")\n",
    "\n",
    "pred_RR = prediction(y, tX_poly_TEST, 0, lambda_RR, 0, method)\n",
    "print(\"pred_RR = \", pred_RR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iters_LogR = 1000\n",
    "gamma_LogR = 1.0e-6\n",
    "method = 5\n",
    "\n",
    "loss_LogR, weights_LogR = logistic_regression(y, tX_stand, gamma_LogR, max_iters_LogR)\n",
    "print(\"\\nweights_LogR:\\n\",weights_LogR,\"\\n\")\n",
    "\n",
    "pred_LogR = prediction(y, tX_stand, gamma_LogR, 0, max_iters_LogR, method)\n",
    "print(\"pred_LogR = \", pred_LogR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iters_RLogR = 1000\n",
    "lambda_RLogR = 2\n",
    "gamma_RLogR = 1.0e-8\n",
    "method = 5\n",
    "\n",
    "loss_RLogR, weights_RLogR = reg_logistic_regression(y, tX_stand, gamma_RLogR, lambda_RLogR, max_iters_RLogR)\n",
    "print(\"\\nweights_RLogR:\\n\",weights_RLogR,\"\\n\")\n",
    "\n",
    "pred_RLogR = prediction(y, tX_stand, gamma_RLogR, lambda_RLogR, max_iters_RLogR, 6)\n",
    "print(\"pred_RLogR = \", pred_RLogR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serparating the data by JET values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 143.905,   81.417,   80.943, ..., -999.   , -999.   ,    0.   ],\n",
       "       [ 175.864,   16.915,  134.805, ..., -999.   , -999.   ,    0.   ],\n",
       "       [ 105.594,   50.559,  100.989, ..., -999.   , -999.   ,    0.   ],\n",
       "       ..., \n",
       "       [-999.   ,   58.179,   68.083, ..., -999.   , -999.   ,    0.   ],\n",
       "       [  94.951,   19.362,   68.812, ..., -999.   , -999.   ,    0.   ],\n",
       "       [-999.   ,   72.756,   70.831, ..., -999.   , -999.   ,    0.   ]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX0_index = tX[:, 22] == 0.0\n",
    "tX[tX0_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separating_by_jet(y, tX):\n",
    "    \n",
    "    # JET 1\n",
    "    tX0_index = tX[:, 22] == 0.0\n",
    "    tX0 = tX[tX0_index]\n",
    "    y0 = y[tX0_index]\n",
    "    \n",
    "    # JET 2\n",
    "    tX1_index = tX[:, 22] == 1.0\n",
    "    tX1 = tX[tX1_index]\n",
    "    y1 = y[tX0_index]\n",
    "    \n",
    "    # JET 3\n",
    "    tX2_index = tX[:, 22] == 2.0\n",
    "    tX2 = tX[tX2_index]\n",
    "    y2 = y[tX0_index]\n",
    "    \n",
    "    # JET 4\n",
    "    tX3_index = tX[:, 22] == 3.0\n",
    "    tX3 = tX[tX3_index]\n",
    "    y3 = y[tX0_index]\n",
    "    \n",
    "    return y0, tX0, tX0_index, y1, tX1, tX1_index, y2, tX2, tX2_index, y3, tX3, tX3_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data_jet(tX) :\n",
    "    \n",
    "    (nbrRows, nbrColumn) = tX.shape\n",
    "    \n",
    "    tX_cleaned, _ = clean_data(tX)\n",
    "    tX_reformed = tX_cleaned.copy()\n",
    "    nbrOfDelete = 0\n",
    "    \n",
    "    for iColumn in range(0, nbrColumn) :\n",
    "        if tX_cleaned[:, iColumn].all():\n",
    "            tX_reformed = np.delete(tX_reformed, iColumn - nbrOfDelete, 1)\n",
    "            nbrOfDelete = nbrOfDelete + 1\n",
    "        \n",
    "    return tX_reformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "tX_jet0, tX_jet1, tX_jet2, tX_jet3 = separating_by_jet(tX)\n",
    "tX_cleaned_jet0 = clean_data_jet(tX_jet0)\n",
    "tX_cleaned_jet1 = clean_data_jet(tX_jet1)\n",
    "tX_cleaned_jet2 = clean_data_jet(tX_jet2)\n",
    "tX_cleaned_jet3 = clean_data_jet(tX_jet3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data for each JET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambda_jet = 5\n",
    "method = 4\n",
    "\n",
    "degree_jet = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JET == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX_reformed_jet0 = tX_cleaned_jet0.copy()\n",
    "w0 = np.ones([1,tX_reformed_jet0.shape[0]])\n",
    "tX_reformed_jet0 = np.insert(tX_reformed_jet0, 0, w0, axis=1)\n",
    "\n",
    "tX_poly_jet0 = build_poly(tX_reformed_jet0, degree_jet)\n",
    "\n",
    "loss_jet0, weights_jet0 = ridge_regression(y, tX_poly_jet0, lambda_jet)\n",
    "# print(\"\\nweights_RR:\\n\", weights_jet0,\"\\n\")\n",
    "\n",
    "pred_jet0 = prediction(y, tX_poly_jet0, 0, lambda_jet, 0, method)\n",
    "print(\"pred_RR = \", pred_jet0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JET == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambda_RR = 5\n",
    "method = 4\n",
    "\n",
    "degree_RR = 12\n",
    "\n",
    "tX_cleaned_TEST = tX_cleaned.copy()\n",
    "w0 = np.ones([1,tX_cleaned_TEST.shape[0]])\n",
    "tX_cleaned_TEST = np.insert(tX_cleaned_TEST, 0, w0, axis=1)\n",
    "\n",
    "tX_poly_TEST = build_poly(tX_cleaned_TEST, degree_RR)\n",
    "\n",
    "loss_RR, weights_RR = ridge_regression(y, tX_poly_TEST, lambda_RR)\n",
    "# print(\"\\nweights_RR:\\n\", weights_RR,\"\\n\")\n",
    "\n",
    "pred_RR = prediction(y, tX_poly_TEST, 0, lambda_RR, 0, method)\n",
    "print(\"pred_RR = \", pred_RR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JET == 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JET == 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 30)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tX_test_cleaned, _ = clean_data(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX_test_cleaned_TEST = tX_test_cleaned.copy()\n",
    "w0 = np.ones([1,tX_test_cleaned.shape[0]])\n",
    "tX_test_cleaned_TEST = np.insert(tX_test_cleaned_TEST, 0, w0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX_test_stand, mean_test, std_test = standardize(tX_test, mean_training, std_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/dataSubmission_GD.csv' \n",
    "y_pred_GD = predict_labels(weights_GD, tX_test_stand)\n",
    "create_csv_submission(ids_test, y_pred_GD, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/dataSubmission_SGD.csv' \n",
    "y_pred_SGD = predict_labels(weights_SGD, tX_test_stand)\n",
    "create_csv_submission(ids_test, y_pred_SGD, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/dataSubmission_LS.csv' \n",
    "y_pred_LS = predict_labels(weights_LeastS, tX_test_stand)\n",
    "create_csv_submission(ids_test, y_pred_LS, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/dataSubmission_RR.csv' \n",
    "y_pred_RR = predict_labels(weights_RR, tX_test_stand)\n",
    "create_csv_submission(ids_test, y_pred_RR, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/dataSubmission_LogR.csv' \n",
    "y_pred_LogR = predict_labels(weights_LogR, tX_test_stand)\n",
    "create_csv_submission(ids_test, y_pred_LogR, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/dataSubmission_RLogR.csv' \n",
    "y_pred_RLogR = predict_labels(weights_RLogR_RLogR, tX_test_stand)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/dataSubmission_LS.csv' \n",
    "y_pred_LS = predict_labels(weights_LeastS, tX_test_cleaned_TEST)\n",
    "create_csv_submission(ids_test, y_pred_LS, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX_test_poly_TEST = build_poly(tX_test_cleaned_TEST, degree_RR)\n",
    "\n",
    "OUTPUT_PATH = '../data/dataSubmission_RR.csv' \n",
    "y_pred_RR = predict_labels(weights_RR, tX_test_poly_TEST)\n",
    "create_csv_submission(ids_test, y_pred_RR, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
