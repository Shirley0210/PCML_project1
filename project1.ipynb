{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT 1: Higgs Boson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the usual import needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Library only used for observing the data in a more visual way.\n",
    "import pandas as pd\n",
    "\n",
    "import methods as md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the data in a pandas DataFrame to see more easily how they look like, but we are not going to use this specific library for other reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.910</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.240</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>89.744</td>\n",
       "      <td>13.550</td>\n",
       "      <td>59.149</td>\n",
       "      <td>116.344</td>\n",
       "      <td>2.636</td>\n",
       "      <td>284.584</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.362</td>\n",
       "      <td>61.619</td>\n",
       "      <td>...</td>\n",
       "      <td>2.237</td>\n",
       "      <td>282.849</td>\n",
       "      <td>3.0</td>\n",
       "      <td>90.547</td>\n",
       "      <td>-2.412</td>\n",
       "      <td>-0.653</td>\n",
       "      <td>56.165</td>\n",
       "      <td>0.224</td>\n",
       "      <td>3.106</td>\n",
       "      <td>193.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100006</th>\n",
       "      <td>1.0</td>\n",
       "      <td>148.754</td>\n",
       "      <td>28.862</td>\n",
       "      <td>107.782</td>\n",
       "      <td>106.130</td>\n",
       "      <td>0.733</td>\n",
       "      <td>158.359</td>\n",
       "      <td>0.113</td>\n",
       "      <td>2.941</td>\n",
       "      <td>2.545</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.443</td>\n",
       "      <td>294.074</td>\n",
       "      <td>2.0</td>\n",
       "      <td>123.010</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1.450</td>\n",
       "      <td>56.867</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>179.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100007</th>\n",
       "      <td>1.0</td>\n",
       "      <td>154.916</td>\n",
       "      <td>10.418</td>\n",
       "      <td>94.714</td>\n",
       "      <td>29.169</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.897</td>\n",
       "      <td>1.526</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.761</td>\n",
       "      <td>187.299</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.638</td>\n",
       "      <td>-0.715</td>\n",
       "      <td>-1.724</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>30.638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        labels        0        1        2        3        4        5        6  \\\n",
       "ids                                                                             \n",
       "100000     1.0  138.470   51.655   97.827   27.980    0.910  124.711    2.666   \n",
       "100001    -1.0  160.937   68.768  103.235   48.146 -999.000 -999.000 -999.000   \n",
       "100002    -1.0 -999.000  162.172  125.953   35.635 -999.000 -999.000 -999.000   \n",
       "100003    -1.0  143.905   81.417   80.943    0.414 -999.000 -999.000 -999.000   \n",
       "100004    -1.0  175.864   16.915  134.805   16.405 -999.000 -999.000 -999.000   \n",
       "100005    -1.0   89.744   13.550   59.149  116.344    2.636  284.584   -0.540   \n",
       "100006     1.0  148.754   28.862  107.782  106.130    0.733  158.359    0.113   \n",
       "100007     1.0  154.916   10.418   94.714   29.169 -999.000 -999.000 -999.000   \n",
       "\n",
       "            7       8   ...        20       21   22       23       24  \\\n",
       "ids                     ...                                             \n",
       "100000  3.064  41.928   ...    -0.277  258.733  2.0   67.435    2.150   \n",
       "100001  3.473   2.078   ...    -1.916  164.546  1.0   46.226    0.725   \n",
       "100002  3.148   9.336   ...    -2.186  260.414  1.0   44.251    2.053   \n",
       "100003  3.310   0.414   ...     0.060   86.062  0.0 -999.000 -999.000   \n",
       "100004  3.891  16.405   ...    -0.871   53.131  0.0 -999.000 -999.000   \n",
       "100005  1.362  61.619   ...     2.237  282.849  3.0   90.547   -2.412   \n",
       "100006  2.941   2.545   ...    -1.443  294.074  2.0  123.010    0.864   \n",
       "100007  2.897   1.526   ...    -1.761  187.299  1.0   30.638   -0.715   \n",
       "\n",
       "             25       26       27       28       29  \n",
       "ids                                                  \n",
       "100000    0.444   46.062    1.240   -2.475  113.497  \n",
       "100001    1.158 -999.000 -999.000 -999.000   46.226  \n",
       "100002   -2.028 -999.000 -999.000 -999.000   44.251  \n",
       "100003 -999.000 -999.000 -999.000 -999.000    0.000  \n",
       "100004 -999.000 -999.000 -999.000 -999.000    0.000  \n",
       "100005   -0.653   56.165    0.224    3.106  193.660  \n",
       "100006    1.450   56.867    0.131   -2.767  179.877  \n",
       "100007   -1.724 -999.000 -999.000 -999.000   30.638  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([pd.DataFrame({'labels': y, 'ids': ids}), pd.DataFrame(tX)], axis=1)\n",
    "df_indexed = df.set_index(['ids'])\n",
    "df_indexed.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed that many entries are filled by the value *-999*, which represent a lack of information. Theses values disturb our statistical calculations. Therefore it is very useful to clean the data before performing any further computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that take as input the matrix of the different features *tX* for all the samples and return the same matrix matrix after some cleaning have been performed.\n",
    "\n",
    "In order to *\"clean\"* our data, we are first going to replace the *-999* values by the mean of the features on all the sample for which we have an information. With this process, we preserve our statistical result of the influence of theses outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "\n",
    "def clean_data(tx):\n",
    "    nbrRows = tx.shape[0]\n",
    "    nbrColunms = tx.shape[1]\n",
    "    tx_temp = np.zeros((nbrRows,nbrColunms))\n",
    "    modified_columns = [False] * nbrColunms\n",
    "    \n",
    "    for columnID in range(nbrColunms):\n",
    "        currentColumn = tx[:,columnID].copy()\n",
    "        \n",
    "        # extract indices with -999 values\n",
    "        nanIndices = []\n",
    "\n",
    "        for rowID in range(nbrRows):\n",
    "            if currentColumn[rowID] == -999.000:\n",
    "                nanIndices.append(rowID)\n",
    "                modified_columns[columnID] = True\n",
    "        \n",
    "        tempColumm = np.delete(currentColumn, nanIndices, axis=0)\n",
    "\n",
    "        # replace -999 values with median\n",
    "        median = np.median(tempColumm)\n",
    "        currentColumn[nanIndices] = median\n",
    "         \n",
    "        tx_temp[:,columnID] = currentColumn\n",
    "        \n",
    "    return tx_temp, modified_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_cleaned, modified_columns = clean_data(tX)\n",
    "tX_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that we replaced correctly the values -999 with the median of each collumn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.910</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.240</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>2.107</td>\n",
       "      <td>225.885</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>47.902</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>112.406</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>2.107</td>\n",
       "      <td>225.885</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>47.902</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>2.107</td>\n",
       "      <td>225.885</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.561</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>47.902</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>2.107</td>\n",
       "      <td>225.885</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.561</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>47.902</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>89.744</td>\n",
       "      <td>13.550</td>\n",
       "      <td>59.149</td>\n",
       "      <td>116.344</td>\n",
       "      <td>2.636</td>\n",
       "      <td>284.584</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.362</td>\n",
       "      <td>61.619</td>\n",
       "      <td>...</td>\n",
       "      <td>2.237</td>\n",
       "      <td>282.849</td>\n",
       "      <td>3.0</td>\n",
       "      <td>90.547</td>\n",
       "      <td>-2.412</td>\n",
       "      <td>-0.653</td>\n",
       "      <td>56.165</td>\n",
       "      <td>0.224</td>\n",
       "      <td>3.106</td>\n",
       "      <td>193.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100006</th>\n",
       "      <td>1.0</td>\n",
       "      <td>148.754</td>\n",
       "      <td>28.862</td>\n",
       "      <td>107.782</td>\n",
       "      <td>106.130</td>\n",
       "      <td>0.733</td>\n",
       "      <td>158.359</td>\n",
       "      <td>0.113</td>\n",
       "      <td>2.941</td>\n",
       "      <td>2.545</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.443</td>\n",
       "      <td>294.074</td>\n",
       "      <td>2.0</td>\n",
       "      <td>123.010</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1.450</td>\n",
       "      <td>56.867</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>179.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100007</th>\n",
       "      <td>1.0</td>\n",
       "      <td>154.916</td>\n",
       "      <td>10.418</td>\n",
       "      <td>94.714</td>\n",
       "      <td>29.169</td>\n",
       "      <td>2.107</td>\n",
       "      <td>225.885</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>2.897</td>\n",
       "      <td>1.526</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.761</td>\n",
       "      <td>187.299</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.638</td>\n",
       "      <td>-0.715</td>\n",
       "      <td>-1.724</td>\n",
       "      <td>47.902</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>30.638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        labels        0        1        2        3      4        5      6  \\\n",
       "ids                                                                         \n",
       "100000     1.0  138.470   51.655   97.827   27.980  0.910  124.711  2.666   \n",
       "100001    -1.0  160.937   68.768  103.235   48.146  2.107  225.885 -0.244   \n",
       "100002    -1.0  112.406  162.172  125.953   35.635  2.107  225.885 -0.244   \n",
       "100003    -1.0  143.905   81.417   80.943    0.414  2.107  225.885 -0.244   \n",
       "100004    -1.0  175.864   16.915  134.805   16.405  2.107  225.885 -0.244   \n",
       "100005    -1.0   89.744   13.550   59.149  116.344  2.636  284.584 -0.540   \n",
       "100006     1.0  148.754   28.862  107.782  106.130  0.733  158.359  0.113   \n",
       "100007     1.0  154.916   10.418   94.714   29.169  2.107  225.885 -0.244   \n",
       "\n",
       "            7       8   ...        20       21   22       23     24     25  \\\n",
       "ids                     ...                                                  \n",
       "100000  3.064  41.928   ...    -0.277  258.733  2.0   67.435  2.150  0.444   \n",
       "100001  3.473   2.078   ...    -1.916  164.546  1.0   46.226  0.725  1.158   \n",
       "100002  3.148   9.336   ...    -2.186  260.414  1.0   44.251  2.053 -2.028   \n",
       "100003  3.310   0.414   ...     0.060   86.062  0.0   65.561  0.000 -0.033   \n",
       "100004  3.891  16.405   ...    -0.871   53.131  0.0   65.561  0.000 -0.033   \n",
       "100005  1.362  61.619   ...     2.237  282.849  3.0   90.547 -2.412 -0.653   \n",
       "100006  2.941   2.545   ...    -1.443  294.074  2.0  123.010  0.864  1.450   \n",
       "100007  2.897   1.526   ...    -1.761  187.299  1.0   30.638 -0.715 -1.724   \n",
       "\n",
       "            26     27     28       29  \n",
       "ids                                    \n",
       "100000  46.062  1.240 -2.475  113.497  \n",
       "100001  47.902 -0.010 -0.002   46.226  \n",
       "100002  47.902 -0.010 -0.002   44.251  \n",
       "100003  47.902 -0.010 -0.002    0.000  \n",
       "100004  47.902 -0.010 -0.002    0.000  \n",
       "100005  56.165  0.224  3.106  193.660  \n",
       "100006  56.867  0.131 -2.767  179.877  \n",
       "100007  47.902 -0.010 -0.002   30.638  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = pd.concat([pd.DataFrame({'labels': y, 'ids': ids}), pd.DataFrame(tX_cleaned)], axis=1)\n",
    "df_cleaned_indexed = df_cleaned.set_index(['ids'])\n",
    "df_cleaned_indexed.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can aslo check which columns have been filled and which ones have not been modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also standardize our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalized data set\n",
    "tX_stand, mean_training, std_training = standardize(tX_cleaned)\n",
    "tX_stand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.341522</td>\n",
       "      <td>0.068332</td>\n",
       "      <td>0.407680</td>\n",
       "      <td>-0.469966</td>\n",
       "      <td>-1.353339</td>\n",
       "      <td>-0.640013</td>\n",
       "      <td>1.579473</td>\n",
       "      <td>0.882478</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147267</td>\n",
       "      <td>0.386847</td>\n",
       "      <td>1.044402</td>\n",
       "      <td>-0.202115</td>\n",
       "      <td>1.556350</td>\n",
       "      <td>0.330677</td>\n",
       "      <td>-0.262878</td>\n",
       "      <td>1.142622</td>\n",
       "      <td>-2.526840</td>\n",
       "      <td>0.412510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.766558</td>\n",
       "      <td>0.552505</td>\n",
       "      <td>0.540136</td>\n",
       "      <td>-0.153167</td>\n",
       "      <td>-0.090817</td>\n",
       "      <td>-0.188805</td>\n",
       "      <td>0.086029</td>\n",
       "      <td>1.404888</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.051683</td>\n",
       "      <td>-0.357719</td>\n",
       "      <td>0.021305</td>\n",
       "      <td>-0.644524</td>\n",
       "      <td>0.525758</td>\n",
       "      <td>0.838833</td>\n",
       "      <td>-0.159461</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.273820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.151562</td>\n",
       "      <td>3.195156</td>\n",
       "      <td>1.096560</td>\n",
       "      <td>-0.349710</td>\n",
       "      <td>-0.090817</td>\n",
       "      <td>-0.188805</td>\n",
       "      <td>0.086029</td>\n",
       "      <td>0.989770</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.200672</td>\n",
       "      <td>0.400135</td>\n",
       "      <td>0.021305</td>\n",
       "      <td>-0.685722</td>\n",
       "      <td>1.486197</td>\n",
       "      <td>-1.428652</td>\n",
       "      <td>-0.159461</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.293970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444343</td>\n",
       "      <td>0.910379</td>\n",
       "      <td>-0.005853</td>\n",
       "      <td>-0.903016</td>\n",
       "      <td>-0.090817</td>\n",
       "      <td>-0.188805</td>\n",
       "      <td>0.086029</td>\n",
       "      <td>1.196690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038692</td>\n",
       "      <td>-0.978149</td>\n",
       "      <td>-1.001792</td>\n",
       "      <td>-0.241206</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>-0.008805</td>\n",
       "      <td>-0.159461</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.745439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.048950</td>\n",
       "      <td>-0.914556</td>\n",
       "      <td>1.313369</td>\n",
       "      <td>-0.651804</td>\n",
       "      <td>-0.090817</td>\n",
       "      <td>-0.188805</td>\n",
       "      <td>0.086029</td>\n",
       "      <td>1.938794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.475042</td>\n",
       "      <td>-1.238475</td>\n",
       "      <td>-1.001792</td>\n",
       "      <td>-0.241206</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>-0.008805</td>\n",
       "      <td>-0.159461</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.745439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.580287</td>\n",
       "      <td>-1.009761</td>\n",
       "      <td>-0.539646</td>\n",
       "      <td>0.918192</td>\n",
       "      <td>0.467139</td>\n",
       "      <td>0.072976</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>-1.291464</td>\n",
       "      <td>...</td>\n",
       "      <td>1.239982</td>\n",
       "      <td>0.577488</td>\n",
       "      <td>2.067499</td>\n",
       "      <td>0.279989</td>\n",
       "      <td>-1.742991</td>\n",
       "      <td>-0.450060</td>\n",
       "      <td>0.304958</td>\n",
       "      <td>0.214296</td>\n",
       "      <td>3.175385</td>\n",
       "      <td>1.230371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100006</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.536077</td>\n",
       "      <td>-0.576543</td>\n",
       "      <td>0.651504</td>\n",
       "      <td>0.757735</td>\n",
       "      <td>-1.540028</td>\n",
       "      <td>-0.489952</td>\n",
       "      <td>0.269245</td>\n",
       "      <td>0.725371</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.790677</td>\n",
       "      <td>0.666224</td>\n",
       "      <td>1.044402</td>\n",
       "      <td>0.957151</td>\n",
       "      <td>0.626286</td>\n",
       "      <td>1.046650</td>\n",
       "      <td>0.344414</td>\n",
       "      <td>0.129322</td>\n",
       "      <td>-2.825182</td>\n",
       "      <td>1.089751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100007</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.652651</td>\n",
       "      <td>-1.098374</td>\n",
       "      <td>0.331435</td>\n",
       "      <td>-0.451288</td>\n",
       "      <td>-0.090817</td>\n",
       "      <td>-0.188805</td>\n",
       "      <td>0.086029</td>\n",
       "      <td>0.669171</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.966153</td>\n",
       "      <td>-0.177852</td>\n",
       "      <td>0.021305</td>\n",
       "      <td>-0.969682</td>\n",
       "      <td>-0.515682</td>\n",
       "      <td>-1.212294</td>\n",
       "      <td>-0.159461</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.432856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        labels    0         1         2         3         4         5  \\\n",
       "ids                                                                     \n",
       "100000     1.0  1.0  0.341522  0.068332  0.407680 -0.469966 -1.353339   \n",
       "100001    -1.0  1.0  0.766558  0.552505  0.540136 -0.153167 -0.090817   \n",
       "100002    -1.0  1.0 -0.151562  3.195156  1.096560 -0.349710 -0.090817   \n",
       "100003    -1.0  1.0  0.444343  0.910379 -0.005853 -0.903016 -0.090817   \n",
       "100004    -1.0  1.0  1.048950 -0.914556  1.313369 -0.651804 -0.090817   \n",
       "100005    -1.0  1.0 -0.580287 -1.009761 -0.539646  0.918192  0.467139   \n",
       "100006     1.0  1.0  0.536077 -0.576543  0.651504  0.757735 -1.540028   \n",
       "100007     1.0  1.0  0.652651 -1.098374  0.331435 -0.451288 -0.090817   \n",
       "\n",
       "               6         7         8    ...           21        22        23  \\\n",
       "ids                                     ...                                    \n",
       "100000 -0.640013  1.579473  0.882478    ...    -0.147267  0.386847  1.044402   \n",
       "100001 -0.188805  0.086029  1.404888    ...    -1.051683 -0.357719  0.021305   \n",
       "100002 -0.188805  0.086029  0.989770    ...    -1.200672  0.400135  0.021305   \n",
       "100003 -0.188805  0.086029  1.196690    ...     0.038692 -0.978149 -1.001792   \n",
       "100004 -0.188805  0.086029  1.938794    ...    -0.475042 -1.238475 -1.001792   \n",
       "100005  0.072976 -0.065881 -1.291464    ...     1.239982  0.577488  2.067499   \n",
       "100006 -0.489952  0.269245  0.725371    ...    -0.790677  0.666224  1.044402   \n",
       "100007 -0.188805  0.086029  0.669171    ...    -0.966153 -0.177852  0.021305   \n",
       "\n",
       "              24        25        26        27        28        29        30  \n",
       "ids                                                                           \n",
       "100000 -0.202115  1.556350  0.330677 -0.262878  1.142622 -2.526840  0.412510  \n",
       "100001 -0.644524  0.525758  0.838833 -0.159461  0.000489 -0.000124 -0.273820  \n",
       "100002 -0.685722  1.486197 -1.428652 -0.159461  0.000489 -0.000124 -0.293970  \n",
       "100003 -0.241206  0.001422 -0.008805 -0.159461  0.000489 -0.000124 -0.745439  \n",
       "100004 -0.241206  0.001422 -0.008805 -0.159461  0.000489 -0.000124 -0.745439  \n",
       "100005  0.279989 -1.742991 -0.450060  0.304958  0.214296  3.175385  1.230371  \n",
       "100006  0.957151  0.626286  1.046650  0.344414  0.129322 -2.825182  1.089751  \n",
       "100007 -0.969682 -0.515682 -1.212294 -0.159461  0.000489 -0.000124 -0.432856  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stand = pd.concat([pd.DataFrame({'labels': y, 'ids': ids}), pd.DataFrame(tX_stand)], axis=1)\n",
    "df_stand_indexed = df_stand.set_index(['ids'])\n",
    "df_stand_indexed.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(y, x, seed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # set mask\n",
    "    ratio = 0.7\n",
    "    msk = np.random.rand(len(y)) < ratio\n",
    "    \n",
    "    # training data set\n",
    "    x_tr = x[msk]\n",
    "    y_tr = y[msk]\n",
    "    \n",
    "    # test data set\n",
    "    x_test = x[~msk]\n",
    "    y_test = y[~msk]\n",
    "    \n",
    "    return x_tr, x_test, y_tr, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prediction(y, tX, gamma, lambda_, max_iters, method):\n",
    "    # split data\n",
    "    x_tr, x_test, y_tr, y_test = split_data(y, tX)\n",
    "    \n",
    "    # training\n",
    "    loss = 0\n",
    "    weights = []\n",
    "    if method == 1:\n",
    "        loss, weights = least_squares_GD(y_tr, x_tr, gamma, max_iters)\n",
    "    elif method == 2:\n",
    "        loss, weights = least_squares_SGD(y_tr, x_tr, gamma, max_iters)\n",
    "    elif method == 3:\n",
    "        loss, weights = least_squares(y_tr, x_tr)\n",
    "    elif method == 4:\n",
    "        loss, weights = ridge_regression(y_tr, x_tr, lambda_)\n",
    "    elif method == 5:\n",
    "        loss, weights = logistic_regression(y_tr, x_tr, gamma, max_iters)\n",
    "    else:\n",
    "        loss, weights = reg_logistic_regression(y_tr, x_tr, lambda_, gamma, max_iters)\n",
    "        \n",
    "    # compute prediction\n",
    "    y_pred = predict_labels(weights, x_test)    \n",
    "    \n",
    "    # accuracy of the prediction\n",
    "    N = y_test.shape[0]\n",
    "    pred = np.sum(y_pred == y_test)/N\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of ML methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression - gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from costs import *\n",
    "\n",
    "def compute_gradient(y, tX, w):\n",
    "    # error\n",
    "    e = y - tX.dot(w)\n",
    "    \n",
    "    # gradient \n",
    "    N=y.shape[0]\n",
    "    gradient = - np.transpose(tX).dot(e)/N\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tX, gamma, max_iters):\n",
    "    # init parameters\n",
    "    threshold = 1e-10\n",
    "    w_init = np.zeros(tX.shape[1])\n",
    "    ws = [w_init]\n",
    "    w_temp = w_init\n",
    "    losses = [8000]\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        grad = compute_gradient(y, tX, w_temp)\n",
    "        loss = compute_loss(y, tX, w_temp)\n",
    "        \n",
    "        # update w by gradient\n",
    "        w_temp -= gamma*grad\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w_temp))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # converge criteria\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "\n",
    "    return losses, ws[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression - stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tX, w):\n",
    "    B = 35 # size of the batch\n",
    "    sum = 0\n",
    "    for minibatch_y, minibatch_tX in batch_iter(y, tX, B):\n",
    "        sum += compute_gradient(minibatch_y, minibatch_tX, w)\n",
    "\n",
    "    return sum / B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tX, gamma, max_iters):    \n",
    "    # init parameters\n",
    "    threshold = 1e-8\n",
    "    w_init = np.zeros(tX.shape[1])\n",
    "    ws = [w_init]\n",
    "    w_temp = w_init\n",
    "    losses = [8000]\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        grad = compute_stoch_gradient(y, tX, w_temp)\n",
    "        loss = compute_loss(y, tX, w_temp)\n",
    "\n",
    "        # update w by gradient\n",
    "        w_temp -= gamma*grad\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w_temp))\n",
    "        losses.append(loss)\n",
    "        \n",
    "    # converge criteria\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "        \n",
    "    return losses, ws[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tX):\n",
    "    # Compute optimum weight\n",
    "    tX_transpose = np.transpose(tX)\n",
    "    A = tX_transpose.dot(tX)\n",
    "    b = tX_transpose.dot(y)\n",
    "    w_opt = np.linalg.solve(A,b)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = compute_loss(y, tX, w_opt)\n",
    "        \n",
    "    return loss, w_opt # returns loss, and optimal weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    poly = np.ones(x.shape)\n",
    "    \n",
    "    for m in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, m)]\n",
    "    \n",
    "    #poly = np.c_[poly, np.absolute(np.sqrt(x))]\n",
    "   \n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tX_poly, lambda_):    \n",
    "    # Initiation variables\n",
    "    lamb_ = 2*len(y)*lambda_\n",
    "    \n",
    "    # Compute optimum weight\n",
    "    A = np.dot(np.transpose(tX_poly), tX_poly) + lamb_*np.eye(tX_poly.shape[1])\n",
    "    b = np.transpose(tX_poly).dot(y)\n",
    "    w_opt = np.linalg.solve(A,b)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = compute_loss(y, tX_poly, w_opt)\n",
    "    \n",
    "    return loss, w_opt # returns mse, and optimal weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    temp = 1+np.exp(-t)\n",
    "    return 1/(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tX, w, gamma, lambda_):\n",
    "    # Initiation variables\n",
    "    lamb_ = 2*len(y)*lambda_\n",
    "    \n",
    "    # compute the loss\n",
    "    N = tX.shape[0]\n",
    "    l1 = tX.dot(w) + np.log(np.ones((N))+np.exp(-tX.dot(w)))\n",
    "    l2 = y*(tX.dot(w))\n",
    "    loss = (np.ones((1,N)).dot(l1-l2))[0]\n",
    "    \n",
    "    # compute the gradient\n",
    "    grad = np.transpose(tX).dot(sigmoid(tX.dot(w))-y) + lamb_*w.dot(w)\n",
    "    \n",
    "    # update w\n",
    "    w -= gamma*grad\n",
    "\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tX, gamma, max_iters):\n",
    "    # init parameters\n",
    "    threshold = 1e-8\n",
    "    w_temp = np.zeros(tX.shape[1]) # initialization of the weight\n",
    "    ws = [w_temp]\n",
    "    losses = [8000]\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iters):        \n",
    "        # get loss and update w.\n",
    "        loss, w_temp = learning_by_gradient_descent(y, tX, w_temp, gamma, 0)\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w_temp))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # converge criteria\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    \n",
    "    return losses, ws[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tX, gamma, lambda_, max_iters):\n",
    "    # init parameters\n",
    "    threshold = 1e-8\n",
    "    w_temp = np.zeros(tX.shape[1]) # initialization of the weight\n",
    "    ws = [w_temp]\n",
    "    losses = [8000]\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iters):        \n",
    "        # get loss and update w.\n",
    "        loss, w_temp = learning_by_gradient_descent(y, tX, w_temp, gamma, lambda_)\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w_temp))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # converge criteria\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    \n",
    "    return losses, ws[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = len(y)\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    \n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train:\n",
    "    x_test = x[k_indices[k]]\n",
    "    y_test = y[k_indices[k]]\n",
    "    \n",
    "    tr_indices = np.delete(k_indices, k, axis=0)\n",
    "    x_tr = np.delete(x, k, axis=0)\n",
    "    y_tr = np.delete(y, k, axis=0)\n",
    "    \n",
    "    # form train and test data with polynomial basis function\n",
    "    poly_x_tr = build_poly(x_tr, degree)\n",
    "    poly_x_test = build_poly(x_test, degree)\n",
    "    \n",
    "    # calcualte weight and loss through least square.\n",
    "    loss_tr, weight_tr = ridge_regression(y_tr, poly_x_tr, lambda_)\n",
    "    loss_test, weight_te = ridge_regression(y_test, poly_x_test, lambda_)\n",
    "    \n",
    "    return loss_tr, loss_te, weight_tr, weight_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on half of our training set and test the model on the other half:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's divide our data in two part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:3: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  app.launch_new_instance()\n",
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:4: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:6: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:7: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/numpy/core/numeric.py:190: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  a = empty(shape, dtype, order)\n",
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:13: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:14: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "N = y.shape[0]\n",
    "\n",
    "y1 = y[:N/2].copy()\n",
    "y2 = y[N/2:].copy()\n",
    "\n",
    "tX1 = tX_cleaned[:N/2, :].copy()\n",
    "tX2 = tX_cleaned[N/2:, :].copy()\n",
    "\n",
    "w0 = np.ones([1,N/2])\n",
    "tX1 = np.insert(tX1, 0, w0, axis=1)\n",
    "tX2 = np.insert(tX2, 0, w0, axis=1)\n",
    "\n",
    "ids1 = ids[:N/2].copy()\n",
    "ids2 = ids[N/2:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression - gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights_GD:\n",
      " [ -7.02181144e-06  -6.76675669e-04  -8.97777621e-04  -5.23192467e-04\n",
      "   2.57001876e-04  -7.23174466e-06   2.64585418e-04  -1.25179752e-05\n",
      "  -1.61558759e-05  -1.24443507e-04  -5.14468323e-05  -1.73979905e-05\n",
      "   1.66560497e-05  -1.55670651e-06   7.19400927e-06  -7.40805407e-08\n",
      "  -2.81217008e-07  -3.21900711e-04   3.46775238e-07  -1.22571171e-07\n",
      "  -2.17271338e-04   7.12554648e-07  -4.05208168e-04   3.29750958e-07\n",
      "  -2.11976201e-04  -1.71803251e-07   1.87877644e-07  -3.17364813e-04\n",
      "   1.33344399e-07  -2.20120610e-07   2.63259972e-04] \n",
      "\n",
      "pred_GD =  0.670099800399\n"
     ]
    }
   ],
   "source": [
    "max_iters_GD_test = 2000\n",
    "gamma_GD_test = 1.0e-8\n",
    "method = 1\n",
    "\n",
    "loss_GD_test, weights_GD_test = least_squares_GD(y1, tX1, gamma_GD_test, max_iters_GD_test)\n",
    "pred_GD_test = prediction(y1, tX1, gamma_GD_test, 0, max_iters_GD_test, method)\n",
    "\n",
    "print(\"\\nweights_GD:\\n\",weights_GD_test,\"\\n\")\n",
    "print(\"pred_GD = \", pred_GD_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.0248"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_GD_test = predict_labels(weights_GD_test, tX2)\n",
    "n_correct_GD = np.count_nonzero((y_pred_GD_test - y2) == 0)\n",
    "perc_correct_GD = n_correct_GD/(N/2) * 100\n",
    "perc_correct_GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression - stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights_SGD:\n",
      " [ -3.75046095e-05   1.40705033e-04  -6.44799833e-03  -4.50093212e-04\n",
      "   2.12750846e-03  -1.85823047e-05   6.30237753e-04  -9.42820546e-05\n",
      "  -2.04482715e-05  -8.98834444e-04   8.45356531e-04  -1.46466146e-04\n",
      "   1.71564204e-04   3.27632594e-06   2.17281745e-03  -3.22683260e-06\n",
      "  -8.82613457e-06  -1.20860000e-03   2.20461637e-06   2.63886386e-06\n",
      "  -1.05067113e-03   6.44749522e-06  -6.17323081e-04  -9.37945950e-06\n",
      "  -1.37941055e-03  -6.67551307e-07   3.06851103e-07  -2.13627821e-03\n",
      "   7.14567462e-07  -2.98916776e-06  -1.18859167e-04] \n",
      "\n",
      "pred_SGD =  0.69623419827\n"
     ]
    }
   ],
   "source": [
    "max_iters_SGD_test = 500\n",
    "gamma_SGD_test = 1.0e-8\n",
    "method = 2\n",
    "\n",
    "loss_SGD_test, weights_SGD_test = least_squares_SGD(y1, tX1, gamma_SGD_test, max_iters_SGD_test)\n",
    "print(\"\\nweights_SGD:\\n\",weights_SGD_test,\"\\n\")\n",
    "\n",
    "pred_SGD_test = prediction(y1, tX1, gamma_SGD_test, 0, max_iters_SGD_test, method)\n",
    "print(\"pred_SGD = \", pred_SGD_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.2272"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_SGD_test = predict_labels(weights_SGD_test, tX2)\n",
    "n_correct_SGD = np.count_nonzero((y_pred_SGD_test - y2) == 0)\n",
    "perc_correct_SGD = n_correct_SGD/(N/2) * 100\n",
    "perc_correct_SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights_LeastS:\n",
      " [ -1.16074033e+00   1.44296347e-04  -7.13890244e-03  -6.35632610e-03\n",
      "   1.18983305e-04   1.66537567e-02   4.56475343e-04   2.70567148e-03\n",
      "   3.54887909e-01  -1.19904231e-03  -4.22515836e+00  -2.21232683e-01\n",
      "   1.02368577e-01   3.52579962e-01   4.23358426e+00  -3.27685587e-03\n",
      "  -1.63291643e-03   4.23825620e+00   1.63545099e-03   2.70568868e-04\n",
      "   3.04571909e-03  -3.81839093e-04  -3.77901455e-04   4.71385100e-02\n",
      "  -8.23199214e-04   1.15058943e-03  -2.02148685e-03  -1.23809562e-03\n",
      "  -3.37838817e-04  -3.15646992e-03   4.22484938e+00] \n",
      "\n",
      "pred_LeastS =  0.747624750499\n"
     ]
    }
   ],
   "source": [
    "method = 3\n",
    "\n",
    "loss_LeastS_test, weights_LeastS_test = least_squares(y1, tX1)\n",
    "print(\"\\nweights_LeastS:\\n\",weights_LeastS_test,\"\\n\")\n",
    "\n",
    "pred_LeastS_test = prediction(y1, tX1, 0, 0, 0, method)\n",
    "print(\"pred_LeastS = \", pred_LeastS_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.4256"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_LS_test = predict_labels(weights_LeastS_test, tX2)\n",
    "n_correct_LS = np.count_nonzero((y_pred_LS_test - y2) == 0)\n",
    "perc_correct_LS = n_correct_LS/(N/2) * 100\n",
    "perc_correct_LS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights_RR:\n",
      " [ -4.85618666e+08   1.46604323e+09   2.45597904e+06  -8.30305735e+08\n",
      "  -6.18084754e+07  -9.67732014e+07   1.84997277e+07  -1.24928535e+07\n",
      "  -2.70981877e-04  -2.70981877e-04  -2.70981877e-04  -2.70981877e-04\n",
      "  -2.70981877e-04  -2.70981877e-04  -2.70981877e-04  -2.70981877e-04\n",
      "  -2.70981877e-04  -2.70981877e-04  -2.70981877e-04  -2.70981877e-04\n",
      "  -2.70981877e-04  -2.70981877e-04  -2.70981877e-04  -2.70981877e-04\n",
      "  -2.70982966e-04  -2.70982966e-04  -2.70982966e-04  -2.70982966e-04\n",
      "  -2.70982966e-04  -2.70982966e-04  -2.70982966e-04  -2.70982966e-04\n",
      "  -7.68945536e-03  -2.08207528e-03  -4.79139626e-03  -3.42676624e-03\n",
      "  -4.26766364e-04  -8.74550129e-03  -7.18849120e-04   1.26432434e-04\n",
      "  -2.33864898e-04  -7.20819274e-03  -4.46489216e-04   1.00147101e-03\n",
      "   7.11221470e-04  -2.30821056e-03  -1.38912141e-04  -9.67043325e-05\n",
      "  -2.65842173e-03   1.74589155e-05   6.76899702e-05  -4.99416690e-03\n",
      "   2.52089926e-05  -6.76571641e-03  -1.62681692e-04  -3.78544631e-03\n",
      "  -1.50592983e-04   4.99741696e-05  -3.33356446e-03   1.72821195e-05\n",
      "  -6.48371365e-05  -2.24628878e-03  -2.70982480e-04  -8.96089713e-04\n",
      "   2.15396596e-04   6.28824442e-04   7.42338694e-05  -5.81077756e-04\n",
      "   3.51774576e-05   5.44383444e-05   6.44469067e-04  -4.19804495e-05\n",
      "  -1.44264177e-04  -8.39163817e-04   3.97810226e-04   8.22236951e-04\n",
      "  -2.94587886e-04  -5.72466892e-04  -3.22657283e-04  -1.15015763e-03\n",
      "  -1.16894055e-03  -3.18029488e-04   8.05629010e-05  -3.37982165e-04\n",
      "   7.46966151e-05  -1.79143855e-04  -1.06593410e-06   1.21109563e-03\n",
      "  -1.80736581e-04  -4.40639461e-03   6.28708310e-04  -1.33054966e-04\n",
      "   3.80499421e-04  -2.70983351e-04   2.18967331e-05  -1.34804393e-05\n",
      "  -1.15203322e-05  -6.72818803e-07  -5.71477941e-04  -7.21826583e-08\n",
      "  -1.92112431e-03   1.30735536e-03   7.51445780e-07   1.54333027e-06\n",
      "  -1.23002675e-03   1.46100772e-03   8.27984981e-04   5.16089862e-05\n",
      "  -2.28716873e-04  -1.20903168e-04   5.83849940e-05  -1.99365975e-05\n",
      "   1.29979804e-04  -3.58869577e-07  -1.37045159e-05  -3.09372894e-07\n",
      "  -2.11086422e-04   4.14243966e-07  -2.53191084e-04   1.05801131e-04\n",
      "   1.51923092e-04   3.65041493e-06  -1.08633628e-04  -3.53456916e-06\n",
      "  -2.70983467e-04  -2.01659289e-07   2.23026684e-07   8.84381218e-08\n",
      "   3.36530807e-09  -1.09205411e-04   8.74396924e-11   4.70628831e-05\n",
      "   1.96345421e-03  -3.90381786e-09  -8.10092415e-09  -8.92513072e-04\n",
      "   5.67776417e-04   8.04651114e-04  -1.42051745e-06  -1.11244137e-03\n",
      "  -3.88834565e-04  -1.11025034e-06  -2.06473293e-03  -4.00767575e-04\n",
      "   3.11882565e-11  -4.60979179e-04   3.50631274e-10  -2.72014502e-04\n",
      "  -6.27029586e-09   2.94378672e-03  -3.08619062e-04  -2.25128818e-06\n",
      "   1.53916780e-03  -2.89342124e-04   1.78901244e-08  -2.70983466e-04\n",
      "   9.64128910e-10  -1.72259553e-09  -3.61591435e-10  -9.29732853e-12\n",
      "   9.25604196e-04  -6.66973675e-14   2.58957581e-05   2.08197437e-03\n",
      "   6.58708549e-12   2.38413408e-11   7.91632190e-04   1.88567345e-03\n",
      "   7.73912574e-04   1.83479590e-08  -2.04496857e-04  -2.11898735e-05\n",
      "   1.12820753e-08  -7.14088363e-05   1.96113402e-04   3.52145528e-12\n",
      "  -1.43389249e-04   1.06081245e-12  -3.84969751e-04   4.48626777e-11\n",
      "  -5.59516767e-05   1.92707106e-04   1.82048494e-08  -6.15277788e-05\n",
      "  -1.75237460e-04  -5.45355104e-11  -2.70983466e-04  -2.66723493e-12\n",
      "   7.35651661e-12   8.63817142e-13   1.32245314e-14   1.41925622e-03\n",
      "   3.28432784e-17  -1.59200298e-06   8.26508492e-04  -1.04021080e-15\n",
      "  -4.20101722e-14  -2.25789592e-04   6.71036527e-04   7.42754253e-04\n",
      "  -1.32833990e-10  -1.49165316e-03  -2.38988147e-04  -6.74204983e-11\n",
      "  -2.57652763e-03  -1.75381754e-04  -7.28451489e-15  -2.80725367e-04\n",
      "  -4.10059929e-15  -5.84195422e-04  -1.77696727e-13   8.60285583e-04\n",
      "  -3.64400729e-04  -8.72690085e-11   5.80358101e-04  -4.21731403e-04\n",
      "   1.03770751e-13  -2.70983466e-04   4.42631153e-15  -1.84044461e-14\n",
      "  -1.24302973e-15  -7.72973332e-18  -8.90546930e-04  -1.03924958e-20\n",
      "  -8.47977415e-08  -1.32053015e-03   1.94078225e-18   4.52340149e-17\n",
      "   3.19183849e-05   2.16862367e-03   7.13515703e-04   5.68681769e-13\n",
      "   1.80548030e-04   1.59012893e-05   2.43377418e-13  -3.77595379e-05\n",
      "  -6.15189487e-05   2.94841389e-18   4.56482800e-05   5.92771774e-18\n",
      "  -9.03136950e-04   4.08095501e-16   1.17473792e-05  -1.39070431e-04\n",
      "   2.54139359e-13   9.01652654e-06   2.66381091e-05  -1.23474439e-16\n",
      "  -2.70983466e-04  -4.34329342e-18   2.68442616e-17   1.05940986e-18\n",
      "   2.89472990e-23   1.70691298e-04   2.03351157e-24   9.17186286e-09\n",
      "   3.49633855e-04  -8.97325195e-21  -2.91086541e-20  -2.39688933e-06\n",
      "   6.96894830e-04   6.86808147e-04  -1.42805782e-15  -7.59560512e-05\n",
      "   4.87032774e-05  -5.20461234e-16  -5.85045551e-05   4.89768145e-05\n",
      "  -6.50665915e-22   7.59861969e-05  -4.42903776e-21  -1.30089222e-03\n",
      "  -5.38082767e-19  -1.18547222e-04   7.40939197e-05  -4.40564609e-16\n",
      "  -7.13097909e-05   6.94083674e-05   8.88787338e-20  -2.70983466e-04\n",
      "   2.32110750e-21  -2.11291284e-20  -4.91954734e-22   4.36921756e-25\n",
      "  -1.24447410e-05  -2.23081711e-28   1.64961698e-11  -2.59035472e-05\n",
      "  -2.84325314e-24   1.02633031e-23   9.10615859e-08   1.86213577e-03\n",
      "   6.62623168e-04   1.94303235e-18  -3.10760228e-05  -1.57962520e-06\n",
      "   6.05729786e-19   7.05334514e-06   3.79655803e-06   3.67540569e-24\n",
      "  -3.26760686e-06   1.69269822e-24  -1.37404361e-03   3.76128572e-22\n",
      "  -3.54313415e-07   1.29086708e-05   4.17438500e-19  -3.27258845e-07\n",
      "  -6.48670748e-07  -3.52507193e-23  -2.70983466e-04  -5.20373431e-25\n",
      "   6.93196208e-24   9.58094000e-26   4.51071977e-28   2.50573753e-07\n",
      "   1.04523048e-32  -1.27513017e-11  -2.91416084e-07  -1.47613663e-27\n",
      "  -1.52262648e-27  -1.37312580e-09   5.64142821e-04   6.40740620e-04\n",
      "  -1.10507644e-21   4.66072780e-05  -2.19658667e-06  -2.95158731e-22\n",
      "   7.22459324e-05  -3.09928667e-06   1.73865398e-27  -4.60177068e-06\n",
      "  -2.61363318e-28   6.31542568e-04  -1.07518402e-25   3.62801213e-06\n",
      "  -3.75017976e-06  -1.66221952e-22   2.01407833e-06  -2.43896495e-06\n",
      "   5.89203715e-27] \n",
      "\n",
      "pred_RR for degree  10  =  0.809394544245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80.4136"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree_RR_test = 10\n",
    "\n",
    "tX1_poly = build_poly(tX1, degree_RR_test)\n",
    "tX2_poly = build_poly(tX2, degree_RR_test)\n",
    "\n",
    "lambda_RR_test = 5\n",
    "method = 4\n",
    "\n",
    "loss_RR_test, weights_RR_test = ridge_regression(y1, tX1_poly, lambda_RR_test)\n",
    "print(\"\\nweights_RR:\\n\",weights_RR_test,\"\\n\")\n",
    "\n",
    "pred_RR_test = prediction(y1, tX1_poly, 0, lambda_RR_test, 0, method)\n",
    "print(\"pred_RR for degree \", str(degree_RR_test), \" = \", pred_RR_test)\n",
    "\n",
    "\n",
    "y_pred_RR_test = predict_labels(weights_RR_test, tX2_poly)\n",
    "n_correct_RR = np.count_nonzero((y_pred_RR_test - y2) == 0)\n",
    "perc_correct_RR = n_correct_RR/(N/2) * 100\n",
    "perc_correct_RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_RR for degree  0  =  0.657165668663\n",
      "pred_RR on testing set for degree  0  =  65.6112 %\n",
      "pred_RR for degree  1  =  0.733546240852\n",
      "pred_RR on testing set for degree  1  =  73.0224 %\n",
      "pred_RR for degree  2  =  0.74876912841\n",
      "pred_RR on testing set for degree  2  =  74.6928 %\n",
      "pred_RR for degree  3  =  0.759015302728\n",
      "pred_RR on testing set for degree  3  =  75.7112 %\n",
      "pred_RR for degree  4  =  0.768622754491\n",
      "pred_RR on testing set for degree  4  =  76.80799999999999 %\n",
      "pred_RR for degree  5  =  0.77870924817\n",
      "pred_RR on testing set for degree  5  =  77.9576 %\n",
      "pred_RR for degree  6  =  0.791137724551\n",
      "pred_RR on testing set for degree  6  =  79.1112 %\n",
      "pred_RR for degree  7  =  0.480958083832\n",
      "pred_RR on testing set for degree  7  =  71.1 %\n",
      "pred_RR for degree  8  =  0.801809713906\n",
      "pred_RR on testing set for degree  8  =  80.0224 %\n",
      "pred_RR for degree  9  =  0.671856287425\n",
      "pred_RR on testing set for degree  9  =  75.0232 %\n",
      "pred_RR for degree  10  =  0.809394544245\n",
      "pred_RR on testing set for degree  10  =  80.4136 %\n",
      "pred_RR for degree  11  =  0.643459747172\n",
      "pred_RR on testing set for degree  11  =  81.2832 %\n",
      "pred_RR for degree  12  =  0.815089820359\n",
      "pred_RR on testing set for degree  12  =  81.444 %\n",
      "pred_RR for degree  13  =  0.814770459082\n",
      "pred_RR on testing set for degree  13  =  81.2696 %\n",
      "pred_RR for degree  14  =  0.795821689953\n",
      "pred_RR on testing set for degree  14  =  57.2832 %\n",
      "pred_RR for degree  15  =  0.783206919494\n",
      "pred_RR on testing set for degree  15  =  76.16159999999999 %\n",
      "pred_RR for degree  16  =  0.804843646041\n",
      "pred_RR on testing set for degree  16  =  65.8256 %\n",
      "pred_RR for degree  17  =  0.794011976048\n",
      "pred_RR on testing set for degree  17  =  76.14800000000001 %\n",
      "pred_RR for degree  18  =  0.632787757818\n",
      "pred_RR on testing set for degree  18  =  56.90880000000001 %\n",
      "pred_RR for degree  19  =  0.614291417166\n",
      "pred_RR on testing set for degree  19  =  76.6048 %\n"
     ]
    }
   ],
   "source": [
    "pred_RR_test_VEC = np.zeros(20)\n",
    "perc_correct_RR_VEC = np.zeros(20)\n",
    "\n",
    "degrees = range(0,20)\n",
    "\n",
    "for degree_RR_test in degrees:\n",
    "\n",
    "    tX1_poly = build_poly(tX1, degree_RR_test)\n",
    "    tX2_poly = build_poly(tX2, degree_RR_test)\n",
    "\n",
    "    lambda_RR_test = 5\n",
    "    method = 4\n",
    "\n",
    "    loss_RR_test, weights_RR_test = ridge_regression(y1, tX1_poly, lambda_RR_test)\n",
    "\n",
    "    pred_RR_test = prediction(y1, tX1_poly, 0, lambda_RR_test, 0, method)\n",
    "    print(\"pred_RR for degree \", str(degree_RR_test), \" = \", pred_RR_test)\n",
    "\n",
    "    pred_RR_test_VEC[degree_RR_test] = pred_RR_test*100\n",
    "\n",
    "    y_pred_RR_test = predict_labels(weights_RR_test, tX2_poly)\n",
    "    n_correct_RR = np.count_nonzero((y_pred_RR_test - y2) == 0)\n",
    "    \n",
    "    perc_correct_RR = n_correct_RR/(N/2) * 100\n",
    "    \n",
    "    perc_correct_RR_VEC[degree_RR_test] = perc_correct_RR\n",
    "    \n",
    "    print(\"pred_RR on testing set for degree \", str(degree_RR_test), \" = \", perc_correct_RR, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAEZCAYAAACn/z6eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x192e3cf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = plt.plot(degrees, pred_RR_test_VEC, degrees, perc_correct_RR_VEC)\n",
    "plt.title(\"Percentage of correct prediction in function of polynomial degree\") \n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('Correct prediction (%)')\n",
    "plt.legend(p, [\"On training set\", \"On testing set\"], bbox_to_anchor=(1.5, 1))\n",
    "plt.ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that with a 7 or more degree polynomial, we are overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights_LogR:\n",
      " [ -7.93971123e-03  -9.25427112e-01  -6.87095681e-01  -6.61222434e-01\n",
      "  -1.67225941e-01  -1.29103044e-02  -1.00695125e+00  -5.13309675e-03\n",
      "  -1.86704410e-02  -1.57261061e-01  -8.31681534e-01  -1.53710195e-02\n",
      "   8.75789549e-03  -2.72906766e-03  -1.81113730e-01   7.07021183e-06\n",
      "  -9.30049190e-05  -3.87601218e-01   2.48637077e-04  -2.40084245e-04\n",
      "  -3.13451695e-01   4.10680696e-04  -1.25211521e+00  -4.67453228e-03\n",
      "  -4.78044443e-01  -7.96808619e-05   1.75176394e-04  -3.92081664e-01\n",
      "   1.14565931e-04  -9.08258862e-05  -2.62966530e-01] \n",
      "\n",
      "pred_LogR =  0.657165668663\n"
     ]
    }
   ],
   "source": [
    "max_iters_LogR_test = 2000\n",
    "gamma_LogR_test = 1.0e-10\n",
    "method = 5\n",
    "\n",
    "loss_LogR_test, weights_LogR_test = logistic_regression(y1, tX1, gamma_LogR_test, max_iters_LogR_test)\n",
    "print(\"\\nweights_LogR:\\n\",weights_LogR_test,\"\\n\")\n",
    "\n",
    "pred_LogR_test = prediction(y1, tX1, gamma_LogR_test, 0, max_iters_LogR_test, method)\n",
    "print(\"pred_LogR = \", pred_LogR_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.6112"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_LogR_test = predict_labels(weights_LogR_test, tX2)\n",
    "n_correct_LogR = np.count_nonzero((y_pred_LogR_test - y2) == 0)\n",
    "perc_correct_LogR = n_correct_LogR/(N/2) * 100\n",
    "perc_correct_LogR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights_RLogR:\n",
      " [ -3.97591135e-03  -4.63417801e-01  -3.43851231e-01  -3.31086769e-01\n",
      "  -8.38949357e-02  -6.46798583e-03  -5.04951374e-01  -2.56499077e-03\n",
      "  -9.34990276e-03  -7.87331013e-02  -4.16658730e-01  -7.69410972e-03\n",
      "   4.38058657e-03  -1.36723968e-03  -9.07765961e-02   3.62428974e-06\n",
      "  -4.64316411e-05  -1.94069099e-01   1.24478454e-04  -1.20333767e-04\n",
      "  -1.56955876e-01   2.05426874e-04  -6.27166821e-01  -2.34213240e-03\n",
      "  -2.39448596e-01  -3.98241840e-05   8.77391719e-05  -1.96338684e-01\n",
      "   5.73464088e-05  -4.53979083e-05  -1.31813007e-01] \n",
      "\n",
      "pred_RLogR =  0.657165668663\n"
     ]
    }
   ],
   "source": [
    "max_iters_RLogR_test = 1000\n",
    "lambda_RLogR_test = 1.0e-8\n",
    "gamma_RLogR_test = 1.0e-10\n",
    "method = 5\n",
    "\n",
    "loss_RLogR_test, weights_RLogR_test = reg_logistic_regression(y1, tX1, gamma_RLogR_test, lambda_RLogR_test, max_iters_RLogR_test)\n",
    "print(\"\\nweights_RLogR:\\n\", weights_RLogR_test,\"\\n\")\n",
    "\n",
    "pred_RLogR_test = prediction(y1, tX1, gamma_RLogR_test, lambda_RLogR_test, max_iters_RLogR_test, 6)\n",
    "print(\"pred_RLogR = \", pred_RLogR_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.6112"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_RLogR_test = predict_labels(weights_RLogR_test, tX2)\n",
    "n_correct_RLogR = np.count_nonzero((y_pred_RLogR_test - y2) == 0)\n",
    "perc_correct_RLogR = n_correct_RLogR/(N/2) * 100\n",
    "perc_correct_RLogR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the entire training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression - gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iters_GD = 5000\n",
    "gamma_GD = 1.0e-8\n",
    "method = 1\n",
    "\n",
    "loss_GD, weights_GD = least_squares_GD(y, tX_stand, gamma_GD, max_iters_GD)\n",
    "pred_GD = prediction(y, tX_stand, gamma_GD, 0, max_iters_GD, method)\n",
    "\n",
    "print(\"\\nweights_GD:\\n\",weights_GD,\"\\n\")\n",
    "print(\"pred_GD = \", pred_GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression - stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iters_SGD = 1000\n",
    "gamma_SGD = 1.0e-8\n",
    "method = 2\n",
    "\n",
    "loss_SGD, weights_SGD = least_squares_SGD(y, tX_stand, gamma_SGD, max_iters_SGD)\n",
    "print(\"\\nweights_SGD:\\n\",weights_SGD,\"\\n\")\n",
    "\n",
    "pred_SGD = prediction(y, tX_stand, gamma_SGD, 0, max_iters_SGD, method)\n",
    "print(\"pred_SGD = \", pred_SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method = 3\n",
    "\n",
    "tX_cleaned_TEST = tX_cleaned.copy()\n",
    "w0 = np.ones([1,tX_cleaned_TEST.shape[0]])\n",
    "tX_cleaned_TEST = np.insert(tX_cleaned_TEST, 0, w0, axis=1)\n",
    "\n",
    "loss_LeastS, weights_LeastS = least_squares(y, tX_cleaned_TEST)\n",
    "print(\"\\nweights_LeastS:\\n\",weights_LeastS,\"\\n\")\n",
    "\n",
    "pred_LeastS = prediction(y, tX_cleaned_TEST, 0, 0, 0, method)\n",
    "print(\"pred_LeastS = \", pred_LeastS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_RR =  0.619521355228\n"
     ]
    }
   ],
   "source": [
    "lambda_RR = 5\n",
    "method = 4\n",
    "\n",
    "degree_RR = 12\n",
    "\n",
    "tX_cleaned_TEST = tX_cleaned.copy()\n",
    "w0 = np.ones([1,tX_cleaned_TEST.shape[0]])\n",
    "tX_cleaned_TEST = np.insert(tX_cleaned_TEST, 0, w0, axis=1)\n",
    "\n",
    "tX_poly_TEST = build_poly(tX_cleaned_TEST, degree_RR)\n",
    "\n",
    "loss_RR, weights_RR = ridge_regression(y, tX_poly_TEST, lambda_RR)\n",
    "# print(\"\\nweights_RR:\\n\", weights_RR,\"\\n\")\n",
    "\n",
    "pred_RR = prediction(y, tX_poly_TEST, 0, lambda_RR, 0, method)\n",
    "print(\"pred_RR = \", pred_RR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iters_LogR = 1000\n",
    "gamma_LogR = 1.0e-6\n",
    "method = 5\n",
    "\n",
    "loss_LogR, weights_LogR = logistic_regression(y, tX_stand, gamma_LogR, max_iters_LogR)\n",
    "print(\"\\nweights_LogR:\\n\",weights_LogR,\"\\n\")\n",
    "\n",
    "pred_LogR = prediction(y, tX_stand, gamma_LogR, 0, max_iters_LogR, method)\n",
    "print(\"pred_LogR = \", pred_LogR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iters_RLogR = 1000\n",
    "lambda_RLogR = 2\n",
    "gamma_RLogR = 1.0e-8\n",
    "method = 5\n",
    "\n",
    "loss_RLogR, weights_RLogR = reg_logistic_regression(y, tX_stand, gamma_RLogR, lambda_RLogR, max_iters_RLogR)\n",
    "print(\"\\nweights_RLogR:\\n\",weights_RLogR,\"\\n\")\n",
    "\n",
    "pred_RLogR = prediction(y, tX_stand, gamma_RLogR, lambda_RLogR, max_iters_RLogR, 6)\n",
    "print(\"pred_RLogR = \", pred_RLogR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serparating the data by JET values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entire set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separating_by_jet(tX):\n",
    "    \n",
    "    # JET 1\n",
    "    tX0_index = tX[:, 22] == 0.0\n",
    "    tX0 = tX[tX0_index]\n",
    "    \n",
    "    # JET 2\n",
    "    tX1_index = tX[:, 22] == 1.0\n",
    "    tX1 = tX[tX1_index]\n",
    "    \n",
    "    # JET 3\n",
    "    tX2_index = tX[:, 22] == 2.0\n",
    "    tX2 = tX[tX2_index]\n",
    "    \n",
    "    # JET 4\n",
    "    tX3_index = tX[:, 22] == 3.0\n",
    "    tX3 = tX[tX3_index]\n",
    "    \n",
    "    return tX0, tX0_index, tX1, tX1_index, tX2, tX2_index, tX3, tX3_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data_jet(tX) :\n",
    "    \n",
    "    (nbrRows, nbrColumn) = tX.shape\n",
    "    \n",
    "    tX_cleaned, _ = clean_data(tX)\n",
    "    tX_reformed = tX_cleaned.copy()\n",
    "    nbrOfDelete = 0\n",
    "    \n",
    "    for iColumn in range(0, nbrColumn) :\n",
    "        if np.isnan(tX_cleaned[:, iColumn]).all():\n",
    "            tX_reformed = np.delete(tX_reformed, iColumn - nbrOfDelete, 1)\n",
    "            nbrOfDelete = nbrOfDelete + 1\n",
    "        \n",
    "    return tX_reformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "tX_jet0, indexes_jet0, tX_jet1, indexes_jet1, tX_jet2, indexes_jet2, tX_jet3, indexes_jet3 = separating_by_jet(tX)\n",
    "y_jet0 = y[indexes_jet0]\n",
    "y_jet1 = y[indexes_jet1]\n",
    "y_jet2 = y[indexes_jet2]\n",
    "y_jet3 = y[indexes_jet3]\n",
    "tX_cleaned_jet0 = clean_data_jet(tX_jet0)\n",
    "tX_cleaned_jet1 = clean_data_jet(tX_jet1)\n",
    "tX_cleaned_jet2 = clean_data_jet(tX_jet2)\n",
    "tX_cleaned_jet3 = clean_data_jet(tX_jet3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "tX_cleaned_jet0_TEST = md.clean_data(tX_jet0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>2.354</td>\n",
       "      <td>-1.285</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.655</td>\n",
       "      <td>0.010</td>\n",
       "      <td>53.321</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>-3.100</td>\n",
       "      <td>31.082</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>1.056</td>\n",
       "      <td>-1.385</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.197</td>\n",
       "      <td>-2.231</td>\n",
       "      <td>29.774</td>\n",
       "      <td>0.798</td>\n",
       "      <td>1.569</td>\n",
       "      <td>2.723</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>105.594</td>\n",
       "      <td>50.559</td>\n",
       "      <td>100.989</td>\n",
       "      <td>4.288</td>\n",
       "      <td>2.904</td>\n",
       "      <td>4.288</td>\n",
       "      <td>65.333</td>\n",
       "      <td>0.675</td>\n",
       "      <td>-1.366</td>\n",
       "      <td>...</td>\n",
       "      <td>2.433</td>\n",
       "      <td>-2.532</td>\n",
       "      <td>26.325</td>\n",
       "      <td>0.210</td>\n",
       "      <td>1.884</td>\n",
       "      <td>37.791</td>\n",
       "      <td>0.024</td>\n",
       "      <td>129.804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>111.452</td>\n",
       "      <td>86.240</td>\n",
       "      <td>79.692</td>\n",
       "      <td>27.201</td>\n",
       "      <td>2.338</td>\n",
       "      <td>27.201</td>\n",
       "      <td>81.734</td>\n",
       "      <td>1.750</td>\n",
       "      <td>-1.412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>2.878</td>\n",
       "      <td>52.016</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-1.288</td>\n",
       "      <td>51.276</td>\n",
       "      <td>0.688</td>\n",
       "      <td>250.178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>82.488</td>\n",
       "      <td>31.663</td>\n",
       "      <td>64.128</td>\n",
       "      <td>8.232</td>\n",
       "      <td>2.823</td>\n",
       "      <td>8.232</td>\n",
       "      <td>58.649</td>\n",
       "      <td>1.303</td>\n",
       "      <td>-1.414</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>-2.990</td>\n",
       "      <td>33.179</td>\n",
       "      <td>-1.665</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>12.439</td>\n",
       "      <td>1.433</td>\n",
       "      <td>163.420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>111.452</td>\n",
       "      <td>109.412</td>\n",
       "      <td>14.398</td>\n",
       "      <td>17.323</td>\n",
       "      <td>0.472</td>\n",
       "      <td>17.323</td>\n",
       "      <td>62.565</td>\n",
       "      <td>1.774</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>...</td>\n",
       "      <td>1.389</td>\n",
       "      <td>1.340</td>\n",
       "      <td>40.013</td>\n",
       "      <td>1.856</td>\n",
       "      <td>1.412</td>\n",
       "      <td>75.197</td>\n",
       "      <td>-1.583</td>\n",
       "      <td>198.616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>111.026</td>\n",
       "      <td>32.096</td>\n",
       "      <td>75.271</td>\n",
       "      <td>23.067</td>\n",
       "      <td>3.205</td>\n",
       "      <td>23.067</td>\n",
       "      <td>69.649</td>\n",
       "      <td>1.276</td>\n",
       "      <td>-1.414</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.107</td>\n",
       "      <td>-1.903</td>\n",
       "      <td>39.043</td>\n",
       "      <td>-1.944</td>\n",
       "      <td>1.191</td>\n",
       "      <td>19.959</td>\n",
       "      <td>2.415</td>\n",
       "      <td>122.176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>127.861</td>\n",
       "      <td>50.953</td>\n",
       "      <td>77.267</td>\n",
       "      <td>26.967</td>\n",
       "      <td>2.833</td>\n",
       "      <td>26.967</td>\n",
       "      <td>79.503</td>\n",
       "      <td>1.586</td>\n",
       "      <td>1.401</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.635</td>\n",
       "      <td>2.603</td>\n",
       "      <td>48.764</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>17.557</td>\n",
       "      <td>-2.975</td>\n",
       "      <td>211.720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels        0        1        2       3      4       5       6      7  \\\n",
       "0    -1.0  143.905   81.417   80.943   0.414  3.310   0.414  75.968  2.354   \n",
       "1    -1.0  175.864   16.915  134.805  16.405  3.891  16.405  57.983  1.056   \n",
       "2    -1.0  105.594   50.559  100.989   4.288  2.904   4.288  65.333  0.675   \n",
       "3    -1.0  111.452   86.240   79.692  27.201  2.338  27.201  81.734  1.750   \n",
       "4    -1.0   82.488   31.663   64.128   8.232  2.823   8.232  58.649  1.303   \n",
       "5    -1.0  111.452  109.412   14.398  17.323  0.472  17.323  62.565  1.774   \n",
       "6     1.0  111.026   32.096   75.271  23.067  3.205  23.067  69.649  1.276   \n",
       "7     1.0  127.861   50.953   77.267  26.967  2.833  26.967  79.503  1.586   \n",
       "\n",
       "       8 ...      10     11      12     13     14      15     16       17  \\\n",
       "0 -1.285 ...  -1.655  0.010  53.321 -0.522 -3.100  31.082  0.060   86.062   \n",
       "1 -1.385 ...  -2.197 -2.231  29.774  0.798  1.569   2.723 -0.871   53.131   \n",
       "2 -1.366 ...   2.433 -2.532  26.325  0.210  1.884  37.791  0.024  129.804   \n",
       "3 -1.412 ...  -0.866  2.878  52.016  0.126 -1.288  51.276  0.688  250.178   \n",
       "4 -1.414 ...  -0.654 -2.990  33.179 -1.665 -0.354  12.439  1.433  163.420   \n",
       "5 -0.272 ...   1.389  1.340  40.013  1.856  1.412  75.197 -1.583  198.616   \n",
       "6 -1.414 ...  -1.107 -1.903  39.043 -1.944  1.191  19.959  2.415  122.176   \n",
       "7  1.401 ...  -0.635  2.603  48.764 -0.343 -0.862  17.557 -2.975  211.720   \n",
       "\n",
       "    18   19  \n",
       "0  0.0  0.0  \n",
       "1  0.0  0.0  \n",
       "2  0.0  0.0  \n",
       "3  0.0  0.0  \n",
       "4  0.0  0.0  \n",
       "5  0.0  0.0  \n",
       "6  0.0  0.0  \n",
       "7  0.0  0.0  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jet0 = pd.concat([pd.DataFrame({'labels': y_jet0}), pd.DataFrame(tX_cleaned_jet0)], axis=1)\n",
    "df_jet0.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>2.354</td>\n",
       "      <td>-1.285</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.655</td>\n",
       "      <td>0.010</td>\n",
       "      <td>53.321</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>-3.100</td>\n",
       "      <td>31.082</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>1.056</td>\n",
       "      <td>-1.385</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.197</td>\n",
       "      <td>-2.231</td>\n",
       "      <td>29.774</td>\n",
       "      <td>0.798</td>\n",
       "      <td>1.569</td>\n",
       "      <td>2.723</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>105.594</td>\n",
       "      <td>50.559</td>\n",
       "      <td>100.989</td>\n",
       "      <td>4.288</td>\n",
       "      <td>2.904</td>\n",
       "      <td>4.288</td>\n",
       "      <td>65.333</td>\n",
       "      <td>0.675</td>\n",
       "      <td>-1.366</td>\n",
       "      <td>...</td>\n",
       "      <td>2.433</td>\n",
       "      <td>-2.532</td>\n",
       "      <td>26.325</td>\n",
       "      <td>0.210</td>\n",
       "      <td>1.884</td>\n",
       "      <td>37.791</td>\n",
       "      <td>0.024</td>\n",
       "      <td>129.804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>111.452</td>\n",
       "      <td>86.240</td>\n",
       "      <td>79.692</td>\n",
       "      <td>27.201</td>\n",
       "      <td>2.338</td>\n",
       "      <td>27.201</td>\n",
       "      <td>81.734</td>\n",
       "      <td>1.750</td>\n",
       "      <td>-1.412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>2.878</td>\n",
       "      <td>52.016</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-1.288</td>\n",
       "      <td>51.276</td>\n",
       "      <td>0.688</td>\n",
       "      <td>250.178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>82.488</td>\n",
       "      <td>31.663</td>\n",
       "      <td>64.128</td>\n",
       "      <td>8.232</td>\n",
       "      <td>2.823</td>\n",
       "      <td>8.232</td>\n",
       "      <td>58.649</td>\n",
       "      <td>1.303</td>\n",
       "      <td>-1.414</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>-2.990</td>\n",
       "      <td>33.179</td>\n",
       "      <td>-1.665</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>12.439</td>\n",
       "      <td>1.433</td>\n",
       "      <td>163.420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>111.452</td>\n",
       "      <td>109.412</td>\n",
       "      <td>14.398</td>\n",
       "      <td>17.323</td>\n",
       "      <td>0.472</td>\n",
       "      <td>17.323</td>\n",
       "      <td>62.565</td>\n",
       "      <td>1.774</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>...</td>\n",
       "      <td>1.389</td>\n",
       "      <td>1.340</td>\n",
       "      <td>40.013</td>\n",
       "      <td>1.856</td>\n",
       "      <td>1.412</td>\n",
       "      <td>75.197</td>\n",
       "      <td>-1.583</td>\n",
       "      <td>198.616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>111.026</td>\n",
       "      <td>32.096</td>\n",
       "      <td>75.271</td>\n",
       "      <td>23.067</td>\n",
       "      <td>3.205</td>\n",
       "      <td>23.067</td>\n",
       "      <td>69.649</td>\n",
       "      <td>1.276</td>\n",
       "      <td>-1.414</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.107</td>\n",
       "      <td>-1.903</td>\n",
       "      <td>39.043</td>\n",
       "      <td>-1.944</td>\n",
       "      <td>1.191</td>\n",
       "      <td>19.959</td>\n",
       "      <td>2.415</td>\n",
       "      <td>122.176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>127.861</td>\n",
       "      <td>50.953</td>\n",
       "      <td>77.267</td>\n",
       "      <td>26.967</td>\n",
       "      <td>2.833</td>\n",
       "      <td>26.967</td>\n",
       "      <td>79.503</td>\n",
       "      <td>1.586</td>\n",
       "      <td>1.401</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.635</td>\n",
       "      <td>2.603</td>\n",
       "      <td>48.764</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>17.557</td>\n",
       "      <td>-2.975</td>\n",
       "      <td>211.720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels        0        1        2       3      4       5       6      7  \\\n",
       "0    -1.0  143.905   81.417   80.943   0.414  3.310   0.414  75.968  2.354   \n",
       "1    -1.0  175.864   16.915  134.805  16.405  3.891  16.405  57.983  1.056   \n",
       "2    -1.0  105.594   50.559  100.989   4.288  2.904   4.288  65.333  0.675   \n",
       "3    -1.0  111.452   86.240   79.692  27.201  2.338  27.201  81.734  1.750   \n",
       "4    -1.0   82.488   31.663   64.128   8.232  2.823   8.232  58.649  1.303   \n",
       "5    -1.0  111.452  109.412   14.398  17.323  0.472  17.323  62.565  1.774   \n",
       "6     1.0  111.026   32.096   75.271  23.067  3.205  23.067  69.649  1.276   \n",
       "7     1.0  127.861   50.953   77.267  26.967  2.833  26.967  79.503  1.586   \n",
       "\n",
       "       8 ...      10     11      12     13     14      15     16       17  \\\n",
       "0 -1.285 ...  -1.655  0.010  53.321 -0.522 -3.100  31.082  0.060   86.062   \n",
       "1 -1.385 ...  -2.197 -2.231  29.774  0.798  1.569   2.723 -0.871   53.131   \n",
       "2 -1.366 ...   2.433 -2.532  26.325  0.210  1.884  37.791  0.024  129.804   \n",
       "3 -1.412 ...  -0.866  2.878  52.016  0.126 -1.288  51.276  0.688  250.178   \n",
       "4 -1.414 ...  -0.654 -2.990  33.179 -1.665 -0.354  12.439  1.433  163.420   \n",
       "5 -0.272 ...   1.389  1.340  40.013  1.856  1.412  75.197 -1.583  198.616   \n",
       "6 -1.414 ...  -1.107 -1.903  39.043 -1.944  1.191  19.959  2.415  122.176   \n",
       "7  1.401 ...  -0.635  2.603  48.764 -0.343 -0.862  17.557 -2.975  211.720   \n",
       "\n",
       "    18   19  \n",
       "0  0.0  0.0  \n",
       "1  0.0  0.0  \n",
       "2  0.0  0.0  \n",
       "3  0.0  0.0  \n",
       "4  0.0  0.0  \n",
       "5  0.0  0.0  \n",
       "6  0.0  0.0  \n",
       "7  0.0  0.0  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jet0 = pd.concat([pd.DataFrame({'labels': y_jet0}), pd.DataFrame(tX_cleaned_jet0_TEST)], axis=1)\n",
    "df_jet0.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105.594</td>\n",
       "      <td>50.559</td>\n",
       "      <td>100.989</td>\n",
       "      <td>4.288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.904</td>\n",
       "      <td>4.288</td>\n",
       "      <td>65.333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024</td>\n",
       "      <td>129.804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111.452</td>\n",
       "      <td>86.240</td>\n",
       "      <td>79.692</td>\n",
       "      <td>27.201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.338</td>\n",
       "      <td>27.201</td>\n",
       "      <td>81.734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688</td>\n",
       "      <td>250.178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82.488</td>\n",
       "      <td>31.663</td>\n",
       "      <td>64.128</td>\n",
       "      <td>8.232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.823</td>\n",
       "      <td>8.232</td>\n",
       "      <td>58.649</td>\n",
       "      <td>...</td>\n",
       "      <td>1.433</td>\n",
       "      <td>163.420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>111.452</td>\n",
       "      <td>109.412</td>\n",
       "      <td>14.398</td>\n",
       "      <td>17.323</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.472</td>\n",
       "      <td>17.323</td>\n",
       "      <td>62.565</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.583</td>\n",
       "      <td>198.616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>111.026</td>\n",
       "      <td>32.096</td>\n",
       "      <td>75.271</td>\n",
       "      <td>23.067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.205</td>\n",
       "      <td>23.067</td>\n",
       "      <td>69.649</td>\n",
       "      <td>...</td>\n",
       "      <td>2.415</td>\n",
       "      <td>122.176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>127.861</td>\n",
       "      <td>50.953</td>\n",
       "      <td>77.267</td>\n",
       "      <td>26.967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.833</td>\n",
       "      <td>26.967</td>\n",
       "      <td>79.503</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.975</td>\n",
       "      <td>211.720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2       3   4   5   6      7       8       9  ...   \\\n",
       "0  143.905   81.417   80.943   0.414 NaN NaN NaN  3.310   0.414  75.968 ...    \n",
       "1  175.864   16.915  134.805  16.405 NaN NaN NaN  3.891  16.405  57.983 ...    \n",
       "2  105.594   50.559  100.989   4.288 NaN NaN NaN  2.904   4.288  65.333 ...    \n",
       "3  111.452   86.240   79.692  27.201 NaN NaN NaN  2.338  27.201  81.734 ...    \n",
       "4   82.488   31.663   64.128   8.232 NaN NaN NaN  2.823   8.232  58.649 ...    \n",
       "5  111.452  109.412   14.398  17.323 NaN NaN NaN  0.472  17.323  62.565 ...    \n",
       "6  111.026   32.096   75.271  23.067 NaN NaN NaN  3.205  23.067  69.649 ...    \n",
       "7  127.861   50.953   77.267  26.967 NaN NaN NaN  2.833  26.967  79.503 ...    \n",
       "\n",
       "      20       21   22  23  24  25  26  27  28   29  \n",
       "0  0.060   86.062  0.0 NaN NaN NaN NaN NaN NaN  0.0  \n",
       "1 -0.871   53.131  0.0 NaN NaN NaN NaN NaN NaN  0.0  \n",
       "2  0.024  129.804  0.0 NaN NaN NaN NaN NaN NaN  0.0  \n",
       "3  0.688  250.178  0.0 NaN NaN NaN NaN NaN NaN  0.0  \n",
       "4  1.433  163.420  0.0 NaN NaN NaN NaN NaN NaN  0.0  \n",
       "5 -1.583  198.616  0.0 NaN NaN NaN NaN NaN NaN  0.0  \n",
       "6  2.415  122.176  0.0 NaN NaN NaN NaN NaN NaN  0.0  \n",
       "7 -2.975  211.720  0.0 NaN NaN NaN NaN NaN NaN  0.0  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_values, _ = clean_data(tX_jet0)\n",
    "TEST = pd.DataFrame(test_values)\n",
    "TEST.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99913, 20)"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_cleaned_jet0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77544, 23)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_cleaned_jet1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50379, 30)"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_cleaned_jet2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22164, 30)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_cleaned_jet3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "tX1_jet0, indexes1_jet0, tX1_jet1, indexes1_jet1, tX1_jet2, indexes1_jet2, tX1_jet3, indexes1_jet3 = separating_by_jet(tX1)\n",
    "tX2_jet0, indexes2_jet0, tX2_jet1, indexes2_jet1, tX2_jet2, indexes2_jet2, tX2_jet3, indexes2_jet3 = separating_by_jet(tX2)\n",
    "\n",
    "y1_jet0 = y1[indexes1_jet0]\n",
    "y1_jet1 = y1[indexes1_jet1]\n",
    "y1_jet2 = y1[indexes1_jet2]\n",
    "y1_jet3 = y1[indexes1_jet3]\n",
    "\n",
    "y2_jet0 = y2[indexes2_jet0]\n",
    "y2_jet1 = y2[indexes2_jet1]\n",
    "y2_jet2 = y2[indexes2_jet2]\n",
    "y2_jet3 = y2[indexes2_jet3]\n",
    "\n",
    "tX1_cleaned_jet0 = clean_data_jet(tX1_jet0)\n",
    "tX1_cleaned_jet1 = clean_data_jet(tX1_jet1)\n",
    "tX1_cleaned_jet2 = clean_data_jet(tX1_jet2)\n",
    "tX1_cleaned_jet3 = clean_data_jet(tX1_jet3)\n",
    "\n",
    "tX2_cleaned_jet0 = clean_data_jet(tX2_jet0)\n",
    "tX2_cleaned_jet1 = clean_data_jet(tX2_jet1)\n",
    "tX2_cleaned_jet2 = clean_data_jet(tX2_jet2)\n",
    "tX2_cleaned_jet3 = clean_data_jet(tX2_jet3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data for each JET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambdas = [1e-10, 2e-10, 3e-10, 4e-10, 5e-10, 6e-10, 7e-10, 8e-10, 9e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-3, 1e-2, 1e-1, 1, 10]\n",
    "degrees = range(0, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JET == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction jet1 =  0.814108133396  with lambda:  1e-10  degree:  1\n",
      "prediction jet1 =  0.814108133396  with lambda:  2e-10  degree:  1\n",
      "prediction jet1 =  0.814108133396  with lambda:  3e-10  degree:  1\n",
      "prediction jet1 =  0.814108133396  with lambda:  4e-10  degree:  1\n",
      "prediction jet1 =  0.814108133396  with lambda:  5e-10  degree:  1\n",
      "prediction jet1 =  0.814108133396  with lambda:  6e-10  degree:  1\n",
      "prediction jet1 =  0.814108133396  with lambda:  7e-10  degree:  1\n",
      "prediction jet1 =  0.814108133396  with lambda:  8e-10  degree:  1\n",
      "prediction jet1 =  0.814108133396  with lambda:  9e-10  degree:  1\n",
      "prediction jet1 =  0.814108133396  with lambda:  1e-09  degree:  1\n",
      "prediction jet1 =  0.814074717637  with lambda:  1e-08  degree:  1\n",
      "prediction jet1 =  0.814108133396  with lambda:  1e-07  degree:  1\n",
      "prediction jet1 =  0.814074717637  with lambda:  1e-06  degree:  1\n",
      "prediction jet1 =  0.814074717637  with lambda:  1e-05  degree:  1\n",
      "prediction jet1 =  0.814007886119  with lambda:  0.001  degree:  1\n",
      "prediction jet1 =  0.811535119963  with lambda:  0.01  degree:  1\n",
      "prediction jet1 =  0.806322261579  with lambda:  0.1  degree:  1\n",
      "prediction jet1 =  0.802780191138  with lambda:  1  degree:  1\n",
      "prediction jet1 =  0.802746775379  with lambda:  10  degree:  1\n",
      "prediction jet1 =  0.825135333823  with lambda:  1e-10  degree:  2\n",
      "prediction jet1 =  0.825135333823  with lambda:  2e-10  degree:  2\n",
      "prediction jet1 =  0.825135333823  with lambda:  3e-10  degree:  2\n",
      "prediction jet1 =  0.825135333823  with lambda:  4e-10  degree:  2\n",
      "prediction jet1 =  0.825135333823  with lambda:  5e-10  degree:  2\n",
      "prediction jet1 =  0.825135333823  with lambda:  6e-10  degree:  2\n",
      "prediction jet1 =  0.825135333823  with lambda:  7e-10  degree:  2\n",
      "prediction jet1 =  0.825135333823  with lambda:  8e-10  degree:  2\n",
      "prediction jet1 =  0.825135333823  with lambda:  9e-10  degree:  2\n",
      "prediction jet1 =  0.825135333823  with lambda:  1e-09  degree:  2\n",
      "prediction jet1 =  0.825135333823  with lambda:  1e-08  degree:  2\n",
      "prediction jet1 =  0.825168749582  with lambda:  1e-07  degree:  2\n",
      "prediction jet1 =  0.825135333823  with lambda:  1e-06  degree:  2\n",
      "prediction jet1 =  0.825135333823  with lambda:  1e-05  degree:  2\n",
      "prediction jet1 =  0.824366771369  with lambda:  0.001  degree:  2\n",
      "prediction jet1 =  0.824132861057  with lambda:  0.01  degree:  2\n",
      "prediction jet1 =  0.822996725256  with lambda:  0.1  degree:  2\n",
      "prediction jet1 =  0.817650203836  with lambda:  1  degree:  2\n",
      "prediction jet1 =  0.812403929693  with lambda:  10  degree:  2\n",
      "prediction jet1 =  0.830247944931  with lambda:  1e-10  degree:  3\n",
      "prediction jet1 =  0.83028136069  with lambda:  2e-10  degree:  3\n",
      "prediction jet1 =  0.83028136069  with lambda:  3e-10  degree:  3\n",
      "prediction jet1 =  0.830247944931  with lambda:  4e-10  degree:  3\n",
      "prediction jet1 =  0.83028136069  with lambda:  5e-10  degree:  3\n",
      "prediction jet1 =  0.830314776449  with lambda:  6e-10  degree:  3\n",
      "prediction jet1 =  0.830181113413  with lambda:  7e-10  degree:  3\n",
      "prediction jet1 =  0.830247944931  with lambda:  8e-10  degree:  3\n",
      "prediction jet1 =  0.830314776449  with lambda:  9e-10  degree:  3\n",
      "prediction jet1 =  0.83028136069  with lambda:  1e-09  degree:  3\n",
      "prediction jet1 =  0.830247944931  with lambda:  1e-08  degree:  3\n",
      "prediction jet1 =  0.830247944931  with lambda:  1e-07  degree:  3\n",
      "prediction jet1 =  0.83028136069  with lambda:  1e-06  degree:  3\n",
      "prediction jet1 =  0.830247944931  with lambda:  1e-05  degree:  3\n",
      "prediction jet1 =  0.82998061886  with lambda:  0.001  degree:  3\n",
      "prediction jet1 =  0.829512798236  with lambda:  0.01  degree:  3\n",
      "prediction jet1 =  0.824801176235  with lambda:  0.1  degree:  3\n",
      "prediction jet1 =  0.821559847624  with lambda:  1  degree:  3\n",
      "prediction jet1 =  0.815879168616  with lambda:  10  degree:  3\n",
      "prediction jet1 =  0.799572278286  with lambda:  1e-10  degree:  4\n",
      "prediction jet1 =  0.828811067299  with lambda:  6e-10  degree:  4\n",
      "prediction jet1 =  0.833756599612  with lambda:  1e-08  degree:  4\n",
      "prediction jet1 =  0.833389026265  with lambda:  1e-07  degree:  4\n",
      "prediction jet1 =  0.833389026265  with lambda:  1e-06  degree:  4\n",
      "prediction jet1 =  0.83382343113  with lambda:  1e-05  degree:  4\n",
      "prediction jet1 =  0.832921205641  with lambda:  0.001  degree:  4\n",
      "prediction jet1 =  0.831283833456  with lambda:  0.01  degree:  4\n",
      "prediction jet1 =  0.827407605427  with lambda:  0.1  degree:  4\n",
      "prediction jet1 =  0.823865534986  with lambda:  1  degree:  4\n",
      "prediction jet1 =  0.822896477979  with lambda:  10  degree:  4\n",
      "prediction jet1 =  0.837031343982  with lambda:  1e-10  degree:  5\n",
      "prediction jet1 =  0.820056138475  with lambda:  3e-10  degree:  5\n",
      "prediction jet1 =  0.813840807325  with lambda:  4e-10  degree:  5\n",
      "prediction jet1 =  0.833589520818  with lambda:  7e-10  degree:  5\n",
      "prediction jet1 =  0.819454654815  with lambda:  8e-10  degree:  5\n",
      "prediction jet1 =  0.836663770634  with lambda:  0.001  degree:  5\n",
      "prediction jet1 =  0.833288778988  with lambda:  0.01  degree:  5\n",
      "prediction jet1 =  0.830983091626  with lambda:  0.1  degree:  5\n",
      "prediction jet1 =  0.82967987703  with lambda:  1  degree:  5\n",
      "prediction jet1 =  0.829746708548  with lambda:  10  degree:  5\n",
      "prediction jet1 =  0.809196016842  with lambda:  0.1  degree:  6\n",
      "prediction jet1 =  0.835928623939  with lambda:  1  degree:  6\n",
      "prediction jet1 =  0.830080866136  with lambda:  10  degree:  6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-439-6fd840cfa1e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtX_poly_jet0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_reformed_jet0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss_jet0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_jet0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_jet0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_poly_jet0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-394afe71e4f2>\u001b[0m in \u001b[0;36mbuild_poly\u001b[0;34m(x, degree)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mpoly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpoly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#poly = np.c_[poly, np.absolute(np.sqrt(x))]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidrivollet/anaconda/lib/python3.5/site-packages/numpy/lib/index_tricks.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred_jet0_VEC = np.zeros((len(lambdas)*len(degrees), 1))\n",
    "i = 0\n",
    "\n",
    "method = 4\n",
    "\n",
    "tX_reformed_jet0 = tX_cleaned_jet0.copy()\n",
    "w0 = np.ones([1,tX_reformed_jet0.shape[0]])\n",
    "tX_reformed_jet0 = np.insert(tX_reformed_jet0, 0, w0, axis=1)\n",
    "\n",
    "for deg in degrees:\n",
    "    for lam in lambdas:\n",
    "\n",
    "        tX_poly_jet0 = build_poly(tX_reformed_jet0, deg)\n",
    "\n",
    "        loss_jet0, weights_jet0 = ridge_regression(y_jet0, tX_poly_jet0, lam)\n",
    "        # print(\"\\nweights_RR:\\n\", weights_jet0,\"\\n\")\n",
    "\n",
    "        pred_jet0 = prediction(y_jet0, tX_poly_jet0, 0, lam, 0, method)\n",
    "        pred_jet0_VEC[i] = pred_jet0\n",
    "        i = i + 1\n",
    "        if pred_jet0 > 0.83 :\n",
    "            print(\"prediction jet1 = \", pred_jet0, \" with lambda: \", str(lam), \" degree: \", str(deg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x133c8c780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = plt.plot(pred_jet0_VEC[:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_RR =  0.830314776449\n"
     ]
    }
   ],
   "source": [
    "lambda_jet0 = 6e-10\n",
    "method = 4\n",
    "\n",
    "degree_jet0 = 3\n",
    "\n",
    "tX_reformed_jet0 = tX_cleaned_jet0.copy()\n",
    "w0 = np.ones([1,tX_reformed_jet0.shape[0]])\n",
    "tX_reformed_jet0 = np.insert(tX_reformed_jet0, 0, w0, axis=1)\n",
    "\n",
    "tX_poly_jet0 = build_poly(tX_reformed_jet0, degree_jet0)\n",
    "\n",
    "loss_jet0, weights_jet0 = ridge_regression(y_jet0, tX_poly_jet0, lambda_jet0)\n",
    "# print(\"\\nweights_RR:\\n\", weights_jet0,\"\\n\")\n",
    "\n",
    "pred_jet0 = prediction(y_jet0, tX_poly_jet0, 0, lambda_jet0, 0, method)\n",
    "print(\"pred_RR = \", pred_jet0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JET == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-449-42218be43fb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtX_poly_jet1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_reformed_jet1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss_jet1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_jet1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_jet1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_poly_jet1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-394afe71e4f2>\u001b[0m in \u001b[0;36mbuild_poly\u001b[0;34m(x, degree)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mpoly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpoly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#poly = np.c_[poly, np.absolute(np.sqrt(x))]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred_jet1_VEC = np.zeros((len(lambdas)*len(degrees), 1))\n",
    "i = 0\n",
    "\n",
    "method = 4\n",
    "\n",
    "tX_reformed_jet1 = tX_cleaned_jet1.copy()\n",
    "w0 = np.ones([1,tX_reformed_jet1.shape[0]])\n",
    "tX_reformed_jet1 = np.insert(tX_reformed_jet1, 0, w0, axis=1)\n",
    "\n",
    "for deg in degrees:\n",
    "    for lam in lambdas:\n",
    "\n",
    "        tX_poly_jet1 = build_poly(tX_reformed_jet1, deg)\n",
    "\n",
    "        loss_jet1, weights_jet1 = ridge_regression(y_jet1, tX_poly_jet1, lam)\n",
    "        # print(\"\\nweights_RR:\\n\", weights_jet0,\"\\n\")\n",
    "\n",
    "        pred_jet1 = prediction(y_jet1, tX_poly_jet1, 0, lam, 0, method)\n",
    "        pred_jet1_VEC[i] = pred_jet1\n",
    "        i = i + 1\n",
    "        if pred_jet1 > 0.799 :\n",
    "            print(\"prediction jet1 = \", pred_jet1, \" with lambda: \", str(lam), \" degree: \", str(deg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRkVAURSlhvn/kqRJf+5YNQgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113553fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = plt.plot(pred_jet1_VEC[100:250])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_RR =  0.800585878603\n"
     ]
    }
   ],
   "source": [
    "lambda_jet1 = 3e-10\n",
    "method = 4\n",
    "\n",
    "degree_jet1 = 9\n",
    "\n",
    "tX_reformed_jet1 = tX_cleaned_jet1.copy()\n",
    "w0 = np.ones([1,tX_reformed_jet1.shape[0]])\n",
    "tX_reformed_jet1 = np.insert(tX_reformed_jet1, 0, w0, axis=1)\n",
    "\n",
    "tX_poly_jet1 = build_poly(tX_reformed_jet1, degree_jet1)\n",
    "\n",
    "loss_jet1, weights_jet1 = ridge_regression(y_jet1, tX_poly_jet1, lambda_jet1)\n",
    "# print(\"\\nweights_RR:\\n\", weights_jet1,\"\\n\")\n",
    "\n",
    "pred_jet1 = prediction(y_jet1, tX_poly_jet1, 0, lambda_jet1, 0, method)\n",
    "print(\"pred_RR = \", pred_jet1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JET == 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction jet2 =  0.830403926511  with lambda:  1e-10  degree:  9\n",
      "prediction jet2 =  0.830536578895  with lambda:  2e-10  degree:  9\n",
      "prediction jet2 =  0.830470252703  with lambda:  3e-10  degree:  9\n",
      "prediction jet2 =  0.830403926511  with lambda:  4e-10  degree:  9\n",
      "prediction jet2 =  0.830602905087  with lambda:  5e-10  degree:  9\n",
      "prediction jet2 =  0.830602905087  with lambda:  6e-10  degree:  9\n",
      "prediction jet2 =  0.830735557472  with lambda:  7e-10  degree:  9\n",
      "prediction jet2 =  0.830735557472  with lambda:  8e-10  degree:  9\n",
      "prediction jet2 =  0.830735557472  with lambda:  9e-10  degree:  9\n",
      "prediction jet2 =  0.830735557472  with lambda:  1e-09  degree:  9\n",
      "prediction jet2 =  0.830536578895  with lambda:  1e-08  degree:  9\n",
      "prediction jet2 =  0.830337600318  with lambda:  1e-07  degree:  9\n",
      "prediction jet2 =  0.830138621742  with lambda:  1e-10  degree:  10\n",
      "prediction jet2 =  0.830005969357  with lambda:  2e-10  degree:  10\n",
      "prediction jet2 =  0.830403926511  with lambda:  3e-10  degree:  10\n",
      "prediction jet2 =  0.830005969357  with lambda:  4e-10  degree:  10\n",
      "prediction jet2 =  0.830403926511  with lambda:  5e-10  degree:  10\n",
      "prediction jet2 =  0.830470252703  with lambda:  6e-10  degree:  10\n",
      "prediction jet2 =  0.830204947934  with lambda:  7e-10  degree:  10\n",
      "prediction jet2 =  0.830204947934  with lambda:  8e-10  degree:  10\n",
      "prediction jet2 =  0.830005969357  with lambda:  9e-10  degree:  10\n",
      "prediction jet2 =  0.830271274126  with lambda:  1e-09  degree:  10\n",
      "prediction jet2 =  0.830337600318  with lambda:  1e-08  degree:  10\n",
      "prediction jet2 =  0.830337600318  with lambda:  1e-07  degree:  10\n",
      "prediction jet2 =  0.830204947934  with lambda:  1e-06  degree:  10\n",
      "prediction jet2 =  0.830204947934  with lambda:  1e-05  degree:  10\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-421-a236a9375a5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtX_poly_jet2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_reformed_jet2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss_jet2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_jet2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_jet2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_poly_jet2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# print(\"\\nweights_RR:\\n\", weights_jet0,\"\\n\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-2286a54bcf0f>\u001b[0m in \u001b[0;36mridge_regression\u001b[0;34m(y, tX_poly, lambda_)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_poly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_poly\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlamb_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_poly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_poly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mw_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidrivollet/anaconda/lib/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'DD->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'dd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidrivollet/anaconda/lib/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "pred_jet2_VEC = np.zeros((len(lambdas)*len(degrees), 1))\n",
    "i = 0\n",
    "\n",
    "method = 4\n",
    "\n",
    "tX_reformed_jet2 = tX_cleaned_jet2.copy()\n",
    "w0 = np.ones([1,tX_reformed_jet2.shape[0]])\n",
    "tX_reformed_jet2 = np.insert(tX_reformed_jet2, 0, w0, axis=1)\n",
    "\n",
    "\n",
    "for deg in degrees:\n",
    "    for lam in lambdas:\n",
    "\n",
    "        tX_poly_jet2 = build_poly(tX_reformed_jet2, deg)\n",
    "\n",
    "        loss_jet2, weights_jet2 = ridge_regression(y_jet2, tX_poly_jet2, lam)\n",
    "        # print(\"\\nweights_RR:\\n\", weights_jet0,\"\\n\")\n",
    "\n",
    "        pred_jet2 = prediction(y_jet2, tX_poly_jet2, 0, lam, 0, method)\n",
    "        pred_jet2_VEC[i] = pred_jet2\n",
    "        i = i + 1\n",
    "\n",
    "        if pred_jet2 > 0.83 :\n",
    "            print(\"prediction jet2 = \", pred_jet2, \" with lambda: \", str(lam), \" degree: \", str(deg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6/oKWAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17aaadf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = plt.plot(pred_jet2_VEC)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_RR =  0.830735557472\n"
     ]
    }
   ],
   "source": [
    "lambda_jet2 = 8e-10\n",
    "method = 4\n",
    "\n",
    "degree_jet2 = 9\n",
    "\n",
    "tX_reformed_jet2 = tX_cleaned_jet2.copy()\n",
    "w0 = np.ones([1,tX_reformed_jet2.shape[0]])\n",
    "tX_reformed_jet2 = np.insert(tX_reformed_jet2, 0, w0, axis=1)\n",
    "\n",
    "tX_poly_jet2 = build_poly(tX_reformed_jet2, degree_jet2)\n",
    "\n",
    "loss_jet2, weights_jet2 = ridge_regression(y_jet2, tX_poly_jet2, lambda_jet2)\n",
    "# print(\"\\nweights_RR:\\n\", weights_jet2,\"\\n\")\n",
    "\n",
    "pred_jet2 = prediction(y_jet2, tX_poly_jet2, 0, lambda_jet2, 0, method)\n",
    "\n",
    "print(\"pred_RR = \", pred_jet2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JET == 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction jet3 =  0.823257202568  with lambda:  1e-10  degree:  8\n",
      "prediction jet3 =  0.823406478579  with lambda:  2e-10  degree:  8\n",
      "prediction jet3 =  0.822958650545  with lambda:  3e-10  degree:  8\n",
      "prediction jet3 =  0.823854306613  with lambda:  4e-10  degree:  8\n",
      "prediction jet3 =  0.823705030602  with lambda:  5e-10  degree:  8\n",
      "prediction jet3 =  0.823257202568  with lambda:  6e-10  degree:  8\n",
      "prediction jet3 =  0.823406478579  with lambda:  7e-10  degree:  8\n",
      "prediction jet3 =  0.824302134647  with lambda:  8e-10  degree:  8\n",
      "prediction jet3 =  0.824302134647  with lambda:  9e-10  degree:  8\n",
      "prediction jet3 =  0.824451410658  with lambda:  1e-09  degree:  8\n",
      "prediction jet3 =  0.823107926556  with lambda:  1e-08  degree:  8\n",
      "prediction jet3 =  0.823854306613  with lambda:  1e-07  degree:  8\n",
      "prediction jet3 =  0.824302134647  with lambda:  1e-06  degree:  8\n",
      "prediction jet3 =  0.824451410658  with lambda:  1e-05  degree:  8\n",
      "prediction jet3 =  0.821615166443  with lambda:  0.1  degree:  8\n",
      "prediction jet3 =  0.830422451112  with lambda:  1e-10  degree:  9\n",
      "prediction jet3 =  0.829825347067  with lambda:  2e-10  degree:  9\n",
      "prediction jet3 =  0.828481862965  with lambda:  3e-10  degree:  9\n",
      "prediction jet3 =  0.830123899089  with lambda:  4e-10  degree:  9\n",
      "prediction jet3 =  0.82907896701  with lambda:  5e-10  degree:  9\n",
      "prediction jet3 =  0.828631138976  with lambda:  7e-10  degree:  9\n",
      "prediction jet3 =  0.828929690999  with lambda:  8e-10  degree:  9\n",
      "prediction jet3 =  0.828034034931  with lambda:  9e-10  degree:  9\n",
      "prediction jet3 =  0.828481862965  with lambda:  1e-09  degree:  9\n",
      "prediction jet3 =  0.821615166443  with lambda:  1e-08  degree:  9\n",
      "prediction jet3 =  0.829377519033  with lambda:  1e-07  degree:  9\n",
      "prediction jet3 =  0.828631138976  with lambda:  1e-06  degree:  9\n",
      "prediction jet3 =  0.830273175101  with lambda:  1e-05  degree:  9\n",
      "prediction jet3 =  0.82907896701  with lambda:  0.001  degree:  9\n",
      "prediction jet3 =  0.828332586953  with lambda:  0.01  degree:  9\n",
      "prediction jet3 =  0.828183310942  with lambda:  1e-10  degree:  10\n",
      "prediction jet3 =  0.829377519033  with lambda:  2e-10  degree:  10\n",
      "prediction jet3 =  0.828780414987  with lambda:  3e-10  degree:  10\n",
      "prediction jet3 =  0.828631138976  with lambda:  4e-10  degree:  10\n",
      "prediction jet3 =  0.824152858636  with lambda:  5e-10  degree:  10\n",
      "prediction jet3 =  0.828631138976  with lambda:  8e-10  degree:  10\n",
      "prediction jet3 =  0.826690550828  with lambda:  9e-10  degree:  10\n",
      "prediction jet3 =  0.829228243021  with lambda:  1e-09  degree:  10\n",
      "prediction jet3 =  0.829974623078  with lambda:  1e-08  degree:  10\n",
      "prediction jet3 =  0.830571727123  with lambda:  1e-07  degree:  10\n",
      "prediction jet3 =  0.830571727123  with lambda:  1e-06  degree:  10\n",
      "prediction jet3 =  0.829228243021  with lambda:  1e-05  degree:  10\n",
      "prediction jet3 =  0.829377519033  with lambda:  0.001  degree:  10\n",
      "prediction jet3 =  0.826541274817  with lambda:  0.01  degree:  10\n",
      "prediction jet3 =  0.821018062397  with lambda:  0.1  degree:  10\n",
      "prediction jet3 =  0.822212270488  with lambda:  10  degree:  10\n",
      "prediction jet3 =  0.82683982684  with lambda:  5e-10  degree:  11\n",
      "prediction jet3 =  0.827436930885  with lambda:  8e-10  degree:  11\n",
      "prediction jet3 =  0.825496342738  with lambda:  1e-08  degree:  11\n",
      "prediction jet3 =  0.827138378863  with lambda:  1e-06  degree:  11\n",
      "prediction jet3 =  0.82579489476  with lambda:  1e-05  degree:  11\n",
      "prediction jet3 =  0.830571727123  with lambda:  0.001  degree:  11\n",
      "prediction jet3 =  0.828332586953  with lambda:  0.01  degree:  11\n",
      "prediction jet3 =  0.828183310942  with lambda:  0.1  degree:  11\n",
      "prediction jet3 =  0.826989102851  with lambda:  1  degree:  11\n",
      "prediction jet3 =  0.827586206897  with lambda:  10  degree:  11\n"
     ]
    }
   ],
   "source": [
    "pred_jet3_VEC = np.zeros((len(lambdas)*len(degrees), 1))\n",
    "i = 0\n",
    "\n",
    "method = 4\n",
    "\n",
    "tX_reformed_jet3 = tX_cleaned_jet3.copy()\n",
    "w0 = np.ones([1,tX_reformed_jet3.shape[0]])\n",
    "tX_reformed_jet3 = np.insert(tX_reformed_jet3, 0, w0, axis=1)\n",
    "\n",
    "for deg in degrees:\n",
    "    for lam in lambdas:\n",
    "\n",
    "        tX_poly_jet3 = build_poly(tX_reformed_jet3, deg)\n",
    "\n",
    "        loss_jet3, weights_jet3 = ridge_regression(y_jet3, tX_poly_jet3, lam)\n",
    "        # print(\"\\nweights_RR:\\n\", weights_jet0,\"\\n\")\n",
    "\n",
    "        pred_jet3 = prediction(y_jet3, tX_poly_jet3, 0, lam, 0, method)\n",
    "        pred_jet3_VEC[i] = pred_jet3\n",
    "        i = i + 1\n",
    "\n",
    "        if pred_jet3 > 0.82 :\n",
    "            print(\"prediction jet3 = \", pred_jet3, \" with lambda: \", str(lam), \" degree: \", str(deg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c543a940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = plt.plot(pred_jet3_VEC)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_RR =  0.830571727123\n"
     ]
    }
   ],
   "source": [
    "lambda_jet3 = 1e-7\n",
    "method = 4\n",
    "\n",
    "degree_jet3 = 10\n",
    "\n",
    "tX_reformed_jet3 = tX_cleaned_jet3.copy()\n",
    "w0 = np.ones([1,tX_reformed_jet3.shape[0]])\n",
    "tX_reformed_jet3 = np.insert(tX_reformed_jet3, 0, w0, axis=1)\n",
    "\n",
    "tX_poly_jet3 = build_poly(tX_reformed_jet3, degree_jet3)\n",
    "\n",
    "loss_jet3, weights_jet3 = ridge_regression(y_jet3, tX_poly_jet3, lambda_jet3)\n",
    "# print(\"\\nweights_RR:\\n\", weights_jet3,\"\\n\")\n",
    "\n",
    "pred_jet3 = prediction(y_jet3, tX_poly_jet3, 0, lambda_jet3, 0, method)\n",
    "print(\"pred_RR = \", pred_jet3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 30)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidrivollet/anaconda/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "tX_test_jet0, indexes_test_jet0, tX_test_jet1, indexes_test_jet1, tX_test_jet2, indexes_test_jet2, tX_test_jet3, indexes_test_jet3 = separating_by_jet(tX_test)\n",
    "\n",
    "tX_test_cleaned_jet0 = clean_data_jet(tX_test_jet0)\n",
    "tX_test_cleaned_jet1 = clean_data_jet(tX_test_jet1)\n",
    "tX_test_cleaned_jet2 = clean_data_jet(tX_test_jet2)\n",
    "tX_test_cleaned_jet3 = clean_data_jet(tX_test_jet3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX_test_reformed_jet0 = tX_test_cleaned_jet0.copy()\n",
    "w0 = np.ones([1,tX_test_reformed_jet0.shape[0]])\n",
    "tX_test_reformed_jet0 = np.insert(tX_test_reformed_jet0, 0, w0, axis=1)\n",
    "\n",
    "tX_test_reformed_jet1 = tX_test_cleaned_jet1.copy()\n",
    "w0 = np.ones([1,tX_test_reformed_jet1.shape[0]])\n",
    "tX_test_reformed_jet1 = np.insert(tX_test_reformed_jet1, 0, w0, axis=1)\n",
    "\n",
    "tX_test_reformed_jet2 = tX_test_cleaned_jet2.copy()\n",
    "w0 = np.ones([1,tX_test_reformed_jet2.shape[0]])\n",
    "tX_test_reformed_jet2 = np.insert(tX_test_reformed_jet2, 0, w0, axis=1)\n",
    "\n",
    "tX_test_reformed_jet3 = tX_test_cleaned_jet3.copy()\n",
    "w0 = np.ones([1,tX_test_reformed_jet3.shape[0]])\n",
    "tX_test_reformed_jet3 = np.insert(tX_test_reformed_jet3, 0, w0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX_test_poly_jet0 = build_poly(tX_test_reformed_jet0, degree_jet0)\n",
    "tX_test_poly_jet1 = build_poly(tX_test_reformed_jet1, degree_jet1)\n",
    "tX_test_poly_jet2 = build_poly(tX_test_reformed_jet2, degree_jet2)\n",
    "tX_test_poly_jet3 = build_poly(tX_test_reformed_jet3, degree_jet3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_jet0 = predict_labels(weights_jet0, tX_test_poly_jet0)\n",
    "y_pred_jet1 = predict_labels(weights_jet1, tX_test_poly_jet1)\n",
    "y_pred_jet2 = predict_labels(weights_jet2, tX_test_poly_jet2)\n",
    "y_pred_jet3 = predict_labels(weights_jet3, tX_test_poly_jet3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_final = np.ones((tX_test.shape[0], 1))\n",
    "a = 0\n",
    "b = 0\n",
    "c = 0\n",
    "d = 0\n",
    "for i in range(0, y_pred_final.shape[0]):\n",
    "    if indexes_test_jet0[i] == True:\n",
    "        y_pred_final[i] = y_pred_jet0[a]\n",
    "        a = a + 1\n",
    "    if indexes_test_jet1[i] == True:\n",
    "        y_pred_final[i] = y_pred_jet1[b]\n",
    "        b = b + 1\n",
    "    if indexes_test_jet2[i] == True:\n",
    "        y_pred_final[i] = y_pred_jet2[c]\n",
    "        c = c + 1\n",
    "    if indexes_test_jet3[i] == True:\n",
    "        y_pred_final[i] = y_pred_jet3[d]\n",
    "        d = d + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 1)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/dataSubmission_RR.csv' \n",
    "create_csv_submission(ids_test, y_pred_final, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 30)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tX_test_cleaned, _ = clean_data(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX_test_cleaned_TEST = tX_test_cleaned.copy()\n",
    "w0 = np.ones([1,tX_test_cleaned.shape[0]])\n",
    "tX_test_cleaned_TEST = np.insert(tX_test_cleaned_TEST, 0, w0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX_test_stand, mean_test, std_test = standardize(tX_test, mean_training, std_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/dataSubmission_RR.csv' \n",
    "y_pred_RR = predict_labels(weights_RR, tX_test_stand)\n",
    "create_csv_submission(ids_test, y_pred_RR, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/dataSubmission_LogR.csv' \n",
    "y_pred_LogR = predict_labels(weights_LogR, tX_test_stand)\n",
    "create_csv_submission(ids_test, y_pred_LogR, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/dataSubmission_RLogR.csv' \n",
    "y_pred_RLogR = predict_labels(weights_RLogR_RLogR, tX_test_stand)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/dataSubmission_LS.csv' \n",
    "y_pred_LS = predict_labels(weights_LeastS, tX_test_cleaned_TEST)\n",
    "create_csv_submission(ids_test, y_pred_LS, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX_test_poly_TEST = build_poly(tX_test_cleaned_TEST, degree_RR)\n",
    "\n",
    "OUTPUT_PATH = '../data/dataSubmission_RR.csv' \n",
    "y_pred_RR = predict_labels(weights_RR, tX_test_poly_TEST)\n",
    "create_csv_submission(ids_test, y_pred_RR, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
