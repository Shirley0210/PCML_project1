{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Don't use it! It deacrease the performance \n",
    "from helpers import *\n",
    "\n",
    "tX_norm, mean_, std_ = standardize(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of ML methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression - gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from costs import *\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    # error\n",
    "    e = y - tx.dot(w)\n",
    "    \n",
    "    # gradient\n",
    "    gradient = - np.mean(np.transpose(tx).dot(e))\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, gamma, max_iters):\n",
    "    # Define parameters to store w and loss\n",
    "    w_temp = np.zeros(tx.shape[1]) # initialization of the weight\n",
    "    ws = [w_temp]\n",
    "    losses = []\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        grad = compute_gradient(y, tx, w_temp)\n",
    "        loss = compute_loss(y, tx, w_temp)\n",
    "        \n",
    "        # update w by gradient\n",
    "        w_temp = w_temp - gamma*grad\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w_temp))\n",
    "        losses.append(loss)\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression - stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    B = 35 # size of the batch\n",
    "    sum = 0\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, B):\n",
    "        sum += compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "\n",
    "    return sum / B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, gamma, max_iters):    \n",
    "    # Define parameters to store w and loss\n",
    "    w_temp = np.zeros(tx.shape[1]) # initialization of the weight\n",
    "    ws = [w_temp]\n",
    "    losses = []\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        grad = compute_stoch_gradient(y, tx, w_temp)\n",
    "        loss = compute_loss(y, tx, w_temp)\n",
    "\n",
    "        # update w by gradient\n",
    "        w_temp = w_temp - gamma*grad\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w_temp))\n",
    "        losses.append(loss)\n",
    "        \n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    # Compute optimum weight\n",
    "    tx_transpose = np.transpose(tx)\n",
    "    A = tx_transpose.dot(tx)\n",
    "    b = tx_transpose.dot(y)\n",
    "    w_opt = np.linalg.solve(A,b)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = compute_loss(y, tx, w_opt)\n",
    "        \n",
    "    return loss, w_opt # returns loss, and optimal weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):    \n",
    "    # Initiation variables\n",
    "    lamb_ = 2*len(y)*lambda_ \n",
    "\n",
    "    # Compute optimum weight\n",
    "    tx_transpose = np.transpose(tx)\n",
    "    A = np.dot(tx_transpose,tx) + lamb_*np.eye(tx.shape[1])\n",
    "    b = tx_transpose.dot(y)\n",
    "    w_opt = np.linalg.solve(A,b)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = compute_loss(y, tx, w_opt)\n",
    "    \n",
    "    return loss, w_opt # returns mse, and optimal weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1/(1+np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    # compute the loss\n",
    "    N = tx.shape[0]\n",
    "    l1 = tx.dot(w) + np.log(np.ones((N))+np.exp(-tx.dot(w)))\n",
    "    l2 = y*(tx.dot(w))\n",
    "    loss = (np.ones((1,N)).dot(l1-l2))[0]\n",
    "    \n",
    "    # compute the gradient\n",
    "    grad = np.transpose(tx).dot(sigmoid(tx.dot(w))-y)\n",
    "    \n",
    "    # update w\n",
    "    w = w - gamma*grad\n",
    "\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, gamma, max_iters):\n",
    "    print(\"\\n\\n\\nlogistic_regression(y, tx, gamma, max_iters):\\n\")\n",
    "\n",
    "    # init parameters\n",
    "    threshold = 1e-8\n",
    "    w_temp = np.zeros(tx.shape[1]) # initialization of the weight\n",
    "    ws = [w_temp]\n",
    "    losses = [8000]\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iters):        \n",
    "        # get loss and update w.\n",
    "        loss, w_temp = learning_by_gradient_descent(y, tx, w_temp, gamma)\n",
    "        \n",
    "        # log info\n",
    "        if iter % 1000 == 0:\n",
    "            print(\"Current iteration={i}:\\nLoss={l}\\nw={w}\\n\".format(i=iter, l=loss, w=ws[-1]))\n",
    "            \n",
    "       \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w_temp))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # converge criteria\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    \n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = len(y)\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    \n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train:\n",
    "    x_test = x[k_indices[k]]\n",
    "    y_test = y[k_indices[k]]\n",
    "    \n",
    "    tr_indices = np.delete(k_indices, k, axis=0)\n",
    "    x_tr = np.delete(x, k, axis=0)\n",
    "    y_tr = np.delete(y, k, axis=0)\n",
    "    \n",
    "    # form train and test data with polynomial basis function\n",
    "    poly_x_tr = build_poly(x_tr, degree)\n",
    "    poly_x_test = build_poly(x_test, degree)\n",
    "    \n",
    "    # calcualte weight and loss through least square.\n",
    "    loss_tr, weight_tr = ridge_regression(y_tr, poly_x_tr, lambda_)\n",
    "    loss_test, weight_te = ridge_regression(y_test, poly_x_test, lambda_)\n",
    "    \n",
    "    return loss_tr, loss_te, weight_tr, weight_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_GD=[0.5, 0.49968396075696725, 0.49937944508339982, 0.49908603280156344, 0.4988033190544382, 0.49853091374708974, 0.49826844100840534, 0.49801553867245951, 0.49777185777878874, 0.49753706209088738, 0.49731082763226198, 0.49709284223939948, 0.49688280513103777, 0.49668042649314109, 0.49648542707900722, 0.49629753782395647, 0.49611649947407044, 0.49594206222846771, 0.49577398539462209, 0.49561203705624979, 0.4954559937533064, 0.49530564017365136, 0.495160768855955, 0.49502117990343891, 0.49488668070805397, 0.49475708568471499, 0.49463221601522583, 0.494511899401543, 0.4943959698280338, 0.49428426733240455, 0.494176637784981, 0.49407293267603669, 0.49397300891087537, 0.49387672861238613, 0.49378395893079735, 0.49369457186036747, 0.49360844406275917, 0.49352545669685527, 0.49344549525477843, 0.49336844940389013, 0.49329421283455294, 0.49322268311343936, 0.49315376154219398, 0.49308735302124673, 0.49302336591859192, 0.49296171194335281, 0.49290230602395557, 0.4928450661907447, 0.49278991346288203, 0.49273677173936342, 0.49268556769401617, 0.49263623067431983, 0.49258869260391924, 0.49254288788869088, 0.49249875332623433, 0.49245622801866468, 0.49241525328858393, 0.49237577259811632, 0.49233773147089677, 0.49230107741690177, 0.49226575986002485, 0.49223173006828763, 0.49219894108660062, 0.49216734767197223, 0.49213690623108147, 0.49210757476012806, 0.49207931278687311, 0.49205208131479555, 0.49202584276928341, 0.4920005609457877, 0.49197620095986655, 0.49195272919905081, 0.49193011327646485, 0.49190832198613754, 0.49188732525994583, 0.49186709412612301, 0.49184760066928557, 0.49182881799191241, 0.49181072017723387, 0.49179328225346808, 0.49177648015936676, 0.4917602907110139, 0.4917446915698358, 0.49172966121177852, 0.49171517889760824, 0.49170122464429428, 0.491687779197437, 0.49167482400469942, 0.49166234119020824, 0.49165031352988897, 0.49163872442769885, 0.491627557892728, 0.49161679851713475, 0.4916064314548847, 0.49159644240126682, 0.49158681757315509, 0.49157754368999046, 0.49156860795545498, 0.49155999803981665, 0.49155170206291599]\n",
      "\n",
      "\n",
      "weights_GD=[ 0.00735567  0.00735567  0.00735567  0.00735567  0.00735567  0.00735567\n",
      "  0.00735567  0.00735567  0.00735567  0.00735567  0.00735567  0.00735567\n",
      "  0.00735567  0.00735567  0.00735567  0.00735567  0.00735567  0.00735567\n",
      "  0.00735567  0.00735567  0.00735567  0.00735567  0.00735567  0.00735567\n",
      "  0.00735567  0.00735567  0.00735567  0.00735567  0.00735567  0.00735567\n",
      "  0.00735567]\n"
     ]
    }
   ],
   "source": [
    "# Linear regression - gradient descent\n",
    "max_iters_GD = 100\n",
    "gamma_GD = 1.0e-8\n",
    "loss_GD, weights_GD = least_squares_GD(y, tX_norm, gamma_GD, max_iters_GD)\n",
    "print(\"loss_GD={loss}\\n\\n\".format(loss=loss_GD))\n",
    "print(\"weights_GD={w}\".format(w=weights_GD[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_SGD=[0.5, 0.4999908888560281, 0.49998178728952664, 0.49997269529042765, 0.49996361284867463, 0.49995453995422046, 0.49994547659702948, 0.49993642276707628, 0.49992737845434598, 0.49991834364883431, 0.49990931834054736, 0.49990030251950207, 0.49989129617572498, 0.49988229929925437, 0.49987331188013834, 0.49986433390843521, 0.49985536537421399, 0.4998464062675545, 0.49983745657854656, 0.49982851629729053, 0.49981958541389671, 0.49981066391848689, 0.49980175180119213, 0.49979284905215449, 0.49978395566152634, 0.49977507161947016, 0.49976619691615859, 0.49975733154177543, 0.49974847548651402, 0.49973962874057853, 0.49973079129418285, 0.49972196313755141, 0.49971314426091923, 0.49970433465453146, 0.49969553430864289, 0.49968674321351958, 0.49967796135943682, 0.49966918873668104, 0.49966042533554811, 0.49965167114634473, 0.49964292615938705, 0.49963419036500201, 0.49962546375352684, 0.4996167463153085, 0.49960803804070386, 0.49959933892008068, 0.49959064894381633, 0.49958196810229838, 0.49957329638592479, 0.49956463378510307, 0.49955598029025139, 0.49954733589179728, 0.4995387005801793, 0.49953007434584501, 0.49952145717925306, 0.49951284907087129, 0.49950425001117815, 0.4994956599906612, 0.49948707899981942, 0.49947850702916041, 0.49946994406920237, 0.49946139011047375, 0.49945284514351229, 0.49944430915886612, 0.49943578214709322, 0.49942726409876154, 0.49941875500444854, 0.49941025485474239, 0.49940176364024036, 0.49939328135155009, 0.49938480797928869, 0.49937634351408372, 0.49936788794657183, 0.49935944126740028, 0.49935100346722555, 0.4993425745367146, 0.49933415446654317, 0.49932574324739792, 0.4993173408699747, 0.49930894732497921, 0.499300562603127, 0.4992921866951433, 0.49928381959176305, 0.4992754612837314, 0.49926711176180211, 0.4992587710167401, 0.49925043903931893, 0.4992421158203223, 0.4992338013505434, 0.49922549562078561, 0.49921719862186109, 0.49920891034459219, 0.49920063077981119, 0.49919235991835936, 0.49918409775108818, 0.49917584426885825, 0.49916759946254025, 0.49915936332301397, 0.4991511358411691, 0.49914291700790508]\n",
      "\n",
      "weights_SGD=[ 0.00044652  0.00044652  0.00044652  0.00044652  0.00044652  0.00044652\n",
      "  0.00044652  0.00044652  0.00044652  0.00044652  0.00044652  0.00044652\n",
      "  0.00044652  0.00044652  0.00044652  0.00044652  0.00044652  0.00044652\n",
      "  0.00044652  0.00044652  0.00044652  0.00044652  0.00044652  0.00044652\n",
      "  0.00044652  0.00044652  0.00044652  0.00044652  0.00044652  0.00044652\n",
      "  0.00044652]\n"
     ]
    }
   ],
   "source": [
    "# Linear regression - stochastic gradient descent\n",
    "max_iters_SGD = 100\n",
    "gamma_SGD = 1.0e-8\n",
    "loss_SGD, weights_SGD = least_squares_SGD(y, tX, gamma_SGD, max_iters_SGD)\n",
    "print(\"loss_SGD={loss}\\n\".format(loss=loss_SGD))\n",
    "print(\"weights_SGD={w}\".format(w=weights_SGD[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_LeastS=0.3394455984893298\n",
      "\n",
      "weights_LeastS=[ -3.14664000e-01   2.93788270e-02  -2.52531475e-01  -2.54791124e-01\n",
      "  -3.03696823e-02  -1.40144743e+00   2.95701642e-01  -1.07889472e+01\n",
      "   2.67880862e-01  -2.44934976e-03  -3.28818528e+02  -1.82647888e-01\n",
      "   1.14039627e-01   2.05045963e+01   6.38835904e+01  -3.18961906e-04\n",
      "  -1.80884296e-03   6.29927695e+01  -4.48641463e-04   1.54379289e-03\n",
      "   1.21462701e-01   3.95268822e-04  -6.33223472e-02  -2.06747093e-01\n",
      "  -1.16655769e-01   9.86256395e-02   1.67907714e-01  -3.35146266e-02\n",
      "  -2.98358689e+00  -5.36388093e+00   2.78471292e+02]\n"
     ]
    }
   ],
   "source": [
    "# Least squares\n",
    "loss_LeastS, weights_LeastS = least_squares(y, tX)\n",
    "print(\"loss_LeastS={loss}\\n\".format(loss=loss_LeastS))\n",
    "print(\"weights_LeastS={w}\".format(w=weights_LeastS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_RR=0.3549991006182304\n",
      "\n",
      "weights_RR=[  2.68150720e-04  -9.13263786e-03  -1.92944215e-03  -1.91986925e-03\n",
      "   5.95207215e-04   5.84831367e-04  -5.45852749e-03   7.83188539e-03\n",
      "   8.66797629e-05   2.89451390e-03  -7.08966760e-03   1.69641158e-02\n",
      "   3.09674014e-03   6.39685801e-03  -1.62492159e-04  -7.91331334e-04\n",
      "   1.90485502e-03  -2.09337083e-04   5.51201056e-04   4.76505676e-03\n",
      "   2.47039308e-04  -7.62871230e-04  -6.41114885e-03   1.48939737e-03\n",
      "  -7.43123705e-04  -5.96195117e-04   2.93411957e-04   8.26006211e-04\n",
      "   2.05161529e-04  -5.40700797e-03]\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression\n",
    "lambda_RR = 2\n",
    "loss_RR, weights_RR = ridge_regression(y, tX, lambda_RR)\n",
    "print(\"loss_RR={loss}\\n\".format(loss=loss_RR))\n",
    "print(\"weights_RR={w}\".format(w=weights_RR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "logistic_regression(y, tx, gamma, max_iters):\n",
      "\n",
      "Current iteration=0:\n",
      "Loss=173286.79513998912\n",
      "w=[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "loss_LogR=[8000, 173286.79513998912, 172483.7577171626, 171685.20036042391, 170891.0665574873, 170101.30049701728, 169315.84706532262, 168534.65184280818, 167757.66110019016, 166984.82179449184, 166216.08156482669, 165451.38872798573, 164690.69227383647, 163933.9418605449, 163181.08780963591, 162432.08110089423, 161686.8733671243, 160945.41688877461, 160207.66458843544, 159473.57002522162, 158743.08738904889, 158016.17149481032, 157292.77777646296, 156572.86228103261, 155856.38166254456, 155143.29317588598, 154433.55467060988, 153727.12458468525, 153023.96193820136, 152324.02632703132, 151627.27791646254, 150933.67743479955, 150243.18616694189, 149555.76594794809, 148871.37915658497, 148189.98870887063, 147511.55805161555, 146836.05115596426, 146163.43251094344, 145493.6671170202, 144826.72047967406, 144162.55860298633, 143501.1479832496, 142842.45560260199, 142186.44892268762, 141533.0958783481, 140882.36487134497, 140234.22476411852, 139588.6448735827, 138945.59496496167, 138305.04524566612, 137666.96635921422, 137031.32937919791, 136398.10580329667, 135767.26754733946, 135138.78693941736, 134512.63671404726, 133888.79000638908, 133267.22034651588, 132647.90165373968, 132030.80823099223, 131415.91475926363, 130803.19629209703, 130192.6282501425, 129584.18641576864, 128977.84692773406, 128373.5862759179, 127771.38129611082, 127171.209164866, 126573.04739441021, 125976.87382761702, 125382.66663303883, 124790.40430000142, 124200.06563375922, 123611.62975071043, 123025.07607367475, 122440.38432722997, 121857.534533111, 121276.50700566807, 120697.2823473859, 120119.84144446222, 119544.1654624471, 118970.23584194016, 118398.03429434837, 117827.54279770177, 117258.74359252755, 116691.6191777823, 116126.1523068418, 115562.32598354784, 115000.12345831165, 114439.52822427321, 113880.52401351694, 113323.09479334185, 112767.22476258695, 112212.89834800959, 111660.10020071949, 111108.81519266372, 110559.02841316516, 110010.72516551327, 109463.89096360488, 108918.51152863743, 108374.57278585121, 107832.06086132154, 107290.96207880063, 106751.26295660686, 106212.9502045628, 105676.01072097993, 105140.43158969052, 104606.20007712554, 104073.30362943764, 103541.72986966951, 103011.4665949663, 102482.50177383234, 101954.82354343001, 101428.42020692241, 100903.28023085659, 100379.39224258981, 99856.745027754892, 99335.327527767717, 98815.128837372817, 98296.138202228554, 97778.345016531559, 97261.738820678147, 96746.309298963883, 96232.046277319838, 95718.939721085015, 95206.979732814885, 94696.156550125103, 94186.460543569992, 93677.88221455476, 93170.412193282697, 92664.041236734134, 92158.760226678743, 91654.560167720672, 91151.432185373676, 90649.367524169182, 90148.357545794308, 89648.393727259885, 89149.467659098853, 88651.571043593285, 88154.695693031012, 87658.833527989758, 87163.976575650086, 86670.116968135102, 86177.246940877449, 85685.358831013466, 85194.445075802287, 84704.498211072001, 84215.510869690479, 83727.47578006117, 83240.385764643608, 82754.233738498078, 82269.012707853661, 81784.715768700466, 81301.336105403549, 80818.866989340982, 80337.301777562665, 79856.633911472163, 79376.856915529628, 78897.964395975199, 78419.95003957492, 77942.807612385121, 77466.530958538351, 76991.113999048714, 76516.550730636285, 76042.83522457123, 75569.96162553638, 75097.924150508523, 74626.717087657977, 74156.334795265691, 73686.771700658399, 73218.022299161006, 72750.081153065505, 72282.942890617516, 71816.6022050186, 71351.05385344493, 70886.292656082049, 70422.313495175316, 69959.111314095353, 69496.681116419117, 69035.017965025589, 68574.116981206185, 68113.973343789345, 67654.58228827978, 67195.939106011327, 66738.039143313072, 66280.877800689937, 65824.450532015442, 65368.752843738221, 64913.780294101103, 64459.528492372388, 64005.993098090374, 63553.169820318974, 63101.054416916333, 62649.64269381432, 62198.930504310149, 61748.913748369363, 61299.58837193939, 60850.950366275007, 60402.995767273984, 59955.720654823279, 59509.121152156404, 59063.193425220234, 58617.933682052397, 58173.338172168529, 57729.403185959338, 57286.125054097007, 56843.500146951468, 56401.524874015791, 55960.195683340498, 55519.509060976852, 55079.461530430068, 54640.049652119298, 54201.270022847762, 53763.119275280602, 53325.594077431262, 52888.69113215564, 52452.407176654422, 52016.738981983726, 51581.683352572574, 51147.237125748899, 50713.397171272125, 50280.160390874022, 49847.523717806172, 49415.484116394553, 48984.038581601599, 48553.184138594748, 48122.917842321578, 47693.236777092345, 47264.138056168136, 46835.618821356271, 46407.676242611531, 45980.307517643654, 45553.50987153138, 45127.280556342048, 44701.616850757251, 44276.516059704445, 43851.97551399417, 43427.992569963186, 43004.564609122514, 42581.689037811695, 42159.363286858017, 41737.584811241017, 41316.351089762233, 40895.659624720196, 40475.507941590309, 40055.89358870963, 39636.814136966612, 39218.26717949567, 38800.250331376403, 38382.761229337302, 37965.797531464341, 37549.356916913806, 37133.437085629492, 36718.035758064521, 36303.15067490701, 35888.779596810477, 35474.920304127852, 35061.570596649821, 34648.728293347245, 34236.391232117268, 33824.557269533485, 33413.224280599788, 33002.390158508228, 32592.052814400136, 32182.210177131186, 31772.860193040091, 31364.000825720388, 30955.630055796217, 30547.74588070107, 30140.346314460177, 29733.42938747591, 29326.993146316796, 28921.035653509261, 28515.554987333009, 28110.549241618945, 27706.016525550669, 27301.95496346859, 26898.36269467706, 26495.237873254606, 26092.57866786663, 25690.38326158133, 25288.649851688002, 24887.376649518468, 24486.561880270609, 24086.20378283516, 23686.300609624624, 23286.850626404965, 22887.852112129654, 22489.3033587764, 22091.202671186034, 21693.548366904026, 21296.338776024149, 20899.572241034646, 20503.24711666656, 20107.361769744264, 19711.914579038363, 19316.90393512051, 18922.328240220711, 18528.185908086354, 18134.475363843649, 17741.195043860796, 17348.343395613367, 16955.918877551652, 16563.919958969851, 16172.345119877155, 15781.192850870959, 15390.461653011575, 15000.150037699019, 14610.256526551504, 14220.779651285757, 13831.717953598945, 13443.069985052502, 13054.834306957422, 12667.009490261433, 12279.594115437696, 11892.586772375045, 11505.986060269941, 11119.790587519907, 10733.998971618545, 10348.609839052018, 9963.6218251970022, 9579.0335742201896, 9194.8437389791834, 8811.050980924756, 8427.6539700046469, 8044.651384568504, 7662.0419112744694, 7279.8242449968893, 6897.9970887353948, 6516.559153525237, 6135.5091583490594, 5754.8458300496586, 5374.5679032441876, 4994.674120239546, 4615.1632309488505, 4236.0339928091262, 3857.2851707003347, 3478.9155368652523, 3100.9238708307012, 2723.3089593297818, 2346.0695962251434, 1969.2045824335735, 1592.7127258512658, 1216.5928412804838, 840.84375035702669, 465.46428147880778, 90.453269735367542, -284.19044316159, -658.46800894668991, -1032.380572868532, -1405.9292737562482, -1779.1152440852629, -2151.9396100420845, -2524.4034915882548, -2896.5080025232942, -3268.2542505470119, -3639.6433373206905, -4010.6763585276917, -4381.3544039330391, -4751.6785574422565, -5121.6498971594474, -5491.269495444516, -5860.5384189696397, -6229.4577287750089, -6598.0284803237064, -6966.2517235560335, -7334.1285029428518, -7701.6598575384669, -8068.84682103257, -8435.6904218016534, -8802.1916829596921, -9168.3516224080468, -9534.1712528847729, -9899.6515820133773, -10264.793612350761, -10629.598341434556, -10994.06676182997, -11358.199861175748, -11721.998622229861, -12085.464022914241, -12448.597036359228, -12811.398630947271, -13173.869770356065, -13536.011413601118, -13897.824515077849, -14259.31002460307, -14620.468887455878, -14981.30204441808, -15341.810431814127, -15701.994981550419, -16061.856621154204, -16421.396273811959, -16780.614858407171, -17139.513289557795, -17498.092477653103, -17856.353328890153, -18214.296745309632, -18571.923624831477, -18929.234861289919, -19286.231344467949, -19642.913960131598, -19999.28359006362, -20355.341112096845, -20711.087400146931, -21066.523324244994, -21421.649750569552, -21776.467541478149, -22130.977555538808, -22485.180647560661, -22839.07766862457, -23192.669466113261, -23545.956883740873, -23898.940761582642, -24251.621936103584, -24604.001240187237, -24956.079503164088, -25307.857550839413, -25659.336205520754, -26010.516286045462, -26361.398607807441, -26711.983982783768, -27062.27321956103, -27412.267123361293, -27761.966496067667, -28111.37213624984, -28460.484839188859, -28809.30539690208, -29157.834598167527, -29506.07322854807, -29854.022070415282, -30201.68190297296, -30549.053502280618, -30896.137641276233, -31242.935089799335, -31589.446614613124, -31935.672979427043, -32281.614944918532, -32627.273268754841, -32972.648705614403, -33317.742007208173, -33662.553922300467, -34007.085196729764, -34351.336573429129, -34695.308792446573, -35039.002590964927, -35382.418703321724, -35725.557861028843, -36068.420792791585, -36411.008224528094, -36753.320879388222, -37095.359477771985, -37437.124737348306, -37778.617373073212, -38119.838097207947, -38460.787619336763, -38801.466646384586, -39141.875882634689, -39482.016029745821, -39821.887786769301, -40161.491850166043, -40500.828913823112, -40839.899669070466, -41178.704804697249, -41517.245006967903, -41855.520959638234, -42193.533343971423, -42531.282838753308, -42868.770120308363, -43205.995862514625, -43542.960736819194, -43879.665412253002, -44216.110555445863, -44552.296830641033, -44888.224899709923, -45223.895422166192, -45559.30905518042, -45894.466453593865, -46229.368269932544, -46564.015154421097, -46898.40775499634, -47232.546717320787, -47566.432684796222, -47900.066298576727, -48233.448197581958, -48566.579018510121, -48899.459395850616, -49232.08996189708, -49564.471346759849, -49896.6041783783, -50228.489082533415, -50560.126682859853, -50891.517600858162, -51222.662455906691, -51553.561865273572, -51884.216444128317, -52214.626805553518, -52544.793560556456, -52874.717318080453, -53204.398685016145, -53533.838266212821, -53863.03666448922, -54191.994480644884, -54520.712313470802, -54849.190759760248, -55177.430414319504, -55505.431869978245, -55833.195717600436, -56160.722546094185, -56488.012942422429, -56815.067491612957, -57141.886776768617, -57468.471379076989, -57794.821877820927, -58120.938850387654, -58446.822872278986, -58772.47451712082, -59097.894356672812, -59423.082960837477, -59748.040897670129, -60072.768733387784, -60397.267032378513, -60721.536357210498, -61045.577268641362, -61369.390325626657, -61692.976085329385, -62016.335103128338, -62339.467932627027, -62662.375125662395, -62985.057232313295, -63307.514800909208, -63629.748378038472, -63951.758508556879, -64273.545735595719, -64595.110600570399, -64916.453643188244, -65237.575401456823, -65558.476411691983, -65879.157208525605, -66199.618324913783, -66519.860292144498, -66839.883639845575, -67159.688895992105, -67479.276586914377, -67798.64723730518, -68117.801370227782, -68436.739507122751, -68755.46216781612, -69073.969870525994, -69392.263131870466, -69710.342466874354, -70028.208388976826, -70345.86141003808, -70663.302040346854, -70980.530788626929, -71297.548162044666, -71614.354666215411, -71930.950805210625, -72247.337081564561, -72563.51399628105, -72879.482048840349, -73195.24173720539, -73510.793557828889, -73826.138005659363, -74141.275574148167, -74456.206755255524, -74770.932039456966, -75085.451915750105, -75399.766871660366, -75713.877393247647, -76027.783965112438, -76341.487070401752, -76654.98719081578, -76968.284806613505, -77281.380396618901, -77594.274438227061, -77906.967407409844, -78219.459778722041, -78531.752025307214, -78843.844618903619, -79155.738029849599, -79467.432727089836, -79778.929178180755, -80090.227849296352, -80401.329205233706, -80712.233709418622, -81022.941823911126, -81333.454009411245, -81643.770725264156, -81953.892429465894, -82263.819578668423, -82573.552628185455, -82883.092031997527, -83192.438242757315, -83501.591711794856, -83810.552889123035, -84119.322223442519, -84427.900162147038, -84736.287151328419, -85044.483635781915, -85352.490059010888, -85660.306863232545, -85967.93448938198, -86275.373377117954, -86582.623964827391, -86889.686689630427, -87196.56198738543, -87503.250292693658, -87809.752038903942, -88116.067658117987, -88422.197581194589, -88728.142237754684, -89033.902056185878, -89339.477463647228, -89644.868886074022, -89950.076748181993, -90255.101473472343, -90559.943484235962, -90864.603201558217, -91169.081045323372, -91473.377434218957, -91777.492785740498, -92081.427516195603, -92385.182040708722, -92688.756773225061, -92992.152126515604, -93295.36851218075, -93598.406340655056, -93901.266021211297, -94203.947961965008, -94506.452569878253, -94808.780250764321, -95110.931409291545, -95412.906448987516, -95714.705772243615, -96016.329780318512, -96317.77887334273, -96619.053450322375, -96920.153909143526, -97221.080646576098, -97521.834058277571, -97822.4145387975, -98122.822481581097, -98423.058278973156, -98723.122322222218, -99023.015001484368, -99322.736705826974, -99622.287823232735, -99921.668740603229, -100220.87984376305, -100519.92151746352, -100818.79414538629, -101117.49811014718, -101416.03379330011, -101714.40157534057, -102012.60183570921, -102310.63495279613, -102608.50130394354, -102906.20126545025, -103203.73521257512, -103501.10351954047, -103798.30655953559, -104095.34470472045, -104392.21832622949, -104688.92779417477, -104985.47347764944, -105281.85574473167, -105578.07496248785, -105874.13149697587, -106170.02571324888, -106465.75797535869, -106761.32864635889, -107056.73808830869, -107351.9866622758, -107647.07472834011, -107942.00264559695, -108236.77077216058, -108531.37946516699, -108825.82908077809, -109120.11997418379, -109414.25249960661, -109708.22701030354, -110002.04385857061, -110295.70339574508, -110589.20597220905, -110882.55193739284, -111175.74163977779, -111468.77542689965, -111761.65364535165, -112054.37664078764, -112346.94475792508, -112639.35834054841, -112931.61773151191, -113223.72327274292, -113515.67530524477, -113807.47416909985, -114099.12020347267, -114390.61374661283, -114681.95513585827, -114973.14470763782, -115264.18279747455, -115555.06973998851, -115845.8058688998, -116136.39151703141, -116426.82701631234, -116717.1126977801, -117007.24889158446, -117297.23592698893, -117587.07413237508, -117876.76383524452, -118166.30536222205, -118455.69903905829, -118744.94519063307, -119034.0441409573, -119322.99621317671, -119611.80172957396, -119900.4610115718, -120188.97437973584, -120477.34215377663, -120765.5646525535, -121053.64219407647, -121341.57509550941, -121629.36367317213, -121917.00824254396, -122204.50911826559, -122491.86661414232, -122779.08104314645, -123066.15271741999, -123353.08194827725, -123639.8690462076, -123926.51432087776, -124213.018081135, -124499.38063500903, -124785.60228971503, -125071.68335165616, -125357.624126426, -125643.42491881116, -125929.086032794, -126214.60777155467, -126499.99043747429, -126785.23433213677, -127070.33975633194, -127355.30701005748, -127640.13639252208, -127924.82820214705, -128209.38273656956, -128493.8002926445, -128778.08116644726, -129062.22565327625, -129346.2340476549, -129630.10664333448, -129913.84373329619, -130197.44560975384, -130480.91256415595, -130764.24488718827, -131047.4428687763, -131330.50679808727, -131613.43696353288, -131896.2336527711, -132178.89715270928, -132461.42774950579, -132743.82572857244, -133026.09137457726, -133308.22497144598, -133590.22680236516, -133872.09714978375, -134153.83629541597, -134435.44452024304, -134716.9221045156, -134998.26932775625, -135279.48646876134, -135560.5738056034, -135841.53161563337, -136122.36017548264, -136403.05976106555, -136683.63064758119, -136964.07310951583, -137244.38742064533, -137524.57385403648, -137804.63268205011, -138084.56417634265, -138364.36860786832, -138644.04624688165, -138923.59736293912, -139203.02222490139, -139482.32110093575, -139761.49425851769, -140040.54196443345, -140319.46448478184, -140598.26208497625, -140876.93502974726, -141155.48358314362, -141433.90800853542, -141712.2085686159, -141990.38552540279, -142268.43914024089, -142546.36967380432, -142824.17738609787, -143101.86253645975, -143379.42538356298, -143656.86618541757, -143934.1851993727, -144211.38268211851, -144488.45888968775, -144765.4140774589, -145042.24850015639, -145318.9624118543, -145595.55606597691, -145872.02971530159, -146148.38361195999, -146424.61800744076, -146700.73315259043, -146976.72929761652, -147252.60669208854, -147528.3655849403, -147804.00622447144, -148079.52885834954, -148354.93373361253, -148630.22109666947, -148905.39119330299, -149180.44426867142, -149455.38056731009, -149730.20033313351, -150004.90380943735, -150279.49123889933, -150553.96286358259, -150828.31892493588, -151102.55966379686, -151376.68532039254, -151650.69613434226, -151924.59234465845, -152198.3741897493, -152472.04190741963, -152745.5957348739, -153019.03590871673, -153292.36266495526, -153565.57623900074, -153838.67686567042, -154111.66477918939, -154384.54021319171, -154657.30340072318, -154929.95457424191, -155202.49396562079, -155474.92180614895, -155747.23832653346, -156019.44375690102, -156291.53832680013, -156563.52226520173, -156835.39580050181, -157107.15916052274, -157378.81257251499, -157650.35626315876, -157921.79045856546, -158193.11538427987, -158464.33126528136, -158735.43832598545, -159006.43679024588, -159277.32688135566, -159548.10882204951, -159818.78283450461, -160089.34914034276, -160359.80796063173, -160630.15951588703, -160900.4040260734, -161170.54171060649, -161440.57278835413, -161710.49747763877, -161980.31599623768, -162250.02856138587, -162519.63538977687, -162789.13669756451, -163058.5327003643, -163327.82361325546, -163597.00965078181, -163866.09102695415, -164135.0679552505, -164403.94064861909, -164672.70931947895, -164941.37417972175, -165209.9354407129, -165478.3933132941, -165746.74800778346, -166014.99973397786, -166283.14870115474, -166551.19511807238, -166819.13919297262, -167086.98113358178, -167354.72114711173, -167622.3594402624, -167889.89621922234, -168157.33168967062, -168424.66605677805, -168691.89952520875, -168959.03229912173, -169226.06458217188, -169492.99657751212, -169759.82848779409, -170026.56051516964, -170293.19286129344, -170559.72572732222, -170826.15931391862, -171092.49382125051, -171358.72944899369, -171624.86639633277, -171890.90486196271, -172156.84504409024, -172422.68714043495, -172688.43134823107, -172954.07786422851, -173219.62688469456, -173485.07860541492, -173750.43322169548, -174015.69092836301, -174280.85191976716, -174545.91638978169, -174810.88453180526, -175075.75653876361, -175340.53260311036, -175605.21291682852, -175869.79767143133, -176134.28705796463, -176398.68126700725, -176662.98048867239, -176927.18491260946, -177191.29472800542, -177455.31012358508, -177719.23128761334, -177983.05840789617, -178246.79167178221, -178510.43126616333, -178773.97737747669, -179037.43019170564, -179300.78989438081, -179564.05667058195, -179827.23070493841, -180090.31218163142, -180353.30128439411, -180616.19819651399, -180879.00310083295, -181141.71617975002, -181404.33761522087, -181666.86758876088, -181929.3062814445, -182191.65387390781, -182453.91054634954, -182716.07647853176, -182978.1518497816, -183240.13683899233, -183502.03162462404, -183763.83638470602, -184025.55129683681, -184287.17653818586, -184548.71228549513, -184810.15871507931, -185071.51600282779, -185332.78432420568, -185593.96385425481, -185855.05476759514, -186116.05723842548, -186376.97144052543, -186637.79754725558, -186898.53573155968, -187159.18616596484, -187419.74902258336, -187680.22447311378, -187940.61268884185, -188200.91384064109, -188461.12809897546, -188721.25563389907, -188981.29661505783, -189241.25121169095, -189501.11959263118, -189760.9019263068, -190020.59838074213, -190280.20912355915, -190539.73432197783, -190799.17414281823, -191058.52875250112, -191317.79831704887, -191576.98300208675, -191836.08297284401, -192095.0983941553, -192354.02943046094, -192612.87624580879, -192871.63900385512, -193130.31786786532, -193388.91300071546, -193647.42456489304, -193905.85272249812, -194164.19763524475, -194422.45946446114, -194680.63837109186, -194938.73451569816, -195196.74805845876, -195454.67915917176, -195712.52797725494, -195970.29467174722, -196227.97940130977, -196485.5823242263, -196743.10359840508, -197000.54338137916]\n",
      "\n",
      "weights_LogR=[ -1.63804065e+00   3.64672855e-01  -5.53586723e-01  -2.50989391e-02\n",
      "   1.62741398e-01   3.47238487e-02   1.51288313e-01   3.23871539e-02\n",
      "   1.54388532e-01  -1.10027994e-01   6.35123090e-02  -3.12998218e-01\n",
      "   3.40701223e-01   3.40762437e-02   3.28488386e-01  -3.32008220e-03\n",
      "  -8.60482392e-03  -6.05178194e-02  -1.07405324e-03   7.03637670e-03\n",
      "  -4.80274850e-02   9.73582148e-03   3.31626622e-02  -4.31637064e-03\n",
      "   8.33161557e-02   7.87563602e-02   7.87567537e-02   3.13487744e-02\n",
      "   3.39014076e-02   3.38838531e-02   1.34874196e-02]\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "max_iters_LogR = 1000\n",
    "gamma_LogR = 1.0e-8\n",
    "loss_LogR, weights_LogR = logistic_regression(y, tX, gamma_LogR, max_iters_LogR)\n",
    "print(\"loss_LogR={loss}\\n\".format(loss=loss_LogR))\n",
    "print(\"weights_LogR={w}\".format(w=weights_LogR[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/dataSubmission_RR.csv' \n",
    "y_pred = predict_labels(weights_RR, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/dataSubmission_LogR.csv' \n",
    "y_pred = predict_labels(weights_LogR[1000], tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
